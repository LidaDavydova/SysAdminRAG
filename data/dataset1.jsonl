{"id": "20b6a14b37", "source_url": "dataset://entry/Docker_build_fails_with_'failed_to_solve_with_fron", "title": "Docker build fails with 'failed to solve with frontend dockerfile.v0: failed to create LLB definitio...", "text": "Question: Docker build fails with 'failed to solve with frontend dockerfile.v0: failed to create LLB definition'. How do I fix it?\n\nSolution:\nThis often indicates a syntax error in the Dockerfile or missing base image. Validate your Dockerfile syntax and ensure the FROM image exists and is accessible. Try rebuilding with `--no-cache` to force a clean build.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 57, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0329229f33", "source_url": "dataset://entry/Docker_container_exits_immediately_after_starting.", "title": "Docker container exits immediately after starting. How to keep it running?", "text": "Question: Docker container exits immediately after starting. How to keep it running?\n\nSolution:\nDocker containers stop when the main process exits. To keep it running, use a foreground process (like `tail -f /dev/null`) or use `docker run -it <image> /bin/bash` for an interactive shell.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 44, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "dbc4b9c2d9", "source_url": "dataset://entry/Docker_daemon_not_starting:_'Error_starting_daemon", "title": "Docker daemon not starting: 'Error starting daemon: pid file found, ensure docker is not running'.", "text": "Question: Docker daemon not starting: 'Error starting daemon: pid file found, ensure docker is not running'.\n\nSolution:\nRemove stale PID files using `sudo rm /var/run/docker.pid` and restart Docker using `sudo systemctl restart docker`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9d3bf19816", "source_url": "dataset://entry/Cannot_connect_to_Docker_daemon_at_unix:___var_run", "title": "Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?", "text": "Question: Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n\nSolution:\nEnsure the Docker service is running: `sudo systemctl start docker`. If still failing, verify your user has access to the docker group: `sudo usermod -aG docker $USER` and re-login.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 43, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a311363cd2", "source_url": "dataset://entry/Docker_image_size_too_large._How_to_reduce_it?", "title": "Docker image size too large. How to reduce it?", "text": "Question: Docker image size too large. How to reduce it?\n\nSolution:\nUse a smaller base image like `alpine`, combine RUN commands to reduce layers, clean up package caches, and use multi-stage builds to discard build dependencies.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d02f8639e4", "source_url": "dataset://entry/Docker_run_fails_with_'port_already_allocated'._Ho", "title": "Docker run fails with 'port already allocated'. How to fix?", "text": "Question: Docker run fails with 'port already allocated'. How to fix?\n\nSolution:\nAnother container or process is using the port. Run `docker ps` to find and stop conflicting containers or change the host port mapping using `-p <new_host_port>:<container_port>`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f8146f7afb", "source_url": "dataset://entry/Docker_build_fails_with_'permission_denied'_when_c", "title": "Docker build fails with 'permission denied' when copying files.", "text": "Question: Docker build fails with 'permission denied' when copying files.\n\nSolution:\nEnsure the build context includes correct file permissions and paths. Avoid copying files from outside the build context. Adjust file permissions using `chmod` or Dockerfile USER directives.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c59d507e9b", "source_url": "dataset://entry/Docker_Compose_up_fails_with_'network_not_found'.", "title": "Docker Compose up fails with 'network not found'.", "text": "Question: Docker Compose up fails with 'network not found'.\n\nSolution:\nRun `docker network ls` to verify existing networks. If missing, remove orphaned containers and rebuild: `docker-compose down && docker-compose up --build`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5929ed1f82", "source_url": "dataset://entry/Docker_container_cannot_access_the_internet.", "title": "Docker container cannot access the internet.", "text": "Question: Docker container cannot access the internet.\n\nSolution:\nCheck that the Docker bridge network is active: `docker network inspect bridge`. Restart Docker if the bridge is missing. Also ensure host firewall isn’t blocking outbound traffic.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b7990aa909", "source_url": "dataset://entry/Image_pull_fails_with_'toomanyrequests:_rate_limit", "title": "Image pull fails with 'toomanyrequests: rate limit exceeded'.", "text": "Question: Image pull fails with 'toomanyrequests: rate limit exceeded'.\n\nSolution:\nAuthenticate with Docker Hub using `docker login`, use a mirror registry, or consider a Docker Hub Pro account to increase pull limits.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6c51b1c62c", "source_url": "dataset://entry/Docker_volume_not_persisting_data.", "title": "Docker volume not persisting data.", "text": "Question: Docker volume not persisting data.\n\nSolution:\nEnsure you’re using a named volume or host mount. Anonymous volumes are recreated each run. Use `-v mydata:/app/data` instead of `-v /app/data`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c4fa9160c6", "source_url": "dataset://entry/Docker_container_time_not_matching_host_time.", "title": "Docker container time not matching host time.", "text": "Question: Docker container time not matching host time.\n\nSolution:\nMount the host’s timezone file with `-v /etc/localtime:/etc/localtime:ro` or use the `TZ` environment variable in your container to set the correct timezone.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0dad7625f7", "source_url": "dataset://entry/Docker_build_context_too_large_causing_slow_builds", "title": "Docker build context too large causing slow builds.", "text": "Question: Docker build context too large causing slow builds.\n\nSolution:\nUse a `.dockerignore` file to exclude unnecessary files (node_modules, build artifacts, logs). Keep context minimal for faster builds.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "70b61f8e3f", "source_url": "dataset://entry/Docker_push_fails_with_'denied:_requested_access_t", "title": "Docker push fails with 'denied: requested access to the resource is denied'.", "text": "Question: Docker push fails with 'denied: requested access to the resource is denied'.\n\nSolution:\nEnsure you’re logged in to the correct registry and have permissions. Use `docker login <registry>` and confirm repository naming matches registry conventions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e974cb8f5e", "source_url": "dataset://entry/Container_logs_show_'standard_init_linux.go:219:_e", "title": "Container logs show 'standard_init_linux.go:219: exec user process caused: no such file or directory...", "text": "Question: Container logs show 'standard_init_linux.go:219: exec user process caused: no such file or directory'.\n\nSolution:\nYour entrypoint or CMD may reference a file with Windows line endings. Convert scripts to Unix format using `dos2unix` before building.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "50e3e45f63", "source_url": "dataset://entry/Docker_network_bridge_conflicts_with_host_network_", "title": "Docker network bridge conflicts with host network IP.", "text": "Question: Docker network bridge conflicts with host network IP.\n\nSolution:\nEdit `/etc/docker/daemon.json` to change the default bridge subnet: `{ \"bip\": \"172.26.0.1/16\" }`, then restart Docker.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 25, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "54e91f660c", "source_url": "dataset://entry/Docker-compose_service_fails_with_'service_depends", "title": "Docker-compose service fails with 'service depends on undefined service'.", "text": "Question: Docker-compose service fails with 'service depends on undefined service'.\n\nSolution:\nEnsure service names are correctly spelled and indentation is valid in your `docker-compose.yml`. YAML formatting errors often cause this.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7272463f92", "source_url": "dataset://entry/Docker_build_caching_not_working._Always_rebuildin", "title": "Docker build caching not working. Always rebuilding layers.", "text": "Question: Docker build caching not working. Always rebuilding layers.\n\nSolution:\nAvoid using dynamic commands like `ADD . /app` early in the Dockerfile. Reorder Dockerfile so static steps occur first, enabling layer caching.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "57f79dad43", "source_url": "dataset://entry/Docker_container_uses_100%_CPU._How_to_limit_it?", "title": "Docker container uses 100% CPU. How to limit it?", "text": "Question: Docker container uses 100% CPU. How to limit it?\n\nSolution:\nUse resource constraints with `--cpus` or `--cpuset-cpus` flags, e.g. `docker run --cpus=1 <image>`. Review app processes inside the container using `docker top`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "29f203b335", "source_url": "dataset://entry/Docker_image_fails_with_'no_space_left_on_device'.", "title": "Docker image fails with 'no space left on device'.", "text": "Question: Docker image fails with 'no space left on device'.\n\nSolution:\nClean up unused images and containers using `docker system prune -a`. Increase disk size or move Docker storage location via `daemon.json` configuration.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0efaa9c56d", "source_url": "dataset://entry/Docker_login_fails_with_'error_storing_credentials", "title": "Docker login fails with 'error storing credentials - err: exit status 1, out: not implemented'.", "text": "Question: Docker login fails with 'error storing credentials - err: exit status 1, out: not implemented'.\n\nSolution:\nThe credential helper is not supported on your OS. Remove or modify the helper entry in `~/.docker/config.json` or install a valid helper like `docker-credential-pass`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 41, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "121bd7fbd1", "source_url": "dataset://entry/Docker_container_can't_resolve_DNS_names.", "title": "Docker container can't resolve DNS names.", "text": "Question: Docker container can't resolve DNS names.\n\nSolution:\nSet DNS servers explicitly with `--dns 8.8.8.8` or configure `/etc/docker/daemon.json` with a valid DNS entry, then restart Docker.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "95bbada7b0", "source_url": "dataset://entry/Docker_build_fails_with_'EACCES:_permission_denied", "title": "Docker build fails with 'EACCES: permission denied, mkdir'.", "text": "Question: Docker build fails with 'EACCES: permission denied, mkdir'.\n\nSolution:\nEnsure build directory permissions allow access to Docker user. Avoid running builds as non-root unless necessary.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5105e69b99", "source_url": "dataset://entry/Docker_Compose_fails_with_version_compatibility_er", "title": "Docker Compose fails with version compatibility errors.", "text": "Question: Docker Compose fails with version compatibility errors.\n\nSolution:\nUpgrade Docker Compose to latest version. Ensure your `version:` field in YAML matches supported schema (e.g., `version: '3.9'`).\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "571b2edc5c", "source_url": "dataset://entry/Docker_container_restarts_repeatedly.", "title": "Docker container restarts repeatedly.", "text": "Question: Docker container restarts repeatedly.\n\nSolution:\nCheck logs with `docker logs <container>`. If restart policy is `always`, fix root cause inside app (e.g., crash) or set `--restart=no` temporarily.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6b0459aed0", "source_url": "dataset://entry/Docker_exec_gives_'container_not_running'_error.", "title": "Docker exec gives 'container not running' error.", "text": "Question: Docker exec gives 'container not running' error.\n\nSolution:\nEnsure container is running: `docker ps -a`. Restart it with `docker start -ai <container>` or use `docker logs` to identify crash reason.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "25fee8648a", "source_url": "dataset://entry/Docker_Swarm_service_stuck_in_Pending_state.", "title": "Docker Swarm service stuck in Pending state.", "text": "Question: Docker Swarm service stuck in Pending state.\n\nSolution:\nCheck available nodes and resource limits. Use `docker service ps <service>` to see placement constraints and ensure images are accessible on all nodes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "92c03b3404", "source_url": "dataset://entry/Docker_logs_command_not_showing_any_output.", "title": "Docker logs command not showing any output.", "text": "Question: Docker logs command not showing any output.\n\nSolution:\nVerify the container is still running. If logs driver is changed, set it to `json-file` or `local`. Use `docker inspect` to check log driver configuration.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "59e19b3377", "source_url": "dataset://entry/Dockerfile_COPY_command_fails_with_'file_not_found", "title": "Dockerfile COPY command fails with 'file not found' though file exists.", "text": "Question: Dockerfile COPY command fails with 'file not found' though file exists.\n\nSolution:\nEnsure paths are relative to the build context. Files outside the build directory cannot be copied. Double-check capitalization on case-sensitive systems.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ff99124553", "source_url": "dataset://entry/Docker_container_can't_write_to_mounted_volume.", "title": "Docker container can't write to mounted volume.", "text": "Question: Docker container can't write to mounted volume.\n\nSolution:\nThe host directory may have restrictive permissions. Adjust ownership using `chown` or mount with appropriate user options.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "57b2522b8b", "source_url": "dataset://entry/Docker_Compose_build_fails_with_'unknown_flag_--mo", "title": "Docker Compose build fails with 'unknown flag --mount'.", "text": "Question: Docker Compose build fails with 'unknown flag --mount'.\n\nSolution:\nYou’re using old Compose version. Update to Docker Compose v2 or newer where BuildKit `--mount` is supported.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b7c9c5c376", "source_url": "dataset://entry/Docker_container_cannot_reach_another_container_by", "title": "Docker container cannot reach another container by name.", "text": "Question: Docker container cannot reach another container by name.\n\nSolution:\nEnsure both containers are on the same user-defined network. Connect them using `docker network connect` or define shared network in Compose.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e96d254bed", "source_url": "dataset://entry/Docker_container_fails_with_'exec_format_error'.", "title": "Docker container fails with 'exec format error'.", "text": "Question: Docker container fails with 'exec format error'.\n\nSolution:\nThe image was built for a different architecture (e.g., arm64 vs amd64). Use the correct platform flag: `--platform linux/amd64` during build or run.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9755f7739f", "source_url": "dataset://entry/Docker_pull_hangs_indefinitely.", "title": "Docker pull hangs indefinitely.", "text": "Question: Docker pull hangs indefinitely.\n\nSolution:\nTry using `--network host` to bypass proxy issues or configure proxy variables in Docker daemon settings. Check for firewall restrictions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "941c5f0289", "source_url": "dataset://entry/Docker_build_fails_with_'Get_https:__registry-1.do", "title": "Docker build fails with 'Get https://registry-1.docker.io/v2/: net/http: request canceled'.", "text": "Question: Docker build fails with 'Get https://registry-1.docker.io/v2/: net/http: request canceled'.\n\nSolution:\nNetwork connectivity or proxy misconfiguration. Configure Docker daemon proxy via `/etc/systemd/system/docker.service.d/http-proxy.conf`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 22, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "2a51b7702c", "source_url": "dataset://entry/Docker_logs_show_'cgroup:_cannot_find_cgroup_mount", "title": "Docker logs show 'cgroup: cannot find cgroup mount destination'.", "text": "Question: Docker logs show 'cgroup: cannot find cgroup mount destination'.\n\nSolution:\nEnsure cgroups are enabled on the host. For WSL2, enable systemd support or upgrade to a version that supports cgroups v2.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a654e270bf", "source_url": "dataset://entry/Docker_container_date_time_incorrect_in_Kubernetes", "title": "Docker container date/time incorrect in Kubernetes.", "text": "Question: Docker container date/time incorrect in Kubernetes.\n\nSolution:\nSync node clocks using NTP. Containers inherit node time; ensure node time is correct and not drifting.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 25, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1cfd3f53bc", "source_url": "dataset://entry/Docker_build_fails_on_CI_CD_with_'no_space_left_on", "title": "Docker build fails on CI/CD with 'no space left on device'.", "text": "Question: Docker build fails on CI/CD with 'no space left on device'.\n\nSolution:\nClean Docker cache in CI pipeline using `docker system prune -af`. Configure ephemeral runners with larger disks.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1a28661cef", "source_url": "dataset://entry/Docker_push_to_private_registry_fails_with_SSL_err", "title": "Docker push to private registry fails with SSL error.", "text": "Question: Docker push to private registry fails with SSL error.\n\nSolution:\nIf using a self-signed cert, add it to Docker’s trusted certificates directory (`/etc/docker/certs.d/<registry>/ca.crt`) and restart Docker.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "58a55296ea", "source_url": "dataset://entry/Docker_image_fails_with_'Segmentation_fault_(core_", "title": "Docker image fails with 'Segmentation fault (core dumped)'.", "text": "Question: Docker image fails with 'Segmentation fault (core dumped)'.\n\nSolution:\nLikely binary incompatibility. Ensure libraries match container architecture and glibc versions. Rebuild image on same architecture as runtime.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "670527f51b", "source_url": "dataset://entry/Docker_Compose_up_fails_with_'Bind_for_0.0.0.0:80_", "title": "Docker Compose up fails with 'Bind for 0.0.0.0:80 failed: port is already allocated'.", "text": "Question: Docker Compose up fails with 'Bind for 0.0.0.0:80 failed: port is already allocated'.\n\nSolution:\nStop existing services occupying port 80 using `sudo lsof -i:80` or change port mapping in docker-compose.yml.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1d6377fa2a", "source_url": "dataset://entry/Docker_container_can't_connect_to_host_service.", "title": "Docker container can't connect to host service.", "text": "Question: Docker container can't connect to host service.\n\nSolution:\nUse host gateway alias: `--add-host=host.docker.internal:host-gateway` or access host IP directly from container.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 21, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9267257e99", "source_url": "dataset://entry/Docker_build_stuck_at_'Sending_build_context'.", "title": "Docker build stuck at 'Sending build context'.", "text": "Question: Docker build stuck at 'Sending build context'.\n\nSolution:\nYour build context is large. Use `.dockerignore` to exclude unnecessary files or move Dockerfile closer to relevant source directory.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "245ea452b9", "source_url": "dataset://entry/Docker_logs_show_'OCI_runtime_create_failed:_conta", "title": "Docker logs show 'OCI runtime create failed: container_linux.go:380: starting container process caus...", "text": "Question: Docker logs show 'OCI runtime create failed: container_linux.go:380: starting container process caused'.\n\nSolution:\nCheck your ENTRYPOINT or CMD. Invalid executable or missing permissions can cause this. Ensure correct shebang line in scripts.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ebbcaea06d", "source_url": "dataset://entry/Dockerfile_FROM_private_registry_image_fails_with_", "title": "Dockerfile FROM private registry image fails with authentication error.", "text": "Question: Dockerfile FROM private registry image fails with authentication error.\n\nSolution:\nCreate a `~/.docker/config.json` with credentials or use `--build-arg` to pass registry credentials securely during build.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "190a15fff5", "source_url": "dataset://entry/Docker_service_memory_limit_not_applied_in_Compose", "title": "Docker service memory limit not applied in Compose.", "text": "Question: Docker service memory limit not applied in Compose.\n\nSolution:\nCompose v3 requires deploying to Swarm mode for `deploy.resources` limits. For local Compose, use `mem_limit` under the service definition.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b2f06a81f9", "source_url": "dataset://entry/Docker_build_fails_with_'error_creating_overlay_mo", "title": "Docker build fails with 'error creating overlay mount'.", "text": "Question: Docker build fails with 'error creating overlay mount'.\n\nSolution:\nFile system may be full or overlay driver misconfigured. Clear unused containers and verify overlayfs support in kernel.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "19c1d898f8", "source_url": "dataset://entry/Docker_container_not_stopping_with_Ctrl+C.", "title": "Docker container not stopping with Ctrl+C.", "text": "Question: Docker container not stopping with Ctrl+C.\n\nSolution:\nForeground process might not handle SIGTERM. Use `--init` flag to include tini as init process to properly handle signals.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "aa6b420732", "source_url": "dataset://entry/Docker_Compose_down_not_removing_networks.", "title": "Docker Compose down not removing networks.", "text": "Question: Docker Compose down not removing networks.\n\nSolution:\nUse `docker-compose down --remove-orphans --volumes --rmi all` to ensure full cleanup of networks, volumes, and images.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 24, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ce59256c79", "source_url": "dataset://entry/Docker_inspect_shows_wrong_IP_address_in_Kubernete", "title": "Docker inspect shows wrong IP address in Kubernetes pod.", "text": "Question: Docker inspect shows wrong IP address in Kubernetes pod.\n\nSolution:\nWhen running under Kubernetes, Docker's IP isn’t pod IP. Use `kubectl get pod -o wide` for correct pod networking details.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ce967deecd", "source_url": "dataset://entry/Docker_build_using_BuildKit_fails_with_'failed_to_", "title": "Docker build using BuildKit fails with 'failed to compute cache key: failed to walk'. How to fix?", "text": "Question: Docker build using BuildKit fails with 'failed to compute cache key: failed to walk'. How to fix?\n\nSolution:\nBuildKit has trouble with large contexts or symlinks. Exclude problematic directories via `.dockerignore` and disable BuildKit temporarily with `DOCKER_BUILDKIT=0` to verify the root cause.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 43, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a22e20bbf9", "source_url": "dataset://entry/Docker_multi-stage_build_fails_when_copying_artifa", "title": "Docker multi-stage build fails when copying artifacts between stages.", "text": "Question: Docker multi-stage build fails when copying artifacts between stages.\n\nSolution:\nEnsure that the intermediate stage has a proper alias like `AS builder` and the COPY command uses `--from=builder`. Also confirm that paths exist in the source stage.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "79168e6468", "source_url": "dataset://entry/Docker_image_verification_with_Notary_fails_with_'", "title": "Docker image verification with Notary fails with 'timestamp expired'.", "text": "Question: Docker image verification with Notary fails with 'timestamp expired'.\n\nSolution:\nUpdate Notary client and reinitialize trust data. Run `docker trust inspect <image>` and `docker trust signer add` to refresh expired timestamp keys.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4b63824698", "source_url": "dataset://entry/Docker_swarm_node_shows_'Node_is_down'_even_though", "title": "Docker swarm node shows 'Node is down' even though it's reachable.", "text": "Question: Docker swarm node shows 'Node is down' even though it's reachable.\n\nSolution:\nThe node might have mismatched Swarm tokens or firewall blocking the gossip port (7946). Rejoin the swarm using correct token and ensure ports 2377, 7946, and 4789 are open.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 42, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e8b3326663", "source_url": "dataset://entry/Docker_container_fails_to_attach_GPU_with_'no_NVID", "title": "Docker container fails to attach GPU with 'no NVIDIA driver found'.", "text": "Question: Docker container fails to attach GPU with 'no NVIDIA driver found'.\n\nSolution:\nInstall NVIDIA Container Toolkit and verify driver version matches host GPU driver. Use `--gpus all` flag and check `nvidia-smi` inside container.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c03b3214d5", "source_url": "dataset://entry/Docker_image_build_reproducibility_differs_across_", "title": "Docker image build reproducibility differs across machines.", "text": "Question: Docker image build reproducibility differs across machines.\n\nSolution:\nEnsure deterministic builds by pinning dependency versions, using BuildKit cache mounts, and normalizing timestamps via `SOURCE_DATE_EPOCH` environment variable.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1db050d1fd", "source_url": "dataset://entry/Docker_container_startup_delayed_due_to_DNS_resolu", "title": "Docker container startup delayed due to DNS resolution timeout.", "text": "Question: Docker container startup delayed due to DNS resolution timeout.\n\nSolution:\nAdd `--dns 8.8.8.8` or configure `/etc/docker/daemon.json` with a reliable DNS. Avoid corporate DNS blocking internal traffic.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f11c2a0bd9", "source_url": "dataset://entry/Docker_build_ARG_variable_not_persisting_between_s", "title": "Docker build ARG variable not persisting between stages.", "text": "Question: Docker build ARG variable not persisting between stages.\n\nSolution:\nARG values are scoped per stage. Redefine `ARG` in each stage or convert to ENV if persistence is required.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6497312fcf", "source_url": "dataset://entry/Docker-compose_containers_not_discovering_each_oth", "title": "Docker-compose containers not discovering each other via service name.", "text": "Question: Docker-compose containers not discovering each other via service name.\n\nSolution:\nEnsure they share the same network in Compose. Explicitly define `networks:` section and confirm each service uses the same named network.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "47baa48ed4", "source_url": "dataset://entry/Docker_container_runs_but_application_reports_'rea", "title": "Docker container runs but application reports 'read-only file system'.", "text": "Question: Docker container runs but application reports 'read-only file system'.\n\nSolution:\nThe container or specific mount is read-only. Check `docker inspect` Mounts configuration and remove `:ro` from volume flags or remount writable.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d18a2ae161", "source_url": "dataset://entry/Docker_build_ARGs_ignored_when_building_on_CI_pipe", "title": "Docker build ARGs ignored when building on CI pipeline.", "text": "Question: Docker build ARGs ignored when building on CI pipeline.\n\nSolution:\nPass args explicitly with `--build-arg` in the CI build command. Environment variables in pipeline are not automatically forwarded to Docker build context.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7423b7c2d8", "source_url": "dataset://entry/Docker_image_built_on_Mac_fails_to_run_on_Linux_se", "title": "Docker image built on Mac fails to run on Linux server with 'exec format error'.", "text": "Question: Docker image built on Mac fails to run on Linux server with 'exec format error'.\n\nSolution:\nImages built on Mac (ARM64) differ from x86 Linux. Specify `--platform linux/amd64` during build or enable `buildx` to produce multi-architecture images.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "bdded2b605", "source_url": "dataset://entry/Docker_Compose_service_depends_on_not_waiting_for_", "title": "Docker Compose service depends_on not waiting for healthcheck.", "text": "Question: Docker Compose service depends_on not waiting for healthcheck.\n\nSolution:\nCompose `depends_on` only manages start order, not readiness. Add `healthcheck` and use `restart: on-failure` or external wait-for scripts to ensure service readiness.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a28c2210d5", "source_url": "dataset://entry/Docker_container_crashes_with_'Out_of_memory'_thou", "title": "Docker container crashes with 'Out of memory' though host has free memory.", "text": "Question: Docker container crashes with 'Out of memory' though host has free memory.\n\nSolution:\nDocker cgroup memory limits might be too restrictive. Increase container memory using `--memory` or check for swap disabled scenarios.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9b8f9dff95", "source_url": "dataset://entry/Docker_build_context_corrupted_with_'tar:_unexpect", "title": "Docker build context corrupted with 'tar: unexpected EOF'.", "text": "Question: Docker build context corrupted with 'tar: unexpected EOF'.\n\nSolution:\nYour context directory has broken symlinks or permission issues. Run `docker build . --no-cache` after fixing permissions or clearing temporary files.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6db5de344e", "source_url": "dataset://entry/Docker_container_networking_breaks_after_VPN_conne", "title": "Docker container networking breaks after VPN connection is established.", "text": "Question: Docker container networking breaks after VPN connection is established.\n\nSolution:\nVPNs modify routing tables. Use `--network host` for testing or create a custom bridge network unaffected by VPN routes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "237ef0c3f5", "source_url": "dataset://entry/Docker_compose_deploy_to_swarm_fails_with_'invalid", "title": "Docker compose deploy to swarm fails with 'invalid mount config for type bind'.", "text": "Question: Docker compose deploy to swarm fails with 'invalid mount config for type bind'.\n\nSolution:\nBind mounts in Swarm require absolute host paths. Replace relative paths with full paths and ensure the directory exists on all swarm nodes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "bea7911f65", "source_url": "dataset://entry/Dockerfile_RUN_apt-get_update_fails_with_'temporar", "title": "Dockerfile RUN apt-get update fails with 'temporary failure resolving archive.ubuntu.com'.", "text": "Question: Dockerfile RUN apt-get update fails with 'temporary failure resolving archive.ubuntu.com'.\n\nSolution:\nThis is a DNS issue inside the build context. Add `--network host` to build command or configure DNS in `daemon.json`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "496b1dca54", "source_url": "dataset://entry/Docker_container_startup_logs_stuck_at_'waiting_fo", "title": "Docker container startup logs stuck at 'waiting for database connection'.", "text": "Question: Docker container startup logs stuck at 'waiting for database connection'.\n\nSolution:\nYour app likely starts before DB readiness. Implement retry logic or use Docker healthcheck and depends_on to delay startup.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f717f42c08", "source_url": "dataset://entry/Docker_swarm_service_fails_to_deploy_with_'no_suit", "title": "Docker swarm service fails to deploy with 'no suitable node (resource constraints)'.", "text": "Question: Docker swarm service fails to deploy with 'no suitable node (resource constraints)'.\n\nSolution:\nIncrease node resource limits or adjust service constraints in Compose using `deploy.resources.reservations` and labels.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7df648f9b9", "source_url": "dataset://entry/Docker_registry_garbage_collection_fails_with_'man", "title": "Docker registry garbage collection fails with 'manifest references a missing blob'.", "text": "Question: Docker registry garbage collection fails with 'manifest references a missing blob'.\n\nSolution:\nRun registry GC with `REGISTRY_STORAGE_DELETE_ENABLED=true` and ensure no ongoing push/pull. Remove orphaned manifest files manually if necessary.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5ee4438329", "source_url": "dataset://entry/Docker_container_startup_too_slow_due_to_many_moun", "title": "Docker container startup too slow due to many mounted volumes.", "text": "Question: Docker container startup too slow due to many mounted volumes.\n\nSolution:\nMinimize number of mounts. Combine mounts where possible or use tmpfs for ephemeral data. Excessive mounts cause overlay driver overhead.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "bbbdf6aa2c", "source_url": "dataset://entry/Docker_network_inspection_shows_stale_containers.", "title": "Docker network inspection shows stale containers.", "text": "Question: Docker network inspection shows stale containers.\n\nSolution:\nRun `docker network prune` to remove unused networks. If network still shows ghosts, restart Docker daemon to refresh overlay cache.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3d4eeecfe4", "source_url": "dataset://entry/Docker_image_push_to_Artifactory_fails_with_'unaut", "title": "Docker image push to Artifactory fails with 'unauthorized: incorrect credentials'.", "text": "Question: Docker image push to Artifactory fails with 'unauthorized: incorrect credentials'.\n\nSolution:\nEnsure credentials are stored in `~/.docker/config.json`. Use `docker login <registry>` and confirm repository permissions match.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a63abbe2c1", "source_url": "dataset://entry/Docker_build_with_COPY_fails_due_to_file_path_exce", "title": "Docker build with COPY fails due to file path exceeding 255 characters.", "text": "Question: Docker build with COPY fails due to file path exceeding 255 characters.\n\nSolution:\nShorten directory names or build context path. Docker uses tar which has filename length limits on some OS filesystems.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c160ac7c0d", "source_url": "dataset://entry/Docker_container_clock_drift_causes_SSL_handshake_", "title": "Docker container clock drift causes SSL handshake failures.", "text": "Question: Docker container clock drift causes SSL handshake failures.\n\nSolution:\nSync container and host clocks with NTP. Mount `/etc/localtime` and `/etc/timezone` or restart the container to resync system time.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f4e9e65262", "source_url": "dataset://entry/Docker_logs_show_'write__dev_stdout:_no_space_left", "title": "Docker logs show 'write /dev/stdout: no space left on device'.", "text": "Question: Docker logs show 'write /dev/stdout: no space left on device'.\n\nSolution:\nThe log driver is full or `/var/lib/docker/containers` partition is full. Use `--log-opt max-size` and `max-file` to limit log rotation.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e54e8249d2", "source_url": "dataset://entry/Docker_build_using_COPY_--chown_fails_on_Windows_h", "title": "Docker build using COPY --chown fails on Windows hosts.", "text": "Question: Docker build using COPY --chown fails on Windows hosts.\n\nSolution:\nThe `--chown` flag is not supported on Windows file systems. Remove it or build on Linux environment.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "959dd53463", "source_url": "dataset://entry/Docker_build_fails_with_'no_matching_manifest_for_", "title": "Docker build fails with 'no matching manifest for linux/amd64 in the manifest list entries'.", "text": "Question: Docker build fails with 'no matching manifest for linux/amd64 in the manifest list entries'.\n\nSolution:\nYour base image lacks an amd64 manifest. Use `--platform` to specify supported architecture or choose a multi-arch base image.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f6bc441aab", "source_url": "dataset://entry/Docker_container_fails_to_bind_to_IPv6_address.", "title": "Docker container fails to bind to IPv6 address.", "text": "Question: Docker container fails to bind to IPv6 address.\n\nSolution:\nEnable IPv6 in Docker daemon configuration by adding `{ \"ipv6\": true, \"fixed-cidr-v6\": \"2001:db8:1::/64\" }` to `daemon.json`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "8246debb72", "source_url": "dataset://entry/Docker_container_starts_but_can't_connect_to_host_", "title": "Docker container starts but can't connect to host database over localhost.", "text": "Question: Docker container starts but can't connect to host database over localhost.\n\nSolution:\nUse `host.docker.internal` instead of `localhost` to access host services from container. Alternatively, use host networking mode.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "8fb9699027", "source_url": "dataset://entry/Docker_image_signing_with_cosign_fails_with_'no_ma", "title": "Docker image signing with cosign fails with 'no matching key found'.", "text": "Question: Docker image signing with cosign fails with 'no matching key found'.\n\nSolution:\nEnsure public/private key pair is available and environment variable `COSIGN_KEY` points to correct key. Re-sign using `cosign sign --key <keyfile>`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "87e32e9eb2", "source_url": "dataset://entry/Docker_system_prune_removes_volumes_unexpectedly.", "title": "Docker system prune removes volumes unexpectedly.", "text": "Question: Docker system prune removes volumes unexpectedly.\n\nSolution:\nAvoid using `--volumes` flag unless intended. Run `docker volume ls` before pruning and use labels to protect critical volumes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b4e1717966", "source_url": "dataset://entry/Docker_compose_build_fails_after_upgrading_to_Comp", "title": "Docker compose build fails after upgrading to Compose V2.", "text": "Question: Docker compose build fails after upgrading to Compose V2.\n\nSolution:\nEnsure the syntax follows latest Compose V2 schema and remove deprecated keys like `links`. Update references to correct CLI (`docker compose`).\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "fb97eff319", "source_url": "dataset://entry/Docker_healthcheck_always_returns_unhealthy_even_t", "title": "Docker healthcheck always returns unhealthy even though service works.", "text": "Question: Docker healthcheck always returns unhealthy even though service works.\n\nSolution:\nHealthcheck command may not match container shell. Use `/bin/sh -c` for proper command evaluation and ensure exit codes reflect success (0).\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9477ef0995", "source_url": "dataset://entry/Docker_container_uses_excessive_disk_space_in__var", "title": "Docker container uses excessive disk space in /var/lib/docker/overlay2.", "text": "Question: Docker container uses excessive disk space in /var/lib/docker/overlay2.\n\nSolution:\nClean up dangling images with `docker image prune -a` and consider enabling BuildKit for more efficient layer deduplication.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "be63b977b9", "source_url": "dataset://entry/Docker_container_fails_to_resolve_internal_hostnam", "title": "Docker container fails to resolve internal hostname in multi-network setup.", "text": "Question: Docker container fails to resolve internal hostname in multi-network setup.\n\nSolution:\nUse service names specific to each network. Containers on different networks require explicit network aliases or links to communicate.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a8d1ad496b", "source_url": "dataset://entry/Docker_build_cache_shared_across_stages_causing_un", "title": "Docker build cache shared across stages causing unexpected artifacts.", "text": "Question: Docker build cache shared across stages causing unexpected artifacts.\n\nSolution:\nUse `--no-cache` for fresh builds or define build stages explicitly with different ARG/ENV to prevent cache collision.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7b4dea56f0", "source_url": "dataset://entry/Docker_service_unable_to_pull_image_from_ECR_with_", "title": "Docker service unable to pull image from ECR with 'no basic auth credentials'.", "text": "Question: Docker service unable to pull image from ECR with 'no basic auth credentials'.\n\nSolution:\nRun `aws ecr get-login-password | docker login --username AWS --password-stdin <aws_account>.dkr.ecr.<region>.amazonaws.com` to authenticate.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "48a0e555ab", "source_url": "dataset://entry/Docker_build_using_SSH_key_fails_with_'no_such_fil", "title": "Docker build using SSH key fails with 'no such file or directory' for /root/.ssh/id_rsa.", "text": "Question: Docker build using SSH key fails with 'no such file or directory' for /root/.ssh/id_rsa.\n\nSolution:\nUse BuildKit SSH forwarding: `docker build --ssh default` and ensure key is loaded in ssh-agent via `ssh-add`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "2cae412c74", "source_url": "dataset://entry/Docker_container_I_O_latency_too_high_under_heavy_", "title": "Docker container I/O latency too high under heavy load.", "text": "Question: Docker container I/O latency too high under heavy load.\n\nSolution:\nSwitch to overlay2 storage driver, enable journaling on host filesystem, and consider using bind mounts for high-performance I/O operations.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "537e693d99", "source_url": "dataset://entry/Docker_Swarm_service_rolling_update_stuck_at_'prep", "title": "Docker Swarm service rolling update stuck at 'preparing'.", "text": "Question: Docker Swarm service rolling update stuck at 'preparing'.\n\nSolution:\nA node might have old images or insufficient resources. Run `docker service ps` and remove stalled tasks manually to resume deployment.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f39db0bb2e", "source_url": "dataset://entry/Docker_push_fails_with_'blob_upload_invalid'.", "title": "Docker push fails with 'blob upload invalid'.", "text": "Question: Docker push fails with 'blob upload invalid'.\n\nSolution:\nRegistry or proxy corrupted upload session. Clear `/var/lib/docker` registry cache or retry push with `--disable-content-trust`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 24, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "781409bef7", "source_url": "dataset://entry/Docker_build_cache_not_shared_across_runners_in_CI", "title": "Docker build cache not shared across runners in CI/CD.", "text": "Question: Docker build cache not shared across runners in CI/CD.\n\nSolution:\nUse BuildKit cache exporter with `--cache-to=type=registry` and `--cache-from` options to share layer cache across builds.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b9e116fa9f", "source_url": "dataset://entry/Docker_daemon_uses_high_memory_after_multiple_buil", "title": "Docker daemon uses high memory after multiple builds.", "text": "Question: Docker daemon uses high memory after multiple builds.\n\nSolution:\nDocker build cache and layer metadata accumulate. Run `docker builder prune -a` regularly to free BuildKit cache memory.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a7148c84e4", "source_url": "dataset://entry/Docker_container_startup_fails_due_to_missing__dev", "title": "Docker container startup fails due to missing /dev/shm mount.", "text": "Question: Docker container startup fails due to missing /dev/shm mount.\n\nSolution:\nIncrease shared memory with `--shm-size=1g` or mount manually: `--mount type=tmpfs,destination=/dev/shm`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 21, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "08657f5c73", "source_url": "dataset://entry/Docker_overlay_network_connectivity_breaks_after_h", "title": "Docker overlay network connectivity breaks after host reboot.", "text": "Question: Docker overlay network connectivity breaks after host reboot.\n\nSolution:\nDocker daemon may restart before network stack stabilizes. Delay Docker startup or restart Docker service manually after boot.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "98dff01e7a", "source_url": "dataset://entry/Docker_container_running_as_non-root_user_can't_bi", "title": "Docker container running as non-root user can't bind to port 80.", "text": "Question: Docker container running as non-root user can't bind to port 80.\n\nSolution:\nPorts below 1024 require root privileges. Change application port or use capability flag `--cap-add=NET_BIND_SERVICE`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "42bbf24d91", "source_url": "dataset://entry/Docker_image_push_fails_intermittently_in_CI_pipel", "title": "Docker image push fails intermittently in CI pipeline.", "text": "Question: Docker image push fails intermittently in CI pipeline.\n\nSolution:\nLikely due to parallel pushes. Serialize push steps or use registry cache. Also verify no rate limiting or unstable network conditions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b6f969967b", "source_url": "dataset://entry/Docker_compose_scale_command_not_updating_containe", "title": "Docker compose scale command not updating container environment variables.", "text": "Question: Docker compose scale command not updating container environment variables.\n\nSolution:\nScaling existing services doesn’t re-evaluate environment variables. Recreate services using `docker compose up -d --force-recreate`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3b88662d8c", "source_url": "dataset://entry/Docker_file_copy_inside_build_ignores_hidden_files", "title": "Docker file copy inside build ignores hidden files.", "text": "Question: Docker file copy inside build ignores hidden files.\n\nSolution:\nBy default, `.dockerignore` might exclude hidden files. Verify patterns and explicitly include hidden files if required using `!.*`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "edcfa360c7", "source_url": "dataset://entry/Docker_container_on_Windows_can't_resolve_Linux_ho", "title": "Docker container on Windows can't resolve Linux hostnames.", "text": "Question: Docker container on Windows can't resolve Linux hostnames.\n\nSolution:\nWindows Docker Desktop uses a VM. Use `host.docker.internal` or configure internal DNS forwarding to access Linux host resources.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6f07951a45", "source_url": "dataset://entry/Pod_stuck_in_Pending_state_in_Kubernetes._What_cou", "title": "Pod stuck in Pending state in Kubernetes. What could be the reason?", "text": "Question: Pod stuck in Pending state in Kubernetes. What could be the reason?\n\nSolution:\nPods remain Pending when no nodes satisfy scheduling constraints. Check taints, node selectors, and resource requests using `kubectl describe pod <pod>`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1f127e0242", "source_url": "dataset://entry/Kubernetes_pod_stuck_in_CrashLoopBackOff._How_to_t", "title": "Kubernetes pod stuck in CrashLoopBackOff. How to troubleshoot?", "text": "Question: Kubernetes pod stuck in CrashLoopBackOff. How to troubleshoot?\n\nSolution:\nCheck logs using `kubectl logs <pod> -p` to inspect previous crashes. Fix the underlying app error and consider adding readiness/liveness probes to handle startup delays.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6d7008cbdf", "source_url": "dataset://entry/Kubernetes_node_shows_'NotReady'_status._What_to_c", "title": "Kubernetes node shows 'NotReady' status. What to check first?", "text": "Question: Kubernetes node shows 'NotReady' status. What to check first?\n\nSolution:\nInspect kubelet logs with `journalctl -u kubelet`. Ensure networking (CNI) is functional and node certificates are valid. Restart kubelet if necessary.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "715c006baf", "source_url": "dataset://entry/Kubernetes_service_not_reachable_externally_using_", "title": "Kubernetes service not reachable externally using NodePort.", "text": "Question: Kubernetes service not reachable externally using NodePort.\n\nSolution:\nVerify service type is NodePort and firewall rules allow traffic. Ensure target pods are Ready and endpoints exist using `kubectl get endpoints <service>`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "357fd57f34", "source_url": "dataset://entry/Pod_cannot_resolve_DNS_names_inside_Kubernetes.", "title": "Pod cannot resolve DNS names inside Kubernetes.", "text": "Question: Pod cannot resolve DNS names inside Kubernetes.\n\nSolution:\nCheck CoreDNS pods in `kube-system` namespace. Restart them if needed. Verify kube-dns config and ensure correct cluster DNS IP in `/etc/resolv.conf`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0531eef50a", "source_url": "dataset://entry/Deployment_rollout_stuck_at_'ProgressDeadlineExcee", "title": "Deployment rollout stuck at 'ProgressDeadlineExceeded'.", "text": "Question: Deployment rollout stuck at 'ProgressDeadlineExceeded'.\n\nSolution:\nYour new pods failed to become ready. Inspect events using `kubectl describe deployment`. Fix readiness probes or image issues before retrying rollout.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f719ef559d", "source_url": "dataset://entry/Kubernetes_pod_stuck_in_'ContainerCreating'.", "title": "Kubernetes pod stuck in 'ContainerCreating'.", "text": "Question: Kubernetes pod stuck in 'ContainerCreating'.\n\nSolution:\nCheck if the image is available and pull secrets are configured. Inspect kubelet logs for CNI or volume mount errors causing delay.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b5670edf5e", "source_url": "dataset://entry/kubectl_command_returns_'connection_refused'_to_AP", "title": "kubectl command returns 'connection refused' to API server.", "text": "Question: kubectl command returns 'connection refused' to API server.\n\nSolution:\nThe API server might be down or kubeconfig misconfigured. Check control plane pods (`kubectl get pods -n kube-system`) or verify `~/.kube/config` cluster endpoint.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a2c9f1bffc", "source_url": "dataset://entry/Pod_running_but_can't_access_another_pod_via_Clust", "title": "Pod running but can't access another pod via ClusterIP.", "text": "Question: Pod running but can't access another pod via ClusterIP.\n\nSolution:\nEnsure both pods are in the same cluster network and no NetworkPolicy is blocking traffic. Use `kubectl exec` to test connectivity using curl or ping.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "8ffa40b5e4", "source_url": "dataset://entry/Kubernetes_ConfigMap_changes_not_reflected_in_runn", "title": "Kubernetes ConfigMap changes not reflected in running pods.", "text": "Question: Kubernetes ConfigMap changes not reflected in running pods.\n\nSolution:\nPods don’t auto-reload updated ConfigMaps. Restart pods or use projected volumes with `subPath` disabled to reflect dynamic changes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c3cd3bdf30", "source_url": "dataset://entry/kubectl_get_pods_shows_'ImagePullBackOff'.", "title": "kubectl get pods shows 'ImagePullBackOff'.", "text": "Question: kubectl get pods shows 'ImagePullBackOff'.\n\nSolution:\nVerify image name, tag, and registry credentials. Run `kubectl describe pod` to view image pull errors and check for private registry secret configuration.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f178706bf5", "source_url": "dataset://entry/PersistentVolumeClaim_stuck_in_Pending_state.", "title": "PersistentVolumeClaim stuck in Pending state.", "text": "Question: PersistentVolumeClaim stuck in Pending state.\n\nSolution:\nCheck if a matching PersistentVolume exists with compatible storage class and access modes. Inspect StorageClass parameters for dynamic provisioning issues.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7acd86fa34", "source_url": "dataset://entry/Pod_terminated_with_'OOMKilled'_status.", "title": "Pod terminated with 'OOMKilled' status.", "text": "Question: Pod terminated with 'OOMKilled' status.\n\nSolution:\nContainer exceeded memory limits. Increase memory limit in the resource spec or optimize application memory usage.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 23, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "17af899417", "source_url": "dataset://entry/Pod_failing_due_to_'CrashLoopBackOff:_Back-off_res", "title": "Pod failing due to 'CrashLoopBackOff: Back-off restarting failed container'.", "text": "Question: Pod failing due to 'CrashLoopBackOff: Back-off restarting failed container'.\n\nSolution:\nInspect container logs and events. If startup dependencies fail, add initContainers or modify liveness probe thresholds to allow more startup time.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "33569c8db6", "source_url": "dataset://entry/Service_type_LoadBalancer_stuck_without_external_I", "title": "Service type LoadBalancer stuck without external IP.", "text": "Question: Service type LoadBalancer stuck without external IP.\n\nSolution:\nEnsure cloud controller manager is running and your cluster is integrated with a supported cloud provider (e.g., AWS, GCP, Azure). Check CCM logs.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c0ed8e5474", "source_url": "dataset://entry/kubectl_apply_fails_with_'forbidden:_User_cannot_p", "title": "kubectl apply fails with 'forbidden: User cannot patch resource'.", "text": "Question: kubectl apply fails with 'forbidden: User cannot patch resource'.\n\nSolution:\nYour user lacks RBAC permissions. Use `kubectl auth can-i patch <resource>` and update ClusterRole or RoleBinding accordingly.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "dd18d3f3f8", "source_url": "dataset://entry/Kubernetes_dashboard_shows_'Unauthorized'.", "title": "Kubernetes dashboard shows 'Unauthorized'.", "text": "Question: Kubernetes dashboard shows 'Unauthorized'.\n\nSolution:\nYou need a valid service account token. Retrieve it using `kubectl -n kubernetes-dashboard create token admin-user` and use it to login.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a505343f18", "source_url": "dataset://entry/Pods_scheduled_on_specific_node_never_start.", "title": "Pods scheduled on specific node never start.", "text": "Question: Pods scheduled on specific node never start.\n\nSolution:\nCheck node taints and ensure pod tolerations match. Inspect kubelet status on the node for runtime or disk space issues.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4835c82d05", "source_url": "dataset://entry/kubectl_logs_shows_'error:_container_<name>_is_ter", "title": "kubectl logs shows 'error: container <name> is terminated'.", "text": "Question: kubectl logs shows 'error: container <name> is terminated'.\n\nSolution:\nAdd `-p` flag to fetch previous container logs. If needed, describe the pod for termination reason and restart policy behavior.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b48a0eb4d9", "source_url": "dataset://entry/Kubernetes_HorizontalPodAutoscaler_not_scaling_pod", "title": "Kubernetes HorizontalPodAutoscaler not scaling pods.", "text": "Question: Kubernetes HorizontalPodAutoscaler not scaling pods.\n\nSolution:\nEnsure metrics-server is deployed and providing CPU/memory metrics. Validate target utilization and HPA configuration with `kubectl get hpa`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 25, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "aa23648b3b", "source_url": "dataset://entry/kubectl_exec_fails_with_'unable_to_upgrade_connect", "title": "kubectl exec fails with 'unable to upgrade connection: container not found'.", "text": "Question: kubectl exec fails with 'unable to upgrade connection: container not found'.\n\nSolution:\nThe pod may have restarted. Re-run `kubectl get pods` to find the new pod name and ensure the target container name matches.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "25bb13733a", "source_url": "dataset://entry/Pod_networking_broken_after_CNI_plugin_upgrade.", "title": "Pod networking broken after CNI plugin upgrade.", "text": "Question: Pod networking broken after CNI plugin upgrade.\n\nSolution:\nCNI binaries might mismatch cluster version. Reinstall or rollback CNI plugin (like Calico, Flannel, or Cilium) and verify `kubectl get pods -n kube-system`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c514afca6c", "source_url": "dataset://entry/Node_disk_pressure_warning_and_pods_being_evicted.", "title": "Node disk pressure warning and pods being evicted.", "text": "Question: Node disk pressure warning and pods being evicted.\n\nSolution:\nFree disk space or increase node storage. Docker or containerd may have stale images; run `kubectl get nodes` to confirm disk pressure status.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "74183790c5", "source_url": "dataset://entry/kube-proxy_crashlooping_in_kube-system_namespace.", "title": "kube-proxy crashlooping in kube-system namespace.", "text": "Question: kube-proxy crashlooping in kube-system namespace.\n\nSolution:\nCheck ConfigMap `kube-proxy` for malformed configuration. Reset it or redeploy daemonset using correct API server and cluster CIDR values.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "66ba57a2dc", "source_url": "dataset://entry/Kubernetes_Ingress_not_routing_traffic.", "title": "Kubernetes Ingress not routing traffic.", "text": "Question: Kubernetes Ingress not routing traffic.\n\nSolution:\nEnsure ingress controller (e.g., NGINX) is deployed. Verify ingress rules and service backend health. Use `kubectl describe ingress` to inspect routing.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a0d9a1e29a", "source_url": "dataset://entry/Pod_stuck_in_Terminating_state_for_long_time.", "title": "Pod stuck in Terminating state for long time.", "text": "Question: Pod stuck in Terminating state for long time.\n\nSolution:\nForce delete using `kubectl delete pod <name> --grace-period=0 --force`. Check for finalizers or volume unmount issues in describe output.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d8c53d8e24", "source_url": "dataset://entry/kubectl_delete_namespace_stuck_in_Terminating_stat", "title": "kubectl delete namespace stuck in Terminating state.", "text": "Question: kubectl delete namespace stuck in Terminating state.\n\nSolution:\nA resource with finalizers prevents deletion. Use `kubectl get namespace <ns> -o json | jq .spec.finalizers` and patch to remove finalizers manually.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a968ced337", "source_url": "dataset://entry/Pods_fail_with_'MountVolume.SetUp_failed_for_volum", "title": "Pods fail with 'MountVolume.SetUp failed for volume'.", "text": "Question: Pods fail with 'MountVolume.SetUp failed for volume'.\n\nSolution:\nCheck PersistentVolume and StorageClass driver compatibility. Inspect kubelet logs for CSI driver errors or missing node permissions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3ec927a89e", "source_url": "dataset://entry/kubectl_rollout_undo_fails_with_'no_revision_found", "title": "kubectl rollout undo fails with 'no revision found'.", "text": "Question: kubectl rollout undo fails with 'no revision found'.\n\nSolution:\nDeployment history not available because previous ReplicaSets were cleaned up. Enable `revisionHistoryLimit` > 0 to retain versions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b561ef1d7d", "source_url": "dataset://entry/Service_discovery_fails_between_namespaces.", "title": "Service discovery fails between namespaces.", "text": "Question: Service discovery fails between namespaces.\n\nSolution:\nUse fully qualified DNS names like `<service>.<namespace>.svc.cluster.local`. Check if NetworkPolicy blocks cross-namespace communication.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 20, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6673156ac2", "source_url": "dataset://entry/kubelet_logs_show_'x509:_certificate_has_expired_o", "title": "kubelet logs show 'x509: certificate has expired or is not yet valid'.", "text": "Question: kubelet logs show 'x509: certificate has expired or is not yet valid'.\n\nSolution:\nRenew certificates with `kubeadm certs renew all` and restart control plane components. Verify time synchronization across nodes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d6aa1d36b3", "source_url": "dataset://entry/kubectl_get_nodes_shows_some_nodes_with_'Schedulin", "title": "kubectl get nodes shows some nodes with 'SchedulingDisabled'.", "text": "Question: kubectl get nodes shows some nodes with 'SchedulingDisabled'.\n\nSolution:\nNode is cordoned. Run `kubectl uncordon <node>` to allow scheduling again.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 21, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d5445f8c50", "source_url": "dataset://entry/Pods_fail_with_'too_many_open_files'_error.", "title": "Pods fail with 'too many open files' error.", "text": "Question: Pods fail with 'too many open files' error.\n\nSolution:\nIncrease ulimit settings in container spec via securityContext or adjust node OS limits for kubelet and container runtime.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7215a5f649", "source_url": "dataset://entry/kubectl_port-forward_fails_with_'error_upgrading_c", "title": "kubectl port-forward fails with 'error upgrading connection'.", "text": "Question: kubectl port-forward fails with 'error upgrading connection'.\n\nSolution:\nPort-forward requires `kubectl proxy` permissions and open connection to API server. Check firewall and ensure the pod is in Running state.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f4317f128f", "source_url": "dataset://entry/Service_has_endpoints_but_requests_time_out.", "title": "Service has endpoints but requests time out.", "text": "Question: Service has endpoints but requests time out.\n\nSolution:\nCheck network policies and kube-proxy mode. If using IPVS, ensure kernel modules `ip_vs` and `ip_vs_rr` are loaded.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4b3e3bb0d9", "source_url": "dataset://entry/Pod_stuck_in_Init_state.", "title": "Pod stuck in Init state.", "text": "Question: Pod stuck in Init state.\n\nSolution:\nInspect initContainer logs using `kubectl logs <pod> -c <init-container>`. Initialization script or dependency likely failing.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 22, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3965737f7d", "source_url": "dataset://entry/Kubernetes_Deployment_shows_more_pods_than_desired", "title": "Kubernetes Deployment shows more pods than desired.", "text": "Question: Kubernetes Deployment shows more pods than desired.\n\nSolution:\nPrevious ReplicaSets might not be cleaned. Reduce `revisionHistoryLimit` or manually delete old ReplicaSets.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 22, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "45b9b05ea6", "source_url": "dataset://entry/kubectl_top_nodes_returns_'metrics_not_available'.", "title": "kubectl top nodes returns 'metrics not available'.", "text": "Question: kubectl top nodes returns 'metrics not available'.\n\nSolution:\nMetrics server not running or misconfigured. Deploy metrics-server and ensure API aggregation layer is enabled in the API server.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "85cb03caf4", "source_url": "dataset://entry/Pod_reports_'permission_denied'_when_writing_to_vo", "title": "Pod reports 'permission denied' when writing to volume.", "text": "Question: Pod reports 'permission denied' when writing to volume.\n\nSolution:\nSet correct securityContext with `fsGroup` or adjust volume permissions using initContainer before mounting.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 23, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c98ab0d5a6", "source_url": "dataset://entry/Kubernetes_secret_not_being_mounted_into_pod.", "title": "Kubernetes secret not being mounted into pod.", "text": "Question: Kubernetes secret not being mounted into pod.\n\nSolution:\nEnsure secret exists in same namespace and mount paths are correct. Restart pod if secret was created after pod start.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6e7fd26fdf", "source_url": "dataset://entry/kubectl_get_events_shows_'FailedMount'_repeatedly.", "title": "kubectl get events shows 'FailedMount' repeatedly.", "text": "Question: kubectl get events shows 'FailedMount' repeatedly.\n\nSolution:\nVolume driver may not support node zone. Verify StorageClass zone configuration and ensure CSI driver pods are healthy.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f81225befc", "source_url": "dataset://entry/Node_reports_'kubelet:_failed_to_get_cgroup_stats_", "title": "Node reports 'kubelet: failed to get cgroup stats for pod'.", "text": "Question: Node reports 'kubelet: failed to get cgroup stats for pod'.\n\nSolution:\nCgroups might be corrupted. Restart kubelet and container runtime, or reboot node if metrics system is stuck.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3e86ac4f70", "source_url": "dataset://entry/Pods_not_scaling_even_after_updating_replica_count", "title": "Pods not scaling even after updating replica count.", "text": "Question: Pods not scaling even after updating replica count.\n\nSolution:\nCheck Deployment selector labels. If they mismatch template labels, new replicas won’t be created.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 24, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "54cf5449e1", "source_url": "dataset://entry/kubectl_describe_pod_shows_'Back-off_pulling_image", "title": "kubectl describe pod shows 'Back-off pulling image'.", "text": "Question: kubectl describe pod shows 'Back-off pulling image'.\n\nSolution:\nRegistry unreachable or DNS misconfigured. Ensure image name correctness and add imagePullSecrets if private registry is used.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ca7d328471", "source_url": "dataset://entry/CoreDNS_pods_CrashLoopBackOff_after_upgrade.", "title": "CoreDNS pods CrashLoopBackOff after upgrade.", "text": "Question: CoreDNS pods CrashLoopBackOff after upgrade.\n\nSolution:\nCorefile syntax may be outdated. Edit ConfigMap and verify `forward` plugin syntax matches your cluster DNS setup.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 24, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f2d15eb390", "source_url": "dataset://entry/kubectl_proxy_command_fails_with_'could_not_get_se", "title": "kubectl proxy command fails with 'could not get server version'.", "text": "Question: kubectl proxy command fails with 'could not get server version'.\n\nSolution:\nCheck API server health and kubeconfig context. Ensure cluster endpoint URL is reachable from your client machine.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "774ac0b54a", "source_url": "dataset://entry/Pod_stuck_at_'Evicted'_status.", "title": "Pod stuck at 'Evicted' status.", "text": "Question: Pod stuck at 'Evicted' status.\n\nSolution:\nNode ran out of resources. Use `kubectl describe pod` for eviction reason. Delete and recreate pod once node pressure clears.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "26964dc2fb", "source_url": "dataset://entry/Kubernetes_ingress_returns_404_on_all_routes.", "title": "Kubernetes ingress returns 404 on all routes.", "text": "Question: Kubernetes ingress returns 404 on all routes.\n\nSolution:\nBackend service name or port mismatch in ingress rule. Verify service targetPort and ingress annotation syntax.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 25, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "dbfa93a1b8", "source_url": "dataset://entry/DaemonSet_pods_not_running_on_all_nodes.", "title": "DaemonSet pods not running on all nodes.", "text": "Question: DaemonSet pods not running on all nodes.\n\nSolution:\nNode selectors, taints, or tolerations may exclude nodes. Inspect DaemonSet events and ensure matchLabels align with node labels.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f0cb43bce6", "source_url": "dataset://entry/Kubernetes_API_server_high_CPU_usage.", "title": "Kubernetes API server high CPU usage.", "text": "Question: Kubernetes API server high CPU usage.\n\nSolution:\nExcessive watch events or metrics queries. Reduce logging verbosity and ensure metrics scraping intervals are optimized.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 24, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a7c6fa6dd4", "source_url": "dataset://entry/kubectl_drain_node_fails_with_'cannot_evict_pod_ma", "title": "kubectl drain node fails with 'cannot evict pod managed by DaemonSet'.", "text": "Question: kubectl drain node fails with 'cannot evict pod managed by DaemonSet'.\n\nSolution:\nUse `--ignore-daemonsets` flag when draining nodes. DaemonSet pods are managed automatically after drain completes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f794e1f8de", "source_url": "dataset://entry/AKS:_Pods_can’t_pull_images_from_Azure_Container_R", "title": "AKS: Pods can’t pull images from Azure Container Registry (ACR). What’s the fix?", "text": "Question: AKS: Pods can’t pull images from Azure Container Registry (ACR). What’s the fix?\n\nSolution:\nGrant the AKS node pool’s managed identity AcrPull on the ACR (or link ACR during cluster create). Run: `az role assignment create --assignee <nodepool_MI_id> --role AcrPull --scope <acr_id>`. For legacy SP clusters, `az aks update -n <cluster> -g <rg> --attach-acr <acrName>`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 56, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "229bfbce4a", "source_url": "dataset://entry/EKS:_New_pods_stuck_in_Pending_with_'insufficient_", "title": "EKS: New pods stuck in Pending with 'insufficient pods' though nodes have CPU/Memory. Why?", "text": "Question: EKS: New pods stuck in Pending with 'insufficient pods' though nodes have CPU/Memory. Why?\n\nSolution:\nEKS VPC CNI IP exhaustion. Each pod needs an ENI IP. Increase IPs per node by using larger instance types, enable prefix delegation, or set `WARM_IP_TARGET`. Alternatively, use secondary CIDRs or adjust maxPods with `--max-pods` and CNI config.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 54, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4bc1af6e55", "source_url": "dataset://entry/AKS:_LoadBalancer_service_never_gets_a_public_IP.", "title": "AKS: LoadBalancer service never gets a public IP.", "text": "Question: AKS: LoadBalancer service never gets a public IP.\n\nSolution:\nYour cluster uses 'outboundType: userDefinedRouting' or a restricted subnet/NSG. Ensure AKS service principal/managed identity has rights on the public IP and LB resource group. Confirm `service.beta.kubernetes.io/azure-load-balancer-ipv4` annotations and that the node resource group exists and is writable.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 47, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1e288fe0c8", "source_url": "dataset://entry/EKS:_kubectl_errors_'You_must_be_logged_in_to_the_", "title": "EKS: kubectl errors 'You must be logged in to the server (Unauthorized)'.", "text": "Question: EKS: kubectl errors 'You must be logged in to the server (Unauthorized)'.\n\nSolution:\nYour IAM user/role isn’t mapped in `aws-auth` ConfigMap. Add the ARN to `mapUsers`/`mapRoles`, apply the ConfigMap, and ensure you’re assuming the correct role with `aws sts get-caller-identity`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 41, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4042fae6ce", "source_url": "dataset://entry/AKS:_Private_cluster;_kubectl_cannot_reach_API_ser", "title": "AKS: Private cluster; kubectl cannot reach API server from local machine.", "text": "Question: AKS: Private cluster; kubectl cannot reach API server from local machine.\n\nSolution:\nPrivate AKS exposes API only on VNet. Use a jumpbox/ExpressRoute/VPN/Private Endpoint. Alternatively enable 'publicFQDN' with authorized IP ranges for limited public access.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "05dc152490", "source_url": "dataset://entry/EKS:_AWS_Load_Balancer_Controller_(ALB)_doesn’t_cr", "title": "EKS: AWS Load Balancer Controller (ALB) doesn’t create an ALB for an Ingress.", "text": "Question: EKS: AWS Load Balancer Controller (ALB) doesn’t create an ALB for an Ingress.\n\nSolution:\nController IAM permissions or OIDC/IRSA missing. Ensure OIDC provider is set (`eksctl utils associate-iam-oidc-provider`), create IAM policy and service account annotation, and verify IngressClass is `alb` with valid `kubernetes.io/ingress.class: alb` or new `ingressClassName`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 48, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c67877d8f6", "source_url": "dataset://entry/AKS:_Cluster_Autoscaler_not_scaling_out_though_pod", "title": "AKS: Cluster Autoscaler not scaling out though pods are Pending.", "text": "Question: AKS: Cluster Autoscaler not scaling out though pods are Pending.\n\nSolution:\nNode pool scaling disabled or constraints. Ensure `--cluster-autoscaler-profile` configured, node taints/tolerations align, max node count > current, and pending pods request resources that fit VM size. Check `k8s-cluster-autoscaler` logs in kube-system.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 43, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "2bd021afef", "source_url": "dataset://entry/EKS:_Cluster_Autoscaler_logs_'failed_to_build_node", "title": "EKS: Cluster Autoscaler logs 'failed to build node group' and doesn’t scale.", "text": "Question: EKS: Cluster Autoscaler logs 'failed to build node group' and doesn’t scale.\n\nSolution:\nUsing unmanaged/self-managed nodes without proper ASG tags. Tag the ASG with `k8s.io/cluster-autoscaler/enabled` and `k8s.io/cluster-autoscaler/<cluster-name>` and provide IAM permissions to describe/scale the ASG.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "306962d068", "source_url": "dataset://entry/AKS:_Pods_can’t_resolve_external_DNS_names.", "title": "AKS: Pods can’t resolve external DNS names.", "text": "Question: AKS: Pods can’t resolve external DNS names.\n\nSolution:\nCoreDNS upstream blocked or VNet DNS override. Update CoreDNS ConfigMap `forward . /etc/resolv.conf` or point to Azure DNS (168.63.129.16). Ensure VNet custom DNS allows recursion and NSG/UDR permits outbound 53/UDP/TCP.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 39, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b7b8de424f", "source_url": "dataset://entry/EKS:_NodeGroup_creation_fails_with_'AccessDenied'_", "title": "EKS: NodeGroup creation fails with 'AccessDenied' on IAM.", "text": "Question: EKS: NodeGroup creation fails with 'AccessDenied' on IAM.\n\nSolution:\nNode role lacks permissions or you used a restricted service-linked role. Attach `AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`, and `AmazonEC2ContainerRegistryReadOnly` to the node instance role, and ensure the cluster role trusts `eks.amazonaws.com`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "cf2f475a12", "source_url": "dataset://entry/AKS:_New_node_pool_join_fails;_nodes_stuck_NotRead", "title": "AKS: New node pool join fails; nodes stuck NotReady with CNI errors.", "text": "Question: AKS: New node pool join fails; nodes stuck NotReady with CNI errors.\n\nSolution:\nSubnet out of IPs or Azure CNI overlay misconfigured. Expand subnet or enable Azure CNI Overlay, ensure `podCIDR`/`serviceCIDR` don’t overlap with VNet peering ranges. Verify NSG allows intra-subnet traffic.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 43, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a83daf605d", "source_url": "dataset://entry/EKS:_Pods_can’t_reach_Internet_though_nodes_can.", "title": "EKS: Pods can’t reach Internet though nodes can.", "text": "Question: EKS: Pods can’t reach Internet though nodes can.\n\nSolution:\nNAT/Gateway or routing issue. For private subnets, ensure NAT Gateway is attached and route tables for ENI subnets have 0.0.0.0/0 to NAT. Verify security group egress rules and Network ACLs.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 40, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "acbb25ee87", "source_url": "dataset://entry/AKS:_CSI_Azure_Disk_PVC_stuck_in_Pending.", "title": "AKS: CSI Azure Disk PVC stuck in Pending.", "text": "Question: AKS: CSI Azure Disk PVC stuck in Pending.\n\nSolution:\nStorageClass zone/sku mismatch or disk encryption set. Ensure `managed-csi` driver installed, correct `skuName` and `zones` match node pool zones. For encryption-at-host or CMK, match cluster configuration and permissions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "414e2244c7", "source_url": "dataset://entry/EKS:_EBS_CSI_driver_PV_attach_fails_with_'Node_not", "title": "EKS: EBS CSI driver PV attach fails with 'Node not found in AZ'.", "text": "Question: EKS: EBS CSI driver PV attach fails with 'Node not found in AZ'.\n\nSolution:\nVolume and node AZ mismatch. Ensure StorageClass uses `topology.kubernetes.io/zone` and your node group spans the same AZs. Use zonal StorageClass or a Multi-AZ capable storage like EFS for cross-AZ mounts.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 45, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ed5e526609", "source_url": "dataset://entry/AKS:_Azure_Files_PVC_mounts_but_app_gets_'permissi", "title": "AKS: Azure Files PVC mounts but app gets 'permission denied'.", "text": "Question: AKS: Azure Files PVC mounts but app gets 'permission denied'.\n\nSolution:\nSamba permissions and UID/GID mapping. Use `mountOptions` (e.g., `dir_mode`, `file_mode`), set `fsGroup` in Pod securityContext, or use Azure Files NFS with proper POSIX permissions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7939314d83", "source_url": "dataset://entry/EKS:_Pods_randomly_drop_connections_across_nodes.", "title": "EKS: Pods randomly drop connections across nodes.", "text": "Question: EKS: Pods randomly drop connections across nodes.\n\nSolution:\nSecurity group for pods or incorrect SG rules with VPC CNI SG-for-Pods feature. Ensure inter-pod SG allows required ports and that `AWS_VPC_K8S_CNI_EXTERNALSNAT` is correct for your NAT design.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f428df09ba", "source_url": "dataset://entry/AKS:_NGINX_Ingress_Controller_returns_502_504_inte", "title": "AKS: NGINX Ingress Controller returns 502/504 intermittently.", "text": "Question: AKS: NGINX Ingress Controller returns 502/504 intermittently.\n\nSolution:\nBackend readiness/health probes or SNAT port exhaustion. Increase `max-connections`, tune `proxy-read-timeout`, enable `tcpReuse`. Consider using Azure NAT Gateway to increase outbound SNAT ports.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4d8a5550e9", "source_url": "dataset://entry/EKS:_ALB_targets_remain_'unhealthy'.", "title": "EKS: ALB targets remain 'unhealthy'.", "text": "Question: EKS: ALB targets remain 'unhealthy'.\n\nSolution:\nTarget type or health check mismatch. If using IP mode, ensure pods are reachable from the ALB subnets, correct security groups, and health checks path/port align with Service `targetPort`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "09378f15ae", "source_url": "dataset://entry/AKS:_Upgrading_cluster_version_fails_with_'operati", "title": "AKS: Upgrading cluster version fails with 'operation not allowed on agentPool'.", "text": "Question: AKS: Upgrading cluster version fails with 'operation not allowed on agentPool'.\n\nSolution:\nNode pool has unsupported SKU or max surge invalid. Set `--max-surge` to a valid value, ensure VMSS supports upgrade, and remove deprecated preview features before upgrade.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 39, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b44ec7e505", "source_url": "dataset://entry/EKS:_kubectl_hangs_until_MFA_expires_or_times_out.", "title": "EKS: kubectl hangs until MFA expires or times out.", "text": "Question: EKS: kubectl hangs until MFA expires or times out.\n\nSolution:\nAWS CLI creds expire; update kubeconfig with a role that can be assumed and configure MFA caching. Use `aws eks update-kubeconfig --role-arn ...` and renew sessions prior to using kubectl.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 41, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f1e6b11bed", "source_url": "dataset://entry/AKS:_PodIdentity_(AAD_Pod_Identity)_stopped_workin", "title": "AKS: PodIdentity (AAD Pod Identity) stopped working after upgrade.", "text": "Question: AKS: PodIdentity (AAD Pod Identity) stopped working after upgrade.\n\nSolution:\nAAD Pod Identity is deprecated; migrate to **Azure AD Workload Identity**. Install the webhook, create a Federated Identity Credential for the service account, and update apps to use token exchange.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 41, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "13077c5455", "source_url": "dataset://entry/EKS:_IRSA_configured_but_pods_still_use_node_IAM_r", "title": "EKS: IRSA configured but pods still use node IAM role.", "text": "Question: EKS: IRSA configured but pods still use node IAM role.\n\nSolution:\nServiceAccount missing `eks.amazonaws.com/role-arn` annotation or wrong audiences. Ensure OIDC provider is present, SA annotated with correct role, and the role trust policy allows the service account issuer & namespace.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 41, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "331e33814d", "source_url": "dataset://entry/AKS:_Node_pool_autoscaling_stuck_at_minimum_size.", "title": "AKS: Node pool autoscaling stuck at minimum size.", "text": "Question: AKS: Node pool autoscaling stuck at minimum size.\n\nSolution:\nScale set API throttling or pending pod constraints. Check Cluster Autoscaler logs for 'NoExpanders' or 'Unschedulable'. Increase `maxCount`, remove restrictive nodeSelectors, and ensure quotas are not exceeded at subscription level.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 40, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "50c401f06c", "source_url": "dataset://entry/EKS:_Fargate_pods_can’t_pull_private_images_from_E", "title": "EKS: Fargate pods can’t pull private images from ECR.", "text": "Question: EKS: Fargate pods can’t pull private images from ECR.\n\nSolution:\nFargate profile uses pod IAM role (via IRSA) that needs `ecr:GetAuthorizationToken` and related ECR permissions. Ensure the correct subnets are selected and route to ECR endpoints via NAT or VPC endpoints.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 42, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c17285f688", "source_url": "dataset://entry/AKS:_CoreDNS_CrashLoopBackOff_after_enabling_Priva", "title": "AKS: CoreDNS CrashLoopBackOff after enabling Private DNS zones.", "text": "Question: AKS: CoreDNS CrashLoopBackOff after enabling Private DNS zones.\n\nSolution:\nLoop or blocked upstream. Point CoreDNS forwarders to Azure DNS (168.63.129.16) or the Azure DNS Private Resolver. Avoid forwarding back into the same private zone causing recursion.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7f627fcead", "source_url": "dataset://entry/EKS:_NodeGroup_creation_via_eksctl_fails_with_'ins", "title": "EKS: NodeGroup creation via eksctl fails with 'insufficient capacity'.", "text": "Question: EKS: NodeGroup creation via eksctl fails with 'insufficient capacity'.\n\nSolution:\nThe chosen instance type is constrained. Use `--instance-types` with multiple options (e.g., m5,m5a,m5n) and enable capacity-optimized Spot or pick on-demand in available AZs.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "273138c5a2", "source_url": "dataset://entry/AKS:_NetworkPolicies_not_enforced.", "title": "AKS: NetworkPolicies not enforced.", "text": "Question: AKS: NetworkPolicies not enforced.\n\nSolution:\nUsing kubenet or policy addon disabled. Ensure you’re on Azure CNI (or Azure CNI Overlay) with a supported network policy (Calico/Azure NP) and the policy add-on is enabled.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0538a02f35", "source_url": "dataset://entry/EKS:_Pods_Scheduling_fails_due_to_PodSecurityPolic", "title": "EKS: Pods Scheduling fails due to PodSecurityPolicy after upgrade.", "text": "Question: EKS: Pods Scheduling fails due to PodSecurityPolicy after upgrade.\n\nSolution:\nPSP is deprecated/removed. Migrate to Pod Security Admission (PSA) or Gatekeeper policies and adjust namespace labels (`pod-security.kubernetes.io/*`).\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6cfb6ced4f", "source_url": "dataset://entry/AKS:_Outbound_connections_fail_intermittently_unde", "title": "AKS: Outbound connections fail intermittently under load.", "text": "Question: AKS: Outbound connections fail intermittently under load.\n\nSolution:\nSNAT port exhaustion on Standard LB outbound. Attach a NAT Gateway or assign multiple outbound IPs to the LB; reduce per-pod outbound bursts via connection pooling.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "66ca86476d", "source_url": "dataset://entry/EKS:_'cni_failed_to_attach_ENI'_in_aws-node_logs.", "title": "EKS: 'cni failed to attach ENI' in aws-node logs.", "text": "Question: EKS: 'cni failed to attach ENI' in aws-node logs.\n\nSolution:\nIAM policy missing for CNI or subnet lacks free IPs. Attach `AmazonEKS_CNI_Policy`, ensure secondary CIDR/IPAM correct, and verify ENI quotas aren’t exhausted.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f33b039548", "source_url": "dataset://entry/AKS:_kubectl_times_out_when_using_Azure_AD_auth.", "title": "AKS: kubectl times out when using Azure AD auth.", "text": "Question: AKS: kubectl times out when using Azure AD auth.\n\nSolution:\nExpired token or context mismatch. Re-login with `az login`, refresh the kubeconfig via `az aks get-credentials --overwrite-existing`, and ensure your AAD group is bound in RBAC.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e29b5ba426", "source_url": "dataset://entry/EKS:_Private_cluster_endpoint;_nodes_can’t_join.", "title": "EKS: Private cluster endpoint; nodes can’t join.", "text": "Question: EKS: Private cluster endpoint; nodes can’t join.\n\nSolution:\nControl plane endpoint private requires node bootstrap from within VPC. Ensure node subnets can reach the endpoint (via VPC endpoints) and that security groups allow 443 to API endpoint.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1362e18af4", "source_url": "dataset://entry/AKS:_Azure_Policy_add-on_blocks_deployments_unexpe", "title": "AKS: Azure Policy add-on blocks deployments unexpectedly.", "text": "Question: AKS: Azure Policy add-on blocks deployments unexpectedly.\n\nSolution:\nA policy or initiative denies resources (e.g., 'deny elevated privileges'). Review `Gatekeeper` or Azure Policy events and exempt namespaces or update constraints as needed.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "05d3272975", "source_url": "dataset://entry/EKS:_NLB_for_a_LoadBalancer_Service_doesn’t_get_a_", "title": "EKS: NLB for a LoadBalancer Service doesn’t get a static IP.", "text": "Question: EKS: NLB for a LoadBalancer Service doesn’t get a static IP.\n\nSolution:\nNLB is L4 and uses per-AZ IPs; allocate Elastic IPs via annotations (`service.beta.kubernetes.io/aws-load-balancer-eip-allocations`) or switch to ALB for HTTP(S) with static hostnames.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "44c0bda6cc", "source_url": "dataset://entry/AKS:_Disk_attach_limit_reached_on_nodes.", "title": "AKS: Disk attach limit reached on nodes.", "text": "Question: AKS: Disk attach limit reached on nodes.\n\nSolution:\nVM SKU has max data disks. Spread pods across nodes, use Azure Files/EFS-like shared storage, or choose VM sizes with higher disk limits.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3dc9fb2dd7", "source_url": "dataset://entry/EKS:_Prometheus_scraping_the_API_server_fails_with", "title": "EKS: Prometheus scraping the API server fails with 403.", "text": "Question: EKS: Prometheus scraping the API server fails with 403.\n\nSolution:\nRBAC and endpoint access. Enable control plane logging/metrics, create RBAC allowing metrics scraping, or use `kubelet`/node exporters instead of direct API server scraping.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9fa3a5efa5", "source_url": "dataset://entry/AKS:_Node_images_outdated_causing_kubelet_mismatch", "title": "AKS: Node images outdated causing kubelet mismatch.", "text": "Question: AKS: Node images outdated causing kubelet mismatch.\n\nSolution:\nUpgrade node image via `az aks nodepool upgrade --node-image-only` per pool. Then do a full cluster version upgrade to align control plane and nodes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "322fc35323", "source_url": "dataset://entry/EKS:_Cluster_upgrade_stuck_at_'updating_nodegroup'", "title": "EKS: Cluster upgrade stuck at 'updating nodegroup'.", "text": "Question: EKS: Cluster upgrade stuck at 'updating nodegroup'.\n\nSolution:\nDraining blocked by PDBs or DaemonSets. Temporarily relax PDBs, use `kubectl drain --ignore-daemonsets --delete-emptydir-data`, then resume the nodegroup update.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ea0c20af27", "source_url": "dataset://entry/AKS:_Ingress_with_Application_Gateway_(AGIC)_doesn", "title": "AKS: Ingress with Application Gateway (AGIC) doesn’t route traffic.", "text": "Question: AKS: Ingress with Application Gateway (AGIC) doesn’t route traffic.\n\nSolution:\nAGIC identity lacks permissions on the App Gateway or wrong subnet. Assign `Contributor` on the App Gateway resource group, ensure proper `ingressClassName: azure/application-gateway` and health probes succeed.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "80563f6c93", "source_url": "dataset://entry/EKS:_aws-iam-authenticator_'x509:_certificate_sign", "title": "EKS: aws-iam-authenticator 'x509: certificate signed by unknown authority'.", "text": "Question: EKS: aws-iam-authenticator 'x509: certificate signed by unknown authority'.\n\nSolution:\nOut-of-date CA bundle or clock skew. Update CA certs on client, ensure time sync (NTP), and refresh kubeconfig with `aws eks update-kubeconfig`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "24deb69cb8", "source_url": "dataset://entry/AKS:_Pod-to-pod_connectivity_broken_across_node_po", "title": "AKS: Pod-to-pod connectivity broken across node pools.", "text": "Question: AKS: Pod-to-pod connectivity broken across node pools.\n\nSolution:\nNSG or UDR blocks traffic between subnets, or different VNets without peering. Ensure NSGs allow intra-subnet and inter-subnet traffic and that VNet peering is configured without 'UseRemoteGateways' conflicts.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d4d3e6865c", "source_url": "dataset://entry/EKS:_Calico_network_policy_logs_not_visible.", "title": "EKS: Calico network policy logs not visible.", "text": "Question: EKS: Calico network policy logs not visible.\n\nSolution:\nAWS VPC CNI by default; Calico policy only works if its dataplane is active. Either enable Calico enforcement (BPF mode with CNI compat) or use AWS Network Policy for VPC CNI where supported.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 42, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7cd4ac291f", "source_url": "dataset://entry/AKS:_CSI_Snapshot_fails_to_create_VolumeSnapshotCo", "title": "AKS: CSI Snapshot fails to create VolumeSnapshotContent.", "text": "Question: AKS: CSI Snapshot fails to create VolumeSnapshotContent.\n\nSolution:\nInstall the snapshot CRDs and controller matching your Kubernetes version. Ensure your CSI driver supports snapshots and the `VolumeSnapshotClass` points to it.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "785aa6ee41", "source_url": "dataset://entry/EKS:_EFS_CSI_mount_stalls_at_'permission_denied'.", "title": "EKS: EFS CSI mount stalls at 'permission denied'.", "text": "Question: EKS: EFS CSI mount stalls at 'permission denied'.\n\nSolution:\nEFS access point POSIX permissions or SG/NACLs. Allow NFS (2049) between node SGs and EFS mount target SGs, use an Access Point with correct UID/GID, and set pod `fsGroup`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 39, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9b5f3eece6", "source_url": "dataset://entry/AKS:_HPA_not_scaling;_metrics.k8s.io_not_available", "title": "AKS: HPA not scaling; metrics.k8s.io not available.", "text": "Question: AKS: HPA not scaling; metrics.k8s.io not available.\n\nSolution:\nEnable the metrics-server add-on or deploy it manually. Ensure API aggregation is functioning and the metrics-server has `--kubelet-insecure-tls` if necessary in private clusters.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ffafcee3dc", "source_url": "dataset://entry/EKS:_CoreDNS_CrashLoop_after_enabling_stubDomains_", "title": "EKS: CoreDNS CrashLoop after enabling stubDomains for on-prem DNS.", "text": "Question: EKS: CoreDNS CrashLoop after enabling stubDomains for on-prem DNS.\n\nSolution:\nDNS loop or unreachable upstream. Verify VPC resolver rules, ensure conditional forwarders route correctly over VPN/Direct Connect, and reduce query timeouts.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "39c97e7857", "source_url": "dataset://entry/AKS:_'QuotaExceeded'_when_creating_node_pool.", "title": "AKS: 'QuotaExceeded' when creating node pool.", "text": "Question: AKS: 'QuotaExceeded' when creating node pool.\n\nSolution:\nAzure subscription or regional quota reached for VM cores or public IPs. Request quota increases for VM series and IPs, or choose another region/VM size.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d732cd6867", "source_url": "dataset://entry/EKS:_Admission_webhook_timeouts_block_deployments.", "title": "EKS: Admission webhook timeouts block deployments.", "text": "Question: EKS: Admission webhook timeouts block deployments.\n\nSolution:\nWebhook endpoint not reachable from API server (private networking). Place webhooks behind a reachable NLB/ALB, use DNS resolvable name, and set reasonable `timeoutSeconds` and `failurePolicy`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "8303014276", "source_url": "dataset://entry/AKS:_Pod_identities_intermittently_fail_to_get_tok", "title": "AKS: Pod identities intermittently fail to get tokens (workload identity).", "text": "Question: AKS: Pod identities intermittently fail to get tokens (workload identity).\n\nSolution:\nFederated identity conditions mismatch. Ensure service account issuer matches the federated credential, subject format `system:serviceaccount:<ns>:<sa>`, and time sync between AKS and AAD.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b1690dd973", "source_url": "dataset://entry/EKS:_'AccessDenied'_when_AWS_LB_Controller_manages", "title": "EKS: 'AccessDenied' when AWS LB Controller manages TargetGroups.", "text": "Question: EKS: 'AccessDenied' when AWS LB Controller manages TargetGroups.\n\nSolution:\nController role missing `elasticloadbalancing:*` permissions. Attach the official AWS policy JSON to the IRSA role and ensure correct tags for TG ownership.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e9d7878d78", "source_url": "dataset://entry/AKS:_Outbound_to_specific_SaaS_is_blocked_though_g", "title": "AKS: Outbound to specific SaaS is blocked though general Internet works.", "text": "Question: AKS: Outbound to specific SaaS is blocked though general Internet works.\n\nSolution:\nFirewall/NSG or Azure FW DNAT rules. Add FQDN tags or explicit rules to allow the SaaS endpoints, verify proxy configuration in CoreDNS/Pods, and consider Private Endpoints if supported.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 41, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6a4f1133cc", "source_url": "dataset://entry/EKS:_Pods_cannot_reach_kube-dns_intermittently.", "title": "EKS: Pods cannot reach kube-dns intermittently.", "text": "Question: EKS: Pods cannot reach kube-dns intermittently.\n\nSolution:\nNode-local DNS cache not deployed or SG rules block 53/UDP between pods and CoreDNS. Deploy NodeLocal DNSCache for stability and ensure SG allows DNS traffic.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "649236c852", "source_url": "dataset://entry/AKS:_'ImagePullBackOff'_for_images_in_another_tena", "title": "AKS: 'ImagePullBackOff' for images in another tenant’s ACR.", "text": "Question: AKS: 'ImagePullBackOff' for images in another tenant’s ACR.\n\nSolution:\nCross-tenant ACR access requires guest permissions or pull through ACR-to-ACR replication. Establish Azure AD B2B or use a shared private link + role assignments for the node identity.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 38, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c88a0419c7", "source_url": "dataset://entry/EKS:_Bottlerocket_node_group_fails_to_join_cluster", "title": "EKS: Bottlerocket node group fails to join cluster.", "text": "Question: EKS: Bottlerocket node group fails to join cluster.\n\nSolution:\nBootstrap settings/AMI mismatch. Use the official Bottlerocket EKS AMI, set correct `bootstrap.sh` settings via user data or config, and ensure IMDS and required IAM policies are enabled.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "aaf5d32994", "source_url": "dataset://entry/Kubernetes_control_plane_became_unresponsive_after", "title": "Kubernetes control plane became unresponsive after etcd compaction. API requests time out. What’s ha...", "text": "Question: Kubernetes control plane became unresponsive after etcd compaction. API requests time out. What’s happening?\n\nSolution:\netcd compaction removed keys still in watch caches. Restart kube-apiserver to rebuild caches and check etcd health via `etcdctl endpoint health`. If corruption persists, restore etcd from latest snapshot.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 45, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "dc08fa92a4", "source_url": "dataset://entry/After_upgrading_Kubernetes_from_1.26_to_1.28,_all_", "title": "After upgrading Kubernetes from 1.26 to 1.28, all CRDs backed by conversion webhooks fail to serve.", "text": "Question: After upgrading Kubernetes from 1.26 to 1.28, all CRDs backed by conversion webhooks fail to serve.\n\nSolution:\nConversion webhook API version changed or TLS cert expired. Check webhook endpoint health, update API version to `v1`, and rotate serving certs if expired.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 42, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c543df3b13", "source_url": "dataset://entry/Pods_across_namespaces_randomly_lose_network_conne", "title": "Pods across namespaces randomly lose network connectivity for seconds every few hours.", "text": "Question: Pods across namespaces randomly lose network connectivity for seconds every few hours.\n\nSolution:\nARP cache exhaustion or conntrack table overflow. Increase `net.netfilter.nf_conntrack_max` and node sysctl, deploy conntrack cleaner DaemonSet, and monitor network saturation.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "29af5c30bf", "source_url": "dataset://entry/After_restoring_etcd_snapshot,_cluster_boots_but_n", "title": "After restoring etcd snapshot, cluster boots but no pods are scheduled.", "text": "Question: After restoring etcd snapshot, cluster boots but no pods are scheduled.\n\nSolution:\nThe scheduler lost lease data in etcd. Delete `kube-scheduler` leases in `kube-system`, restart controller-manager and scheduler to recreate coordination leases.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f192d5f14c", "source_url": "dataset://entry/One_node_constantly_flaps_between_Ready_and_NotRea", "title": "One node constantly flaps between Ready and NotReady with kubelet log 'cgroup driver mismatch'.", "text": "Question: One node constantly flaps between Ready and NotReady with kubelet log 'cgroup driver mismatch'.\n\nSolution:\nContainer runtime uses systemd while kubelet expects cgroupfs. Align drivers in kubelet config (`cgroupDriver: systemd`) or switch containerd configuration accordingly.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0ed1ececb0", "source_url": "dataset://entry/CPU_utilization_in_API_server_spikes_when_many_CRD", "title": "CPU utilization in API server spikes when many CRDs installed.", "text": "Question: CPU utilization in API server spikes when many CRDs installed.\n\nSolution:\nList/watch load from CRDs. Enable API priority and fairness, set proper cache size, or move infrequently accessed CRDs to aggregated API servers to isolate load.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3708710460", "source_url": "dataset://entry/All_pods_with_initContainers_stuck_in_Init:0_1_aft", "title": "All pods with initContainers stuck in Init:0/1 after upgrading to containerd 1.7.", "text": "Question: All pods with initContainers stuck in Init:0/1 after upgrading to containerd 1.7.\n\nSolution:\nCRI plugin race condition. Restart containerd, check sandbox creation logs, and ensure cni-plugins binary path matches kubelet config. Upgrade to patched containerd version.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ef28f817cf", "source_url": "dataset://entry/Pods_using_hostPath_volumes_start_failing_after_no", "title": "Pods using hostPath volumes start failing after node disk cleanup.", "text": "Question: Pods using hostPath volumes start failing after node disk cleanup.\n\nSolution:\nGarbage collection removed files hostPath depended on. Migrate hostPath usage to PersistentVolumes or bind mounts under controlled directories outside kubelet-managed paths.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "97e13af337", "source_url": "dataset://entry/etcd_leader_election_thrashes_every_few_seconds_ca", "title": "etcd leader election thrashes every few seconds causing API latency.", "text": "Question: etcd leader election thrashes every few seconds causing API latency.\n\nSolution:\nOne etcd node suffers I/O or network latency. Replace slow node, verify same CPU and disk IOPS, check NTP sync across control-plane nodes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c2f6e3c1ee", "source_url": "dataset://entry/Ingress_controller_stopped_reconciling_routes_afte", "title": "Ingress controller stopped reconciling routes after large configmap changes.", "text": "Question: Ingress controller stopped reconciling routes after large configmap changes.\n\nSolution:\nIngress controller hit rate limit on Kubernetes API. Increase informer resync period, enable caching, or use external datastore (like Redis) for dynamic config reloads.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f64761fc28", "source_url": "dataset://entry/Pods_continuously_evicted_due_to_node_'PIDPressure", "title": "Pods continuously evicted due to node 'PIDPressure'.", "text": "Question: Pods continuously evicted due to node 'PIDPressure'.\n\nSolution:\nToo many short-lived processes. Increase node PID limit (`--pod-max-pids` or `/proc/sys/kernel/pid_max`) or tune app process model to reuse workers instead of spawning per-request processes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5aebd55d99", "source_url": "dataset://entry/Admission_webhook_blocks_all_new_pod_creations_wit", "title": "Admission webhook blocks all new pod creations with 500 error.", "text": "Question: Admission webhook blocks all new pod creations with 500 error.\n\nSolution:\nWebhook backend unavailable or invalid TLS chain. Check webhook service health, ensure CA bundle matches service cert, and update ValidatingWebhookConfiguration.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6a04e61a93", "source_url": "dataset://entry/After_adding_Calico_BPF_mode,_DNS_requests_start_f", "title": "After adding Calico BPF mode, DNS requests start failing intermittently.", "text": "Question: After adding Calico BPF mode, DNS requests start failing intermittently.\n\nSolution:\nNode-local-dns conflicts with Calico BPF policy enforcement. Exclude CoreDNS namespace from BPF dataplane or set `CALICO_IPV4POOL_BPFENABLED=false` temporarily.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4e4e43e901", "source_url": "dataset://entry/Cluster_metrics_show_sudden_spike_in_etcd_DB_size_", "title": "Cluster metrics show sudden spike in etcd DB size though no new objects created.", "text": "Question: Cluster metrics show sudden spike in etcd DB size though no new objects created.\n\nSolution:\nLeaked lease objects or stale events. Compact etcd manually (`etcdctl compact`) and defragment (`etcdctl defrag`) to reclaim space.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 34, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "292f67c7c7", "source_url": "dataset://entry/Pods_using_projected_service_account_tokens_cannot", "title": "Pods using projected service account tokens cannot reach external APIs.", "text": "Question: Pods using projected service account tokens cannot reach external APIs.\n\nSolution:\nBound ServiceAccount tokens expire fast. Update app to reload tokens dynamically or increase `--service-account-issuer` TTL in kube-apiserver.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e604181ce6", "source_url": "dataset://entry/kube-controller-manager_logs_show_'Failed_to_acqui", "title": "kube-controller-manager logs show 'Failed to acquire leader lease'.", "text": "Question: kube-controller-manager logs show 'Failed to acquire leader lease'.\n\nSolution:\nMultiple controllers competing with same identity. Ensure unique `--leader-elect-lease-name` or fix RBAC so only one controller instance accesses the lease resource.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6161720813", "source_url": "dataset://entry/Upgraded_CoreDNS_from_1.8_to_1.11_and_now_external", "title": "Upgraded CoreDNS from 1.8 to 1.11 and now external DNS queries time out.", "text": "Question: Upgraded CoreDNS from 1.8 to 1.11 and now external DNS queries time out.\n\nSolution:\nUpstream resolver misconfigured. Edit Corefile to include `forward . /etc/resolv.conf` and verify node-level DNS works. Roll back config if plugin chain changed.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 37, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ab2a790f66", "source_url": "dataset://entry/API_server_requests_hang_during_large_ConfigMap_up", "title": "API server requests hang during large ConfigMap update.", "text": "Question: API server requests hang during large ConfigMap update.\n\nSolution:\nConfigMap objects exceed size limit causing slow serialization. Move large binary data to a volume or external store, or increase `maxRequestBodyBytes` parameter.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0b68092639", "source_url": "dataset://entry/Node_reboot_causes_pods_with_local_storage_to_cras", "title": "Node reboot causes pods with local storage to crash permanently.", "text": "Question: Node reboot causes pods with local storage to crash permanently.\n\nSolution:\nEphemeral local PVs lost after reboot. Use StatefulSets with persistent storage or use `local-storage` CSI driver to recreate local PVs bound to nodes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "67ba416895", "source_url": "dataset://entry/Network_policies_suddenly_block_all_traffic_after_", "title": "Network policies suddenly block all traffic after CNI restart.", "text": "Question: Network policies suddenly block all traffic after CNI restart.\n\nSolution:\nCNI cache lost network state. Reapply network policies, ensure the CNI plugin (e.g. Calico) reconciler pod is healthy, and reload iptables/eBPF maps.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ff8e8d1234", "source_url": "dataset://entry/Cluster_upgrade_stuck_due_to_PodDisruptionBudget_v", "title": "Cluster upgrade stuck due to PodDisruptionBudget violations.", "text": "Question: Cluster upgrade stuck due to PodDisruptionBudget violations.\n\nSolution:\nCritical workloads set PDB with `minAvailable=1` and only 1 replica. Temporarily remove or adjust PDB, complete upgrade, and restore later.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5c4c5ca6d2", "source_url": "dataset://entry/Pods_with_sidecars_never_terminate,_causing_job_co", "title": "Pods with sidecars never terminate, causing job completion delays.", "text": "Question: Pods with sidecars never terminate, causing job completion delays.\n\nSolution:\nSidecar containers keep process alive. Use Kubernetes 'Sidecar container' feature (1.28+) or add preStop hooks and shared termination signals via postStart script.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 33, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "fcb79307e9", "source_url": "dataset://entry/All_kubelet_logs_show_'certificate_expired'.", "title": "All kubelet logs show 'certificate expired'.", "text": "Question: All kubelet logs show 'certificate expired'.\n\nSolution:\nNode bootstrap cert rotation failed. Manually delete old certs in `/var/lib/kubelet/pki`, restart kubelet, and approve CSR with `kubectl certificate approve`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5960b6cb62", "source_url": "dataset://entry/Scheduler_logs_'No_fit_found_for_pod'_even_though_", "title": "Scheduler logs 'No fit found for pod' even though nodes have resources.", "text": "Question: Scheduler logs 'No fit found for pod' even though nodes have resources.\n\nSolution:\nPod anti-affinity, topology spread, or taints prevent placement. Inspect pod events, check required labels, and temporarily disable anti-affinity to confirm root cause.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 36, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1036e4dc8a", "source_url": "dataset://entry/Pods_using_initContainers_that_download_files_over", "title": "Pods using initContainers that download files over HTTPS fail with 'x509 unknown authority'.", "text": "Question: Pods using initContainers that download files over HTTPS fail with 'x509 unknown authority'.\n\nSolution:\nCA bundle missing. Mount `/etc/ssl/certs` from host or include `ca-certificates` in initContainer image.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7568ea1799", "source_url": "dataset://entry/Custom_controllers_using_informers_cause_excessive", "title": "Custom controllers using informers cause excessive etcd writes.", "text": "Question: Custom controllers using informers cause excessive etcd writes.\n\nSolution:\nController creates unnecessary updates. Use `Patch` instead of `Update`, reduce resync frequency, and debounce reconcile loops to avoid high etcd churn.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a97ba7d518", "source_url": "dataset://entry/Pod_sandbox_creation_fails_after_installing_seccom", "title": "Pod sandbox creation fails after installing seccomp profile policies.", "text": "Question: Pod sandbox creation fails after installing seccomp profile policies.\n\nSolution:\nDefault seccomp profile blocks syscalls required by container runtime. Adjust seccomp profile or disable via `seccompProfile: type: Unconfined` temporarily.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ce17486b09", "source_url": "dataset://entry/High_API_latency_after_enabling_audit_logging.", "title": "High API latency after enabling audit logging.", "text": "Question: High API latency after enabling audit logging.\n\nSolution:\nSynchronous audit backend causing I/O bottleneck. Use buffered webhook backend or asynchronous mode, and set `--audit-batch-max-size` to balance throughput.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "bcdca0bcf5", "source_url": "dataset://entry/kubelet_memory_usage_continuously_grows_without_dr", "title": "kubelet memory usage continuously grows without dropping.", "text": "Question: kubelet memory usage continuously grows without dropping.\n\nSolution:\nImage GC or pod metrics leak. Restart kubelet periodically, enable `--housekeeping-interval`, and check cadvisor heap profiles for leaks.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5b7fbabb38", "source_url": "dataset://entry/Rolling_updates_take_hours_though_few_pods_exist.", "title": "Rolling updates take hours though few pods exist.", "text": "Question: Rolling updates take hours though few pods exist.\n\nSolution:\nReadiness probes delay rollout due to strict thresholds. Tune `failureThreshold` and probe intervals, or parallelize rollout using `maxUnavailable` in Deployment strategy.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "8b556b7d32", "source_url": "dataset://entry/Pods_can't_resolve_custom_DNS_zones_after_migratio", "title": "Pods can't resolve custom DNS zones after migration to CoreDNS.", "text": "Question: Pods can't resolve custom DNS zones after migration to CoreDNS.\n\nSolution:\nMissing stubDomain entry in ConfigMap. Add `stubDomains` pointing to custom DNS servers and reload CoreDNS pods.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "5a5b1967bc", "source_url": "dataset://entry/Webhook-based_OPA_Gatekeeper_blocks_all_traffic_wi", "title": "Webhook-based OPA Gatekeeper blocks all traffic with 'timeout'.", "text": "Question: Webhook-based OPA Gatekeeper blocks all traffic with 'timeout'.\n\nSolution:\nGatekeeper audit webhook under load. Increase webhook timeoutSeconds, allocate more replicas, and ensure webhook service DNS resolves correctly inside cluster.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "d73547ce89", "source_url": "dataset://entry/Cluster-autoscaler_scales_nodes_but_pending_pods_s", "title": "Cluster-autoscaler scales nodes but pending pods still not scheduled.", "text": "Question: Cluster-autoscaler scales nodes but pending pods still not scheduled.\n\nSolution:\nPods require more ephemeral-storage than available. Add ephemeral storage requests/limits or switch node type with larger ephemeral disk.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "61b96ef994", "source_url": "dataset://entry/Pods_crash_with_'Invalid_CrossDeviceLink'_during_v", "title": "Pods crash with 'Invalid CrossDeviceLink' during volume mount.", "text": "Question: Pods crash with 'Invalid CrossDeviceLink' during volume mount.\n\nSolution:\nContainerd snapshotter type incompatible with overlay filesystem. Use `overlayfs` snapshotter or switch to fuse-overlayfs compatible configuration.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "dc4a196b16", "source_url": "dataset://entry/Pod_logs_disappear_after_restart_though_logging_ag", "title": "Pod logs disappear after restart though logging agent is running.", "text": "Question: Pod logs disappear after restart though logging agent is running.\n\nSolution:\nLogs stored in ephemeral container filesystem. Configure stdout/stderr log driver to persist or mount hostPath `/var/log/pods` to centralized log collector.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "730cf9886e", "source_url": "dataset://entry/Cluster_access_from_CI_CD_fails_with_'x509:_certif", "title": "Cluster access from CI/CD fails with 'x509: certificate signed by unknown authority'.", "text": "Question: Cluster access from CI/CD fails with 'x509: certificate signed by unknown authority'.\n\nSolution:\nCA certificate changed post-rotation. Update CI kubeconfig with new cluster CA bundle, or regenerate service account token with correct root CA.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 35, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "455abc65a5", "source_url": "dataset://entry/Pods_experience_30s_startup_delays_after_CNI_plugi", "title": "Pods experience 30s startup delays after CNI plugin migration.", "text": "Question: Pods experience 30s startup delays after CNI plugin migration.\n\nSolution:\nOld IPAM caches remain. Clear `/var/lib/cni` and restart kubelet. Validate new CNI binary path and IPAM plugin configuration for correct version.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 32, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "afa08f4a0a", "source_url": "dataset://entry/etcd_disk_fills_rapidly_even_after_compaction.", "title": "etcd disk fills rapidly even after compaction.", "text": "Question: etcd disk fills rapidly even after compaction.\n\nSolution:\nLarge number of Events or Leases not garbage-collected. Enable TTL controller, prune expired events, and check for controllers spamming resource updates.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "60ff1b8af2", "source_url": "dataset://entry/kube-scheduler_crashlooping_with_'nil_pointer_dere", "title": "kube-scheduler crashlooping with 'nil pointer dereference'.", "text": "Question: kube-scheduler crashlooping with 'nil pointer dereference'.\n\nSolution:\nCustom scheduler config malformed. Validate `kubescheduler.config.k8s.io/v1` syntax and remove plugin weights causing invalid config merge.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 23, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1d8e51b562", "source_url": "dataset://entry/API_server_crashes_under_load_with_'too_many_open_", "title": "API server crashes under load with 'too many open files'.", "text": "Question: API server crashes under load with 'too many open files'.\n\nSolution:\nFile descriptor limit too low. Raise `LimitNOFILE` in systemd service for kube-apiserver and adjust ulimit for all control-plane processes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c1b96da12f", "source_url": "dataset://entry/Kubernetes_jobs_intermittently_stuck_in_'Active'_w", "title": "Kubernetes jobs intermittently stuck in 'Active' with completed pods.", "text": "Question: Kubernetes jobs intermittently stuck in 'Active' with completed pods.\n\nSolution:\nJob controller race. Enable job controller `ttlSecondsAfterFinished` cleanup, delete finalizers manually, or upgrade to Kubernetes ≥1.25 where bug is fixed.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "842d5a738e", "source_url": "dataset://entry/Pod_eviction_storms_when_nodes_under_IO_pressure.", "title": "Pod eviction storms when nodes under IO pressure.", "text": "Question: Pod eviction storms when nodes under IO pressure.\n\nSolution:\nEnable `NodePressureEviction` tuning via kubelet `--eviction-hard` thresholds. Investigate disk I/O saturation using iostat and node logs.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 26, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "49fa7cc8e1", "source_url": "dataset://entry/CRD_webhook_API_degraded_after_adding_new_version_", "title": "CRD webhook API degraded after adding new version to conversion chain.", "text": "Question: CRD webhook API degraded after adding new version to conversion chain.\n\nSolution:\nConversion webhook fails version negotiation. Ensure `storageVersion` matches served version and conversion strategy supports both v1beta1 and v1.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "10f1484af9", "source_url": "dataset://entry/Pod_stuck_'Terminating'_because_finalizer_never_re", "title": "Pod stuck 'Terminating' because finalizer never removed.", "text": "Question: Pod stuck 'Terminating' because finalizer never removed.\n\nSolution:\nCustom controller hung or deleted. Manually patch the resource to remove finalizers and investigate controller logs for stuck reconcile loops.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0f20a175f2", "source_url": "dataset://entry/Kubelet_reports_'NodeHasNetworkUnavailable'.", "title": "Kubelet reports 'NodeHasNetworkUnavailable'.", "text": "Question: Kubelet reports 'NodeHasNetworkUnavailable'.\n\nSolution:\nCNI plugin failed initialization. Restart kubelet, ensure `/opt/cni/bin` populated, and network interface created by plugin exists. Inspect `/var/log/cni.log`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 23, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "118f32469d", "source_url": "dataset://entry/PersistentVolume_detachment_stuck_for_terminated_p", "title": "PersistentVolume detachment stuck for terminated pods.", "text": "Question: PersistentVolume detachment stuck for terminated pods.\n\nSolution:\nCSI driver not responding to `NodeUnpublishVolume`. Restart node plugin DaemonSet, ensure `volumeAttachment` objects reconcile, and clear orphaned attachments manually.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 27, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "14d9683141", "source_url": "dataset://entry/Metrics_API_reports_negative_CPU_usage_for_pods.", "title": "Metrics API reports negative CPU usage for pods.", "text": "Question: Metrics API reports negative CPU usage for pods.\n\nSolution:\nMetrics-server rounding bug or stale cadvisor data. Restart metrics-server, ensure `--kubelet-preferred-address-types` set to InternalIP, and upgrade to fixed version.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 29, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ce329c5e2b", "source_url": "dataset://entry/API_latency_spikes_every_5_minutes.", "title": "API latency spikes every 5 minutes.", "text": "Question: API latency spikes every 5 minutes.\n\nSolution:\nController-manager full resyncs. Reduce resync interval, increase API QPS limits, and check CRDs with heavy informer watches causing periodic load.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1c71b86754", "source_url": "dataset://entry/Kubernetes_audit_logs_missing_for_some_requests.", "title": "Kubernetes audit logs missing for some requests.", "text": "Question: Kubernetes audit logs missing for some requests.\n\nSolution:\nAudit policy misconfigured with exclude rules too broad. Review audit policy YAML and ensure correct stages (`RequestResponse`) and non-blocking backends configured.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 30, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b2c09327f9", "source_url": "dataset://entry/Pods_on_ARM_nodes_fail_image_pull_with_'manifest_n", "title": "Pods on ARM nodes fail image pull with 'manifest not found'.", "text": "Question: Pods on ARM nodes fail image pull with 'manifest not found'.\n\nSolution:\nImage built for amd64 only. Use multi-arch manifest or specify platform during build (`docker buildx build --platform linux/arm64,linux/amd64`).\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7c0333d111", "source_url": "dataset://entry/Ingress_Controller_fails_TLS_handshake_for_certain", "title": "Ingress Controller fails TLS handshake for certain domains.", "text": "Question: Ingress Controller fails TLS handshake for certain domains.\n\nSolution:\nIngress secret missing intermediate CA or mismatch in CN/SAN. Recreate secret with complete certificate chain and confirm domain matches CN/SAN entries.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 31, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4b0c26481f", "source_url": "dataset://entry/Node-level_kube-proxy_dies_after_enabling_IPVS.", "title": "Node-level kube-proxy dies after enabling IPVS.", "text": "Question: Node-level kube-proxy dies after enabling IPVS.\n\nSolution:\nIPVS kernel modules not loaded. Run `modprobe ip_vs` and `ip_vs_rr`. Update kube-proxy ConfigMap to include mode: ipvs and reload daemonset.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 28, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "320565124a", "source_url": "dataset://entry/A_production_Kubernetes_cluster_(1.28)_intermitten", "title": "A production Kubernetes cluster (1.28) intermittently returns 5xx for public APIs behind an NGINX In...", "text": "Question: A production Kubernetes cluster (1.28) intermittently returns 5xx for public APIs behind an NGINX Ingress. Spikes align with heavy deployments and ConfigMap reloads. You also see elevated latency in kube-apiserver and CoreDNS. How do you isolate the bottleneck and stabilize traffic during control-plane churn?\n\nSolution:\nStart by separating data-plane from control-plane symptoms. \n\n1) Observe ingress dataplane: Check NGINX Ingress Controller metrics (requests, p50/p90/p99 latency, reload events). If reload storms exist, enable `--enable-dynamic-certificates` and `--update-status-on-shutdown=false`. Consider using a dedicated configuration ConfigMap and setting `worker-shutdown-timeout` for graceful reloads. \n2) CoreDNS: Inspect `kubectl -n kube-system logs deploy/coredns` for timeouts. Enable NodeLocal DNSCache to reduce upstream lookups; verify Corefile plugins (`cache`, `forward`) and ensure `forward . /etc/resolv.conf` or the required upstream resolvers are reachable. \n3) API server pressure: Enable API Priority and Fairness (APF) if not already; tune controller watches that cause large object lists (e.g., CRDs). Use audit logs and apiserver metrics (`apiserver_request_duration_seconds`) to identify expensive endpoints. \n4) Deployment storm mitigation: Roll deployments with `maxUnavailable` > 0 but reasonable; stagger rollouts, and enable HorizontalPodAutoscaler stabilization windows to avoid thrashing. \n5) Ingress config reload strategy: Use NGINX `--election-id` for leader election stability; for massive config changes, switch to canary IngressClass to warm new config before flipping traffic. \n6) Network: Validate ENI/SNAT capacity (cloud) or conntrack limits (`nf_conntrack_max`). \n7) Verification: Run synthetic probes during a canary rollout; confirm p99 stabilizes before full rollout. Combine these to smooth reload-induced jitter and reduce control-plane coupling with the data plane.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 243, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "34fe4c25db", "source_url": "dataset://entry/A_multi-tenant_cluster_uses_Calico_in_eBPF_mode._A", "title": "A multi-tenant cluster uses Calico in eBPF mode. After enabling strict NetworkPolicies and migrating...", "text": "Question: A multi-tenant cluster uses Calico in eBPF mode. After enabling strict NetworkPolicies and migrating tenants to separate namespaces, some services randomly become unreachable across namespaces. Packet captures show dropped traffic with no matching deny rules. What’s the systematic approach?\n\nSolution:\nThis pattern often indicates policy ordering, namespace selectors, or eBPF program conflicts. \n\n1) Policy audit: Use `calicoctl get networkpolicy -o yaml` and verify namespaceSelectors resolve to intended namespaces. Check for default-deny in each namespace and confirm `egress` rules for DNS and observability sidecars. \n2) eBPF verification: `calico-node` logs may reveal map sync failures. Ensure kernel supports required eBPF features and that BPF programs were reattached after node upgrades (`bpftool prog show`). \n3) Identity resolution: If you use service accounts or labels in `networkPolicy`, confirm label propagation via admission webhooks; a delayed label change can cause transient drops. \n4) Cross-namespace services: Prefer `namespaceSelector`+`podSelector` combos and ensure FQDN policies or egress allow to kube-dns. \n5) Rollback plan: Temporarily switch Calico to iptables dataplane on a subset of nodes to confirm dataplane issue vs. policy logic. \n6) Observability: Enable Calico flow logs; determine which rule matched the drop. Adjust policies to include necessary `egress`/`ingress` to cluster services, and add explicit allowlists for control-plane DNS/metrics endpoints.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 203, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "c1af619104", "source_url": "dataset://entry/After_a_control-plane_node_disk_failure,_etcd_was_", "title": "After a control-plane node disk failure, etcd was restored from a snapshot. The cluster came back, b...", "text": "Question: After a control-plane node disk failure, etcd was restored from a snapshot. The cluster came back, but HorizontalPodAutoscalers and Leases behave oddly: some controllers never reacquire leader election and new objects sporadically fail with 'resource version too old'. How do you heal the control plane without full redeploy?\n\nSolution:\nYou're seeing watch cache inconsistencies post-restore. \n\n1) Verify etcd health: `etcdctl endpoint status --write-out=table` and `etcdctl alarm list`. Clear any NO_SPACE or CORRUPT alarms. Run a compaction (`etcdctl compact <rev>`) followed by `etcdctl defrag` to reclaim space. \n2) API server restart loop: Restart kube-apiserver to rebuild in-memory watch caches. Stagger restarts to keep quorum available. \n3) Lease cleanup: List leases in `kube-system` and controller namespaces. If leases are stale, delete them (e.g., `kubectl delete lease -n kube-system kube-scheduler`) so controllers can reacquire leadership. \n4) HPA and metrics chain: Validate metrics-server (`kubectl get apiservice v1beta1.metrics.k8s.io -o yaml` for STATUS). If APIServices show discovery failures, restart metrics-server and aggregation layer. \n5) Admission webhooks: Ensure their endpoints are healthy; stale endpoints can block create/updates. \n6) Post-restore consistency: Run conformance checks (e.g., `sonobuoy`) and verify controllers (scheduler, controller-manager) reconcile loops stabilize (look for reduction in 'resource version too old' warnings).\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 196, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "545863cb19", "source_url": "dataset://entry/A_StatefulSet_with_50_shards_uses_local_NVMe_via_t", "title": "A StatefulSet with 50 shards uses local NVMe via the local-static-provisioner. After a kernel update...", "text": "Question: A StatefulSet with 50 shards uses local NVMe via the local-static-provisioner. After a kernel update + node reboots, some shards fail to mount with 'device busy' and others mount but show silent data corruption. What’s your recovery and future-proofing plan?\n\nSolution:\nLocal PVs bind pods to specific disks; reboots can leave stale mounts or file system issues. \n\nImmediate recovery: \n1) Quarantine nodes with failing shards: cordon and drain (respecting PDBs). \n2) On each problematic node: unmount stale mounts; check `lsblk`, `mount`, `lsof` to identify processes holding devices. \n3) Run fsck (ext4/xfs_repair) as appropriate. If corruption is severe, restore from the most recent verified backup (consider per-shard snapshots). \n4) Validate udev rules and device naming stability; pin by UUID rather than `/dev/nvmeXnY`. \n\nFuture-proofing: \n1) Use the `local-storage` CSI driver which supports proper node fencing and persistent identity; store a disk identity marker file. \n2) Automate pre-reboot hooks to flush I/O and unmount cleanly; use systemd `Before=shutdown.target`. \n3) Implement shard replication or synchronous quorum (if DB supports it) so a single disk failure does not cause RPO>0. \n4) Periodically run scrubbing and integrity checks; collect SMART data to predict failure.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 189, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "611d79c2c3", "source_url": "dataset://entry/During_a_blue_green_rollout,_two_versions_of_a_mic", "title": "During a blue/green rollout, two versions of a microservice behind an Istio mesh show intermittent '...", "text": "Question: During a blue/green rollout, two versions of a microservice behind an Istio mesh show intermittent '503 upstream connect error or disconnect/reset before headers'. Only under high RPS, and mostly for HTTP/2. Liveness/readiness probes pass. What’s the end-to-end diagnosis path?\n\nSolution:\nMesh + rollout issues often come from connection pool limits, HTTP/2 settings, or slow TLS handshakes. \n\n1) Sidecar (Envoy) metrics: Inspect `cluster.upstream_rq_pending_overflow` and `upstream_cx_active` counters. Increase `maxRequestsPerConnection` or disable HTTP/2 where not needed. \n2) Connection pool tuning: In DestinationRule, set outlier detection and connection pool thresholds. For high RPS with short-lived connections, raise `http2MaxRequests` and `maxConnections`. \n3) mTLS: Verify certificate rotation hasn’t caused brief trust gaps; ensure SDS is healthy and `istiod` is not CPU throttled. \n4) Pod resources: Confirm containers are not CPU throttled; bursts for TLS and HPACK decompression need headroom. \n5) Retry budget: Configure exponential backoff with jitter and failover to same-version subset during surge. \n6) Load test: Reproduce with controlled traffic; compare HTTP/1.1 vs HTTP/2. If HTTP/2 exacerbates the issue, consider disabling H2 between sidecars for this service pair. \n7) Validate probe/Ingress mismatch: NGINX/ALB health may succeed on a non-mesh path while mesh traffic fails; unify health endpoint through mesh for consistency.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 197, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0834e5a55f", "source_url": "dataset://entry/A_GitOps_pipeline_(Argo_CD)_floods_the_API_server.", "title": "A GitOps pipeline (Argo CD) floods the API server. Every sync triggers hundreds of full-list calls f...", "text": "Question: A GitOps pipeline (Argo CD) floods the API server. Every sync triggers hundreds of full-list calls for CRDs, causing apiserver p99>1s. RBAC is correct; caching seems ineffective. How do you tame API pressure without losing desired-state guarantees?\n\nSolution:\nFocus on reducing list/watch intensity and object churn. \n\n1) Argo CD settings: Enable resource exclusions (e.g., Events) and narrow the set of watched namespaces. Use ApplicationSet generators to shard apps across multiple controllers to distribute load. \n2) Informer resync: Increase `--app-resync` interval; enable `--sharding` with `--shard` flags. \n3) Large manifests: Split monolithic Helm charts; avoid embedding large binaries or gigantic ConfigMaps/Secrets. \n4) Server-side apply: Prefer SSA with field managers to reduce patch conflicts. Ensure `--prune` is used judiciously to prevent wholesale DELETE/CREATE churn. \n5) API PF (Priority & Fairness): Define fair-queuing for Argo CD clients to prevent starvation of system controllers. \n6) Observability: Track apiserver metrics by user agent; confirm drops after sharding/exclusions. \n7) Last resort: Introduce dedicated APIServer for aggregated CRDs (API Aggregation) isolating heavy CRDs from core paths.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 169, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "210c0c93c0", "source_url": "dataset://entry/After_migrating_from_Docker_Engine_to_containerd,_", "title": "After migrating from Docker Engine to containerd, several Jobs fail with 'text file busy' during ent...", "text": "Question: After migrating from Docker Engine to containerd, several Jobs fail with 'text file busy' during entrypoint replacement. Re-running sometimes works. How do you make image + runtime settings robust against this race?\n\nSolution:\nThis is typically due to overlayfs + concurrent file access during exec. \n\n1) Stop mutating binaries at startup; bake final entrypoints into the image. If you must replace, write to a new path and `exec` it rather than in-place replace. \n2) Use `initContainers` to stage artifacts into an `emptyDir` (medium: Memory if small or default if larger), then point CMD to that immutable copy. \n3) If you wrap shells, ensure `fsGroup`/permissions are set once and avoid chmod/chown on hot code paths. \n4) On containerd, consider `overlayfs` vs `native` snapshotter tradeoffs. If fuse-overlayfs is used (rootless), upgrade to a version with race fixes. \n5) Validate that anti-virus/EDR on nodes isn’t locking files. \n6) Add `terminationGracePeriodSeconds` and retry with backoff for transient locks while you remove mutation from the startup path.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 163, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1506bb2d28", "source_url": "dataset://entry/Kubernetes_audit_logs_show_a_sudden_surge_of_CREAT", "title": "Kubernetes audit logs show a sudden surge of CREATE/UPDATE calls from a custom controller. etcd size...", "text": "Question: Kubernetes audit logs show a sudden surge of CREATE/UPDATE calls from a custom controller. etcd size explodes, API server CPU pegs, and the controller logs show reconcile loops reacting to their own status updates. How do you break the feedback loop safely?\n\nSolution:\nThe controller is likely updating status/spec in a way that retriggers its watch. \n\n1) Compare resource versions: Ensure you only PATCH the minimal fields and use `If-Match` semantics (resourceVersion) to avoid unnecessary updates. \n2) Split `status` updates: Use the `/status` subresource and avoid touching `metadata`/`spec` during status writes. \n3) Debounce & backoff: Introduce a jittered rate-limit queue; coalesce multiple events before a reconcile. Increase workqueue delay after no-op reconciles. \n4) Idempotency: Make reconcile functions idempotent; diff desired vs observed and only patch when changes exist. \n5) Rollout mitigation: Temporarily scale the controller to 0, reduce the CR volume (delete noise CRs), or pause the GitOps tool. Then deploy the fixed controller and re-enable gradually. \n6) Post-mortem: Add metrics (`reconcile_attempts`, `status_updates`) and alerts for runaway reconcile patterns.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 170, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "2e3be07a41", "source_url": "dataset://entry/An_AKS_private_cluster_uses_Azure_CNI_overlay._Aft", "title": "An AKS private cluster uses Azure CNI overlay. After enabling a Private DNS zone and Azure Firewall,...", "text": "Question: An AKS private cluster uses Azure CNI overlay. After enabling a Private DNS zone and Azure Firewall, pods intermittently fail to resolve external FQDNs; NodeLocal DNSCache is deployed. Where do you focus and what concrete fixes do you apply?\n\nSolution:\nOverlay makes pod IPs not routable outside; DNS path must be consistent. \n\n1) Path validation: From a pod, `dig +trace` to see where resolution fails. Confirm NodeLocal DNSCache forwards to CoreDNS, which in turn forwards to Azure DNS (168.63.129.16) or the resolver you intend. \n2) Azure Firewall: Allow outbound 53/UDP+TCP from node and firewall rules to resolver. If using FQDN tags, verify the domains. \n3) Private DNS zone linking: Ensure the VNet is linked to the Private DNS zone and that split-horizon doesn’t loop. Avoid forwarding back into the same zone. \n4) CoreDNS: Increase `max_concurrent` and cache TTL. Ensure stubdomains don’t point to resolvers only reachable via blocked UDRs. \n5) NodeLocal DNSCache: Update to latest; set `-localip` and `-upstreamsvc` explicitly. If cache evictions high, grow cache capacity. \n6) Verification: Run steady DNS qps tests; ensure p99 latency is stable and NXDOMAIN rates match expectations.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 185, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "3c6f8ade8c", "source_url": "dataset://entry/On_EKS_with_IRSA,_a_high-throughput_uploader_pod_i", "title": "On EKS with IRSA, a high-throughput uploader pod intermittently gets 403 on S3 PUT even though the r...", "text": "Question: On EKS with IRSA, a high-throughput uploader pod intermittently gets 403 on S3 PUT even though the role has s3:PutObject. CloudTrail shows AssumeRoleWithWebIdentity failures during bursts. How do you fully bulletproof token exchange?\n\nSolution:\nIRSA relies on OIDC tokens that may expire or hit throttles. \n\n1) SDK config: Ensure your SDK uses a cached credential provider and retries AssumeRoleWithWebIdentity with exponential backoff. Increase HTTP connection pooling. \n2) Token audiences & clock skew: Confirm the projected service account token’s aud matches IAM role trust policy; fix NTP drift on nodes to prevent early/late token rejection. \n3) Token refresh: Lengthen token rotation window; mount the projected token and ensure the SDK reloads automatically (newer AWS SDKs support file-watching). \n4) Scale-out: If bursts exceed STS throttling, shard workloads across multiple roles or pre-warm connections. Consider larger pod replicas to smooth spikes. \n5) Observability: Emit STS metrics and S3 retry counts; validate drop after adding backoff + connection reuse.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 156, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9e46f30bee", "source_url": "dataset://entry/An_on-prem_cluster_upgraded_to_1.29_with_container", "title": "An on-prem cluster upgraded to 1.29 with containerd 1.7. A subset of nodes repeatedly show 'NodeHasD...", "text": "Question: An on-prem cluster upgraded to 1.29 with containerd 1.7. A subset of nodes repeatedly show 'NodeHasDiskPressure' despite ample free space. `du` reveals massive growth under `/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots`. GC seems stuck. What next?\n\nSolution:\nContainerd snapshot GC can stall if references are leaked. \n\n1) List dangling snapshots: `ctr snapshots ls` and `ctr content ls` to find unreferenced blobs. \n2) Restart sequence: Stop kubelet, then containerd, run `ctr snapshots cleanup`, start containerd, then kubelet. \n3) Check CRI plugin: `containerd-shim` leaks or old pods that never terminated can pin snapshots. Remove dead sandboxes with `crictl rmp` and `crictl rmi` for old images. \n4) Prevent reoccurrence: Enable periodic `containerd` GC via timers; prune unused images using a controlled policy. Ensure logging agents rotate logs so large layers aren’t recreated constantly.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 126, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "ab7fbcebc3", "source_url": "dataset://entry/A_platform_team_enforces_Pod_Security_Standards_(r", "title": "A platform team enforces Pod Security Standards (restricted) plus Gatekeeper policies. After rollout...", "text": "Question: A platform team enforces Pod Security Standards (restricted) plus Gatekeeper policies. After rollout, Jobs and initContainers widely fail with 'permission denied' and 'capability not allowed'. How do you restore function without breaking security posture?\n\nSolution:\nYou need case-by-case exceptions while maintaining baseline. \n\n1) Inventory failures: Collect OPA/Gatekeeper violations via audit; map constraints blocking specific workloads (initContainers often need elevated permissions). \n2) Namespaced exceptions: Label specific namespaces with PSA exemptions or Gatekeeper `excludedNamespaces`. \n3) Fine-grained policies: For workloads requiring NET_BIND_SERVICE or SYS_TIME, define Constraints allowing these capabilities for labeled service accounts only. \n4) Migration: Refactor pods to drop all caps by default and add minimal ones; use ephemeral containers for debug with elevated profiles in a break-glass namespace. \n5) Verification: Add admission tests in CI validating new manifests against the policy set to prevent regressions.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 135, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "080d8062e9", "source_url": "dataset://entry/Multiple_clusters_share_a_single_external_etcd_for", "title": "Multiple clusters share a single external etcd for historical reasons. After a network partition, on...", "text": "Question: Multiple clusters share a single external etcd for historical reasons. After a network partition, only one cluster recovered; the others show frequent leader re-elections and high latency. How do you de-risk and migrate with minimal downtime?\n\nSolution:\nShared etcd is a liability. \n\n1) Stabilize: Increase etcd quorum stability by ensuring low, consistent latency between etcd peers. Remove unhealthy members and add dedicated nodes for each cluster’s etcd (or move to embedded control plane etcd). \n2) Snapshot: Take consistent snapshots for each logical dataset (ideally you already separated). If not separated, you must sequence migrations: stand up per-cluster etcd, restore snapshot filtered to that cluster’s keys (advanced), or rebuild control plane from scratch and re-register nodes. \n3) Cutover: Drain control plane components to new etcd endpoints (update manifests on static pods). \n4) Validate: Run conformance and watch apiserver latencies. \n5) Long term: Never share etcd across clusters; isolate failure domains.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 150, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "db3f1b171f", "source_url": "dataset://entry/Rolling_out_Pod_Topology_Spread_Constraints_(PTSC)", "title": "Rolling out Pod Topology Spread Constraints (PTSC) to avoid AZ outages led to scheduling failures fo...", "text": "Question: Rolling out Pod Topology Spread Constraints (PTSC) to avoid AZ outages led to scheduling failures for critical Deployments during a regional rebalancing event. How do you keep PTSC benefits while avoiding deadlock?\n\nSolution:\nPTSC can overconstrain placement. \n\n1) Move from `whenUnsatisfiable: DoNotSchedule` to `ScheduleAnyway` with associated `maxSkew` tuned to your node counts. \n2) Combine with soft anti-affinity and appropriate `tolerations` for tainted nodes to increase options. \n3) Provide surge capacity: Temporarily increase replicas or relax PDBs to allow redistributions. \n4) Add a fallback topology key (`topology.kubernetes.io/zone` -> `kubernetes.io/hostname`) to let the scheduler pack if zones are imbalanced. \n5) Simulate with `cluster-autoscaler` and descheduler in staging to verify no deadlocks under node loss.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 113, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "402032fa87", "source_url": "dataset://entry/Your_Helm_umbrella_chart_renders_ConfigMaps_over_1", "title": "Your Helm umbrella chart renders ConfigMaps over 1MB, causing slow apiserver responses and sporadic ...", "text": "Question: Your Helm umbrella chart renders ConfigMaps over 1MB, causing slow apiserver responses and sporadic 'Request entity too large' via ingress. You must keep large static routing tables. What’s your design that avoids degrading the control plane?\n\nSolution:\nMove bulk data off the API server. \n\n1) Store static routing data in an object store (S3/GCS/Azure Blob) and have initContainers fetch it to a shared `emptyDir`. \n2) If you must keep it in Kubernetes, use a PersistentVolume with read-only projection and a sidecar to sync content from source. \n3) Keep ConfigMaps tiny: only include pointers + checksums for integrity. \n4) Apply gzip compression and segment the data into logical chunks, fetched on demand. \n5) Add liveness probes tied to checksum validation and fallback logic.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 123, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f4ed1b3be8", "source_url": "dataset://entry/During_a_mass_node_upgrade,_kubelet_certificate_ro", "title": "During a mass node upgrade, kubelet certificate rotations failed. Nodes rejoined with new identities...", "text": "Question: During a mass node upgrade, kubelet certificate rotations failed. Nodes rejoined with new identities, but old Node objects linger with taints and finalizers, blocking scheduling. How do you cleanly reconcile?\n\nSolution:\nYou need to reconcile Node objects and CSRs. \n\n1) List stuck Nodes; check their `status.addresses` vs actual host IPs. If obsolete, cordon and delete them with `--force --grace-period=0`. \n2) Clean finalizers: Patch Node resources to remove custom finalizers only after ensuring no controllers rely on them. \n3) CSR approval: Auto-approve bootstrap signer, but enforce policy to auto-deny malformed CSRs. \n4) Reconcile taints: Ensure remaining Nodes have only expected taints; remove installation-time taints if pools are ready. \n5) Audit: Build a rotation playbook that drains, rotates, verifies, then uncordons; automate rollback if CSR approval fails.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 126, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "285acd7b19", "source_url": "dataset://entry/KServe_model_inference_pods_(GPU-enabled)_sporadic", "title": "KServe model inference pods (GPU-enabled) sporadically die with OOM even though `nvidia-smi` shows f...", "text": "Question: KServe model inference pods (GPU-enabled) sporadically die with OOM even though `nvidia-smi` shows free VRAM. Node memory seems fine. Dumps show the process is killed by the kernel OOM killer. Root cause and fixes?\n\nSolution:\nLikely host RAM fragmentation or pinned memory spikes (CUDA). \n\n1) Monitor `oom_kill` in dmesg; correlate with spike in page cache or pinned memory. CUDA can pin host memory beyond container limits. \n2) Set memory limits and `memorySwap` appropriately; disable host overcommit if needed (`vm.overcommit_memory=2`). \n3) Use `NVIDIA_VISIBLE_DEVICES` and MIG to partition GPU, but also cap CPU to avoid burst allocations. \n4) Tune frameworks to limit data prefetch queues and batch sizes. \n5) Consider `--shm-size` for large tensors; offload temp data to NVMe local scratch to reduce RAM pressure.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 124, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "eaa6d02837", "source_url": "dataset://entry/A_cluster_uses_ExternalDNS_with_multiple_providers", "title": "A cluster uses ExternalDNS with multiple providers (Route53 + Cloudflare). After adding wildcard rec...", "text": "Question: A cluster uses ExternalDNS with multiple providers (Route53 + Cloudflare). After adding wildcard records for canary, ExternalDNS started flapping records on every sync due to TTL/ownership conflicts. How do you stabilize multi-provider reconciliation?\n\nSolution:\nMulti-provider needs careful ownership. \n\n1) Use distinct TXT ownership records per provider (e.g., `external-dns/owner=prod-r53` vs `prod-cf`). Configure separate ExternalDNS deployments, each with `--provider` and `--txt-owner-id` unique. \n2) Normalize TTLs in annotations so reconcile diffs don’t churn. \n3) Scope sources via label selectors and `--domain-filter` so providers don’t overlap the same zones. \n4) Add `--policy upsert-only` where deletion is risky. \n5) Validate that wildcards don’t collide with explicit A/AAAA records; prefer ALIAS/CNAME for canary with deterministic ownership.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 111, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7043bebff2", "source_url": "dataset://entry/After_enabling_IPVS_mode_for_kube-proxy,_some_Serv", "title": "After enabling IPVS mode for kube-proxy, some Services with externalTrafficPolicy=Local return 503 o...", "text": "Question: After enabling IPVS mode for kube-proxy, some Services with externalTrafficPolicy=Local return 503 on one AZ during scale-out. Nodes in that AZ have ready pods, but ipvsadm shows missing endpoints briefly. How to remedy?\n\nSolution:\nEndpoint programming lag can cause brief blackholes. \n\n1) Ensure CPU headroom for kube-proxy; increase `--proxy-mode=ipvs` sync periods if too aggressive. \n2) Enable EndpointSlice and reduce slice churn by stable labels. \n3) For externalTrafficPolicy=Local, ensure `externalIPs`/LB health checks target correct NodePort and that your LB uses pod health (or `kube-proxy` health) checks per node. \n4) Consider `topologyKeys` or enabling `GracefulTermination` features that keep endpoints shortly after termination. \n5) Validate conntrack settings so new flows don’t race against stale endpoints.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 113, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "068b6e25be", "source_url": "dataset://entry/A_multi-cluster_failover_(east_west)_uses_Istio_lo", "title": "A multi-cluster failover (east/west) uses Istio locality-aware routing and shared GSLB. During a reg...", "text": "Question: A multi-cluster failover (east/west) uses Istio locality-aware routing and shared GSLB. During a regional outage, failover succeeded but recovery caused traffic pinning to the recovered region for long. What should be tuned to ensure fast failback without flapping?\n\nSolution:\nBalance DNS/GSLB TTLs with mesh outlier detection. \n\n1) Reduce GSLB TTL moderately; add health checks with hysteresis so recovered regions need several consecutive passes to re-enter rotation. \n2) Istio: tune outlier detection `interval`, `baseEjectionTime`, and `maxEjectionPercent` so endpoints gradually rejoin. \n3) Warm pools: Pre-scale in recovered region, run synthetic probes so readiness becomes meaningful. \n4) Use canary weight ramp for service subsets (DestinationRule) so traffic trickles back before full switch. \n5) Telemetry: Alert on skewed locality percentages to detect sticky recovery.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 121, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "6a3fd5e538", "source_url": "dataset://entry/Developers_complain_that_`kubectl_exec`_and_`port-", "title": "Developers complain that `kubectl exec` and `port-forward` frequently fail with 'unable to upgrade c...", "text": "Question: Developers complain that `kubectl exec` and `port-forward` frequently fail with 'unable to upgrade connection' in a hardened cluster using strict egress policies and a corporate proxy. How do you enable these features without weakening the perimeter?\n\nSolution:\nThese features are API-server initiated SPDY/HTTP2 tunnels. \n\n1) Allow API server to reach kubelet/pods: open required ports on nodes (usually 10250 for kubelet). NetworkPolicies must allow control-plane CIDRs to pods for exec/port-forward. \n2) If using a proxy, bypass for cluster internal traffic; set NO_PROXY to include service CIDRs, pod CIDRs, and apiserver DNS names. \n3) RBAC: Ensure users have `pods/exec` and `pods/portforward` verbs only for necessary namespaces. \n4) Audit: Log exec/port-forward events; rotate tokens regularly. \n5) Test: Validate using `kubectl auth can-i` and run exec from jumpbox inside VPC to avoid proxy hops.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 131, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "7e8bc8fb97", "source_url": "dataset://entry/A_cluster_leverages_KEDA_for_event-driven_scale._D", "title": "A cluster leverages KEDA for event-driven scale. During Kafka outages, some ScaledObjects oscillate ...", "text": "Question: A cluster leverages KEDA for event-driven scale. During Kafka outages, some ScaledObjects oscillate replicas aggressively, destabilizing consumers on recovery. What’s a design to make scaling resilient to source instability?\n\nSolution:\nIntroduce stabilization and backpressure awareness. \n\n1) Use `cooldownPeriod` and `minReplicaCount` > 0 to keep warm instances. \n2) Configure trigger metadata to avoid hyper-sensitivity (e.g., set `lagThreshold`/aggregation windows). \n3) Add HPA `behavior` with scaleDown stabilization windows. \n4) Apply circuit breakers in applications to avoid thrash on broker bounce. \n5) Consider buffering (e.g., dead letter or backoff topics) so scale matches sustainable consumption, not instantaneous lag spikes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 96, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9c33b1c776", "source_url": "dataset://entry/Istio_sidecars_increase_p99_latency_by_~20ms_for_g", "title": "Istio sidecars increase p99 latency by ~20ms for gRPC calls only in nodes with high context switches...", "text": "Question: Istio sidecars increase p99 latency by ~20ms for gRPC calls only in nodes with high context switches. CPU not pegged. Perf shows time in kernel networking stack. What would you try?\n\nSolution:\nLikely kernel + Envoy + gRPC interaction. \n\n1) Pin sidecar CPU with guaranteed QoS or dedicated cpuset to reduce context switch overhead. \n2) Enable `SO_REUSEPORT` listener sharding; increase Envoy worker threads to match cores. \n3) Try disabling TCP segmentation offload (TSO)/generic receive offload (GRO) only for a test; sometimes NIC/virt drivers exacerbate latency. \n4) For gRPC, evaluate `h2c` in-cluster to avoid TLS overhead where acceptable; otherwise enable session resumption and tune `initialWindowSize`. \n5) Update kernel and Envoy to versions with gRPC HTTP/2 performance fixes.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 117, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e05a2e40ea", "source_url": "dataset://entry/A_`kubectl_apply`_of_many_CRs_fails_due_to_webhook", "title": "A `kubectl apply` of many CRs fails due to webhook timeouts. Webhook service is up, but endpoints ar...", "text": "Question: A `kubectl apply` of many CRs fails due to webhook timeouts. Webhook service is up, but endpoints are spread across namespaces and use a mesh ingress for mTLS. How do you ensure reliable webhook admission at scale?\n\nSolution:\nAdmission webhooks must be reachable from the API server even during mesh/ingress disturbances. \n\n1) Host the webhook service on a stable in-cluster Service with clusterIP and skip mesh sidecar (annotation to opt-out) to reduce path complexity. \n2) Pin replicas across zones with PTSC and avoid HPA scaling to zero. \n3) Set `timeoutSeconds` conservatively (1–5s) and `failurePolicy: Ignore` for non-critical validations. \n4) Ensure the CA bundle in the webhook config matches the service’s serving cert; rotate proactively. \n5) Create a canary webhook with sampled admission to test upgrades before global rollout.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 129, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "b6d34a24af", "source_url": "dataset://entry/Your_descheduler_evicts_pods_per_policy_to_improve", "title": "Your descheduler evicts pods per policy to improve bin-packing, but during business hours it occasio...", "text": "Question: Your descheduler evicts pods per policy to improve bin-packing, but during business hours it occasionally triggers cascading reschedules that hit PDBs and cause traffic dips. How to keep the benefits without customer impact?\n\nSolution:\nTime-box and scope the descheduler. \n\n1) Run descheduler only in off-peak windows via CronJob. \n2) Limit policies to `RemoveDuplicates` and moderate `LowNodeUtilization` thresholds; exclude critical namespaces. \n3) Honor PDBs strictly; simulate with `dryRun` and `evictLocalStoragePods=false`. \n4) Cap simultaneous evictions and add a guard on per-namespace eviction rate. \n5) Monitor latency/availability SLOs during runs; abort if breach.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 91, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9769b14f90", "source_url": "dataset://entry/A_team_uses_ephemeral_containers_for_production_de", "title": "A team uses ephemeral containers for production debugging. After enabling, you notice secrets occasi...", "text": "Question: A team uses ephemeral containers for production debugging. After enabling, you notice secrets occasionally exposed in debug shells. How do you keep this tool while preventing data exfiltration?\n\nSolution:\nEphemeral containers should be gated. \n\n1) RBAC: Restrict `ephemeralcontainers` subresource to a small SRE group; all actions audited. \n2) PSP/PSA/Gatekeeper: enforce non-root, drop all capabilities, read-only rootfs, and deny mounts. No network or only loopback if possible. \n3) Secret redaction: Inject a shell profile that masks env vars; block `/var/run/secrets/kubernetes.io/serviceaccount` mounts. \n4) Session recording: Enable tty logging via audit and centralize session logs. \n5) Operational playbook: ephemeral containers allowed only in diagnosed namespaces; cleanup hooks kill them after TTL.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 109, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "92f3ef9c6f", "source_url": "dataset://entry/Large_nodes_(64_cores,_512GB)_show_degraded_pod_st", "title": "Large nodes (64 cores, 512GB) show degraded pod start times. Profiling reveals kubelet spent time ma...", "text": "Question: Large nodes (64 cores, 512GB) show degraded pod start times. Profiling reveals kubelet spent time managing cgroups and image pulls. How can you regain fast startups?\n\nSolution:\nScale nodes are great but can hurt cold-start. \n\n1) Pre-pull hot images via DaemonSet on rollout; use `crane`/`ctr` with registry cache to avoid cold pulls. \n2) Tune kubelet: increase parallel image pulls (`--image-pull-progress-deadline`), raise `--pods-per-core` appropriately, and ensure systemd cgroup driver is used consistently. \n3) Reduce init tasks: minimize initContainers and heavy shell logic; offload to baked images. \n4) Storage: place containerd root on fast NVMe; ensure no I/O contention with logs. \n5) Networking: pre-create CNI cache; upgrade to CNI versions with faster IPAM.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 112, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1860f4eef5", "source_url": "dataset://entry/Multi-arch_workloads_(arm64_+_amd64)_on_EKS_someti", "title": "Multi-arch workloads (arm64 + amd64) on EKS sometimes pull the wrong arch image manifest leading to ...", "text": "Question: Multi-arch workloads (arm64 + amd64) on EKS sometimes pull the wrong arch image manifest leading to 'exec format error'. The manifests are multi-arch. What’s the root cause and hardening steps?\n\nSolution:\nMismatch can be due to platform negotiation bugs or local `--platform` overrides. \n\n1) Ensure node’s containerd has correct `SystemdCgroup=true` and reports the right `Runtime.GOARCH`. \n2) In Helm charts/Manifests avoid setting `image:tag@sha256:...` with a digest not matching the node arch; instead use per-arch digests or tags. \n3) Use `nodeSelector`/`affinity` to constrain ARM pods to ARM nodes and AMD pods to AMD nodes. \n4) Validate via admission webhook that image manifests include the node’s architecture. \n5) Build with `buildx` and verify manifest list contains expected variants.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 116, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1b62939b48", "source_url": "dataset://entry/Your_cluster-wide_rate_limiter_(Envoy_rate_limit_s", "title": "Your cluster-wide rate limiter (Envoy rate limit service) uses Redis. During a Redis failover, the p...", "text": "Question: Your cluster-wide rate limiter (Envoy rate limit service) uses Redis. During a Redis failover, the proxy started allowing all traffic (fail-open) causing backend overload. How do you design a safe-degraded mode?\n\nSolution:\nIntroduce layered protection. \n\n1) Set per-route local rate limits in Envoy as a fallback, with conservative defaults. \n2) Configure fail-closed for critical routes if SLO demands it, returning 429 instead of hammering backends. \n3) Use a Redis cluster with quorum and health checks; keep latency budgets for remote calls. \n4) Emit overload signals to Kubernetes HPA/PodAutoscaler to temporarily scale services. \n5) Run chaos drills to validate desired behavior under store failures.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 104, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9830168ebe", "source_url": "dataset://entry/A_logging_agent_DaemonSet_(Fluent_Bit)_spikes_CPU_", "title": "A logging agent DaemonSet (Fluent Bit) spikes CPU and drops logs during high churn of short-lived po...", "text": "Question: A logging agent DaemonSet (Fluent Bit) spikes CPU and drops logs during high churn of short-lived pods. Backpressure causes container log file rotation to fall behind. How do you guarantee lossless (or bounded-loss) logging?\n\nSolution:\nDesign for burst absorption and bounded loss. \n\n1) Use filesystem buffer with quotas and backpressure signals in Fluent Bit; increase `Mem_Buf_Limit` and `storage.max_chunks_up`. \n2) Move container logs to faster disks (NVMe) and enable logrotate with size caps. \n3) Tune retry/backoff to avoid tight loops; enable multiline parsers carefully. \n4) Shard outputs by namespace to parallelize sinks; add a local queue (e.g., Loki Promtail + local WAL). \n5) SLOs: Document acceptable lag, alert when queue depth > threshold; scale DaemonSet by node labels if high-density nodes exist.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 122, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "2fc428f4c1", "source_url": "dataset://entry/A_PCI-compliant_namespace_forbids_mounting_`emptyD", "title": "A PCI-compliant namespace forbids mounting `emptyDir` with medium=Memory. Your in-house service need...", "text": "Question: A PCI-compliant namespace forbids mounting `emptyDir` with medium=Memory. Your in-house service needs fast scratch space. What’s a compliant alternative?\n\nSolution:\nUse persistent storage or tmpfs-like volumes under policy. \n\n1) Provision a small fast PVC (NVMe-backed) with strict quotas and encryption. \n2) If CSI supports ephemeral volumes, use `ephemeral` inline PVCs with `ReadWriteOncePod`. \n3) Employ application-level in-memory caches with bounded size and spillover to PVC; tune for GC. \n4) Ensure audit trails for access; wipe on pod termination via preStop hooks.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 81, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a0c560cde3", "source_url": "dataset://entry/A_large_Helm_upgrade_failed_halfway;_some_CRDs_upd", "title": "A large Helm upgrade failed halfway; some CRDs updated, some CustomResources not. Controllers are cr...", "text": "Question: A large Helm upgrade failed halfway; some CRDs updated, some CustomResources not. Controllers are crashlooping due to version skew. How do you achieve a consistent state without full rollback?\n\nSolution:\nRepair CRDs and CRs in correct order. \n\n1) Pause controllers by scaling Deployments to 0 to stop bad reconciles. \n2) Apply CRD manifests to target version; verify `storedVersions`. \n3) Run conversion jobs or manual conversions for CRs (export old YAMLs, patch to new schema). \n4) Resume controllers gradually; check leader election and reconcile success. \n5) If necessary, use `kubectl-convert` or custom scripts to re-encode objects in the new version.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 100, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f8ec294f24", "source_url": "dataset://entry/Kube-proxy_iptables_mode_shows_massive_rule_chains", "title": "Kube-proxy iptables mode shows massive rule chains causing slow updates and packet processing. Migra...", "text": "Question: Kube-proxy iptables mode shows massive rule chains causing slow updates and packet processing. Migrating to IPVS isn’t immediate. What mitigations are available now?\n\nSolution:\nTrim churn and simplify chains. \n\n1) Enable EndpointSlice; reduce endpoint updates by stabilizing pod labels. \n2) Increase kube-proxy sync period; batch changes. \n3) Reduce number of NodePorts/LoadBalancer Services; collapse internal services behind a mesh. \n4) Pin frequently updated services to headless and use DNS SRV on clients. \n5) Plan phased migration to IPVS with kernel module pre-load.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 82, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "51d3ff1eed", "source_url": "dataset://entry/Developers_depend_on_`hostNetwork:_true`_for_a_lat", "title": "Developers depend on `hostNetwork: true` for a latency-sensitive service but it conflicts with NodeP...", "text": "Question: Developers depend on `hostNetwork: true` for a latency-sensitive service but it conflicts with NodePorts and monitoring agents. How can you safely provide near-host networking without global side effects?\n\nSolution:\nOptions to isolate ports and still get low latency: \n\n1) Use dedicated nodes (taints/tolerations) for hostNetwork workloads to avoid conflicts. \n2) Bind to specific interfaces/ports; coordinate with kube-proxy reserved ranges. \n3) Evaluate DPDK or SR-IOV for direct NIC access if ultra-low latency is needed. \n4) Add NetworkPolicies to restrict hostNetwork pods' access and egress. \n5) Ensure observability via sidecar agents or eBPF-based collectors that respect hostNetwork.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 96, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "9eb17a4cbf", "source_url": "dataset://entry/A_cluster’s_PDBs_protect_availability,_but_during_", "title": "A cluster’s PDBs protect availability, but during an emergency node drain you must evict pods quickl...", "text": "Question: A cluster’s PDBs protect availability, but during an emergency node drain you must evict pods quickly without violating SLAs. What pre-planning and live steps keep both goals?\n\nSolution:\nPre-plan graceful degradation. \n\n1) Design PDBs with realistic `minAvailable`; scale critical services to tolerate at least one node loss per AZ. \n2) Add surge capacity (HPA min > steady state) so drains can proceed. \n3) During emergency, temporarily relax PDBs for targeted services (patch `minAvailable`), drain with `--ignore-daemonsets --delete-emptydir-data`. \n4) After recovery, restore original PDBs and reconcile HPA targets. \n5) Run tabletop exercises to test timings and blast radius.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 98, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "a8163ced19", "source_url": "dataset://entry/`kubectl_cp`_fails_against_some_pods_with_'tar_not", "title": "`kubectl cp` fails against some pods with 'tar not found' or hangs on large files. You need a reliab...", "text": "Question: `kubectl cp` fails against some pods with 'tar not found' or hangs on large files. You need a reliable file transfer method for incident response. What’s the robust approach?\n\nSolution:\n`kubectl cp` requires `tar` in the container and uses SPDY streams. \n\n1) Include `tar` (or `busybox`) in base images for debug. \n2) Prefer an in-cluster SFTP/HTTPS scratch service; mount it as an initContainer destination for uploads. \n3) Use `kubectl exec -- curl`/`wget` to pull artifacts from a secure endpoint. \n4) For large files, use chunked transfers and validate checksums. \n5) Consider `kubectl debug` with ephemeral containers to temporarily add tooling, then remove.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 103, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "bc50453933", "source_url": "dataset://entry/An_admission_controller_injects_sidecars._After_a_", "title": "An admission controller injects sidecars. After a CA rotation, new pods fail admission intermittentl...", "text": "Question: An admission controller injects sidecars. After a CA rotation, new pods fail admission intermittently with TLS errors. Old pods run fine. How do you rotate webhooks with zero downtime?\n\nSolution:\nEnsure CA bundle and serving cert rotation is atomic. \n\n1) Deploy new webhook with dual-trust: include both old and new CA in `caBundle` temporarily. \n2) Issue new serving certs for the webhook service; roll the webhook Deployment first, verify readiness. \n3) Update `MutatingWebhookConfiguration`/`ValidatingWebhookConfiguration` to only the new CA after rollout. \n4) Keep replicas >1, and set `failurePolicy: Ignore` for non-critical hooks during rotation window. \n5) Add canary webhook with 1% scope to validate before full switch.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 107, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "63e7696801", "source_url": "dataset://entry/A_cluster_wide_feature_gate_was_enabled_via_API_se", "title": "A cluster wide feature gate was enabled via API server flags. Some nodes still behave as if the gate...", "text": "Question: A cluster wide feature gate was enabled via API server flags. Some nodes still behave as if the gate is off, causing divergent behavior in controllers. What is the proper procedure to ensure uniform feature state?\n\nSolution:\nFeature gates must be consistent across apiserver, controller-manager, scheduler, and kubelets. \n\n1) Audit all component manifests (static pods, systemd units) for the gate. \n2) Roll control plane sequentially, then nodes by pool. \n3) Validate via `/metrics` or component logs that the feature gate is recognized. \n4) If the feature is beta/GA-sensitive, run conformance tests for a sample workload that depends on it. \n5) Document rollback flags to flip off consistently if issues arise.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 111, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1babebd68e", "source_url": "dataset://entry/A_workload_uses_projected_service_account_tokens_t", "title": "A workload uses projected service account tokens to call an internal OIDC gateway. After enabling sh...", "text": "Question: A workload uses projected service account tokens to call an internal OIDC gateway. After enabling short-lived tokens, intermittent 401s occur on long-lived HTTP/2 connections. What’s the fix?\n\nSolution:\nTokens expire but connections persist. \n\n1) Ensure the client library refreshes tokens and renegotiates authorization on long-lived streams; for gRPC, add interceptors to refresh metadata. \n2) Reduce token TTL only if client can handle frequent refresh; otherwise use slightly longer TTL in line with security policy. \n3) On the server, support token revalidation per request/stream and allow grace periods. \n4) Add health checks that fail fast when token refresh fails.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 99, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "1f40c36691", "source_url": "dataset://entry/A_cluster_with_SPIRE_SPIFFE_identities_experiences", "title": "A cluster with SPIRE/SPIFFE identities experiences sudden mTLS failures between services after node ...", "text": "Question: A cluster with SPIRE/SPIFFE identities experiences sudden mTLS failures between services after node replacements. Certificates are valid. Logs show SVID rotation stalls on some nodes. Steps?\n\nSolution:\nNode attestation and agent rotation likely stuck. \n\n1) Verify SPIRE Agent on nodes has correct join token or attestor config; check clock skew. \n2) Restart agents and server; inspect bundle endpoints and federation. \n3) Ensure the workload API socket is accessible and not blocked by SELinux/AppArmor. \n4) Force reissue SVIDs for affected workloads; validate rotation interval and backoff not overlapping with node drains. \n5) Add alerts for SVID age and rotation latency.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 100, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f42b1019ac", "source_url": "dataset://entry/On_AKS,_you_attached_a_NAT_Gateway_to_increase_SNA", "title": "On AKS, you attached a NAT Gateway to increase SNAT ports. Still, sporadic 502s occur under spikes. ...", "text": "Question: On AKS, you attached a NAT Gateway to increase SNAT ports. Still, sporadic 502s occur under spikes. Connection tracking shows many TIME_WAITs. What else can you tune?\n\nSolution:\nSNAT is one dimension; ephemeral ports and kernel TCP settings matter. \n\n1) Increase client keep-alive and HTTP connection pooling in apps to reuse ports. \n2) Tune node sysctls: `net.ipv4.ip_local_port_range`, `net.ipv4.tcp_tw_reuse` (careful), and conntrack max. \n3) Move chatty services to private Link/Private Endpoint to reduce egress SNAT needs. \n4) Consider per-pod egress IP (Azure CNI with multi-IP) for distribution. \n5) Observe p99s after changes; ensure no collateral drops.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 96, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "4b39e3a36c", "source_url": "dataset://entry/A_multi-tenant_platform_offers_per-namespace_Resou", "title": "A multi-tenant platform offers per-namespace ResourceQuotas and LimitRanges. Tenants complain about ...", "text": "Question: A multi-tenant platform offers per-namespace ResourceQuotas and LimitRanges. Tenants complain about unexpected OOMKills when concurrency spikes. Metrics show burstable QoS pods get throttled and then OOM. How do you align policy with reality?\n\nSolution:\nRight-size and classify workloads. \n\n1) For latency-critical services, use Guaranteed QoS by setting `requests==limits`, with headroom in quotas. \n2) Provide dedicated burst pools with higher memory/CPU caps for short windows; enforce via LimitRanges that default requests are not too small. \n3) Educate tenants to set `resources` aligned with observed p95; use Vertical Pod Autoscaler in recommend mode. \n4) Add pod-level circuit breakers to shed load gracefully before OOM.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 103, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "e2b0a45182", "source_url": "dataset://entry/During_cluster_restore_testing,_you_find_that_Secr", "title": "During cluster restore testing, you find that Secrets re-applied from backups are base64-decoded inc...", "text": "Question: During cluster restore testing, you find that Secrets re-applied from backups are base64-decoded incorrectly by a homegrown tool, corrupting credentials. How do you make secret restoration safe and verifiable?\n\nSolution:\nTreat Secrets as opaque and validate integrity. \n\n1) Store encrypted secrets (SOPS/SealedSecrets/External Secrets) instead of raw base64. \n2) Write restore jobs that compare SHA-256 hashes or test decryption before apply. \n3) Use a dry-run apply (`--server-side --dry-run=server`) to validate schemas. \n4) After restore, run connectivity checks (DB login, API tokens). \n5) Version and sign backups; audit toolchain for base64 double-encode/strip issues.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 92, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "077011faed", "source_url": "dataset://entry/A_cluster_uses_Gatekeeper_to_block_`latest`_tags._", "title": "A cluster uses Gatekeeper to block `latest` tags. Developers need controlled exceptions for sandboxe...", "text": "Question: A cluster uses Gatekeeper to block `latest` tags. Developers need controlled exceptions for sandboxes. What’s a policy design that’s robust and auditable?\n\nSolution:\nImplement allow-by-annotation with scope. \n\n1) ConstraintTemplate checks image tag; deny if `latest` unless namespace has label `policy.k8s.io/allow-latest=true` and object has annotation `policy.k8s.io/exception-ticket=<id>`. \n2) Gatekeeper Audit surfaces exceptions; CI enforces annotation + ticket existence. \n3) Nightly audit report lists all exceptions with age; auto-expire after TTL via a controller. \n4) Education: provide a tool to rewrite tags to digests before prod.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 84, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "fe6d036f8b", "source_url": "dataset://entry/You_suspect_kube-apiserver_memory_leak_when_many_w", "title": "You suspect kube-apiserver memory leak when many watches are open from a service mesh control plane....", "text": "Question: You suspect kube-apiserver memory leak when many watches are open from a service mesh control plane. Memory climbs slowly over days. How to confirm and mitigate without downtime?\n\nSolution:\nTriangulate leak and reduce pressure. \n\n1) Enable pprof on apiserver, collect heap profiles under load. \n2) Identify the mesh control plane user agents; reduce watch fan-out (shard controllers, increase resync intervals). \n3) Enable APF to isolate their queues. \n4) Increase apiserver replicas and put them behind a local NLB to spread watches. \n5) If leak confirmed in your version, roll a minor upgrade patch with the fix, staggering apiservers to avoid downtime.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 102, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "928bf08d8f", "source_url": "dataset://entry/Cluster_nodes_occasionally_reboot_due_to_underlyin", "title": "Cluster nodes occasionally reboot due to underlying host patches. After reboot, some pods fail becau...", "text": "Question: Cluster nodes occasionally reboot due to underlying host patches. After reboot, some pods fail because `subPath` volume mounts point to stale inodes. How do you sanitize and prevent recurrence?\n\nSolution:\n`subPath` is sensitive to path existence/time. \n\n1) On node boot, run a systemd unit to clean stale kubelet `pods/volumes` directories if not mounted. \n2) Avoid dynamic creation under `subPath`; precreate directories via initContainers. \n3) Prefer projected volumes or CSI volumes instead of deep `subPath` trees. \n4) Monitor kubelet logs for `reconstruct volume` errors; alert and cordon nodes with repeated failures.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 91, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "92abde7e87", "source_url": "dataset://entry/Your_cluster_relies_on_Vertical_Pod_Autoscaler_(Au", "title": "Your cluster relies on Vertical Pod Autoscaler (Auto) for some services. After upgrading to a new JV...", "text": "Question: Your cluster relies on Vertical Pod Autoscaler (Auto) for some services. After upgrading to a new JVM, the VPA over-recommends memory, starving neighbors. What’s your stabilization plan?\n\nSolution:\nTame recommendations and isolate noisy apps. \n\n1) Switch to VPA `Recommend` mode temporarily; pin requests via HPA (CPU) and fixed memory until samples stabilize. \n2) Add VPA min/max caps per workload to bound recommendations. \n3) Warm up JVM flags to reduce early allocation spikes; enable `-XX:+UseContainerSupport` and set heap ergonomics. \n4) Use canary workloads with new JVM; only roll to full after VPA proves stable.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 94, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "53856f744d", "source_url": "dataset://entry/Canary_analysis_(Kayenta)_frequently_aborts_due_to", "title": "Canary analysis (Kayenta) frequently aborts due to metrics gaps when Prometheus undergoes remote wri...", "text": "Question: Canary analysis (Kayenta) frequently aborts due to metrics gaps when Prometheus undergoes remote write hiccups. How do you keep progressive delivery safe?\n\nSolution:\nGuard against observability failures. \n\n1) Add SLO for metrics freshness; if freshness fails, pause canaries instead of promoting. \n2) Buffer remote write or deploy local Prometheus shards for canary namespaces to reduce dependency. \n3) Use multiple independent metrics sources if possible. \n4) Decrease metric cardinality for canary to avoid scrape timeouts. \n5) Automate rollback only on bad metrics with good quality flags; otherwise hold state and alert humans.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 92, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "0bfc34c6d3", "source_url": "dataset://entry/You_run_GPU_+_CPU_workloads_on_the_same_nodes._Und", "title": "You run GPU + CPU workloads on the same nodes. Under CPU saturation, GPU jobs slow dramatically desp...", "text": "Question: You run GPU + CPU workloads on the same nodes. Under CPU saturation, GPU jobs slow dramatically despite VRAM headroom. How do you isolate and guarantee GPU throughput?\n\nSolution:\nCPU starvation throttles GPU feeders. \n\n1) Use node pools dedicated to GPU jobs; taint others. \n2) Pin CPU for GPU pods with Guaranteed QoS and `cpuset`. \n3) Enable `nvidia-cuda-mps` for multi-process sharing and set pod CPU limits to ensure input pipelines keep up. \n4) Separate logging/sidecars to different nodes; keep IRQ balancing sane for NICs feeding data.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 87, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "94c9e08c7c", "source_url": "dataset://entry/A_custom_CSI_driver_occasionally_deadlocks_during_", "title": "A custom CSI driver occasionally deadlocks during NodeUnpublishVolume, leaving volumes stuck. What r...", "text": "Question: A custom CSI driver occasionally deadlocks during NodeUnpublishVolume, leaving volumes stuck. What remediation limits blast radius during an incident?\n\nSolution:\nConstrain and auto-heal. \n\n1) Add liveness probes with a short failure window; restart nodeplugin pods to break deadlocks. \n2) Use `CSIDriver` object with `fsGroupPolicy` and `attachRequired` configured correctly to avoid unnecessary attach cycles. \n3) Enable `volumeAttachment` garbage collection via controller-manager flags. \n4) Quarantine affected nodes (cordon) and drain volumes gracefully before reschedule.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 73, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "f685665911", "source_url": "dataset://entry/A_microservice_uses_gRPC_streaming_and_abruptly_fa", "title": "A microservice uses gRPC streaming and abruptly fails during Node drains even with preStop hooks. Co...", "text": "Question: A microservice uses gRPC streaming and abruptly fails during Node drains even with preStop hooks. Connections drop before load balancer detects unready. How to make drains lossless?\n\nSolution:\nSequence conditions and delays. \n\n1) On SIGTERM, immediately fail readiness probe and keep liveness passing; add `terminationGracePeriodSeconds` large enough for stream teardown. \n\n2) Configure load balancer health checks with short interval and `healthy_threshold` low so nodes exit quickly. \n3) Implement server-side graceful shutdown: stop accepting new streams, drain existing with deadlines. \n4) Use connection draining at Ingress/Envoy with `drain_timeout`. \n5) Test end-to-end by draining a single pod under load.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 98, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "056a261d03", "source_url": "dataset://entry/You_detect_abnormal_cluster_DNS_query_volume_due_t", "title": "You detect abnormal cluster DNS query volume due to a retry storm from a library bug. CoreDNS CPU sp...", "text": "Question: You detect abnormal cluster DNS query volume due to a retry storm from a library bug. CoreDNS CPU spikes, and other services degrade. Fast mitigation?\n\nSolution:\nRate-limit and cache aggressively. \n\n1) Patch offending app to back off; meanwhile, add CoreDNS `cache` plugin with increased `max_ttl` and `prefetch`. \n2) Deploy NodeLocal DNSCache to absorb retries locally. \n3) Apply NetworkPolicy to block egress to external DNS from the buggy namespace if queries are external. \n4) Horizontally scale CoreDNS temporarily; after fix, scale down.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 82, "main_field": "question", "has_question": true, "has_solution": true}}
{"id": "2b4b34be49", "source_url": "dataset://entry/A_legacy_image_uses_`ENTRYPOINT_[\"_bin_sh\",\"-c\",\"e", "title": "A legacy image uses `ENTRYPOINT [\"/bin/sh\",\"-c\",\"exec \"$0\" \"$@\"\", \"myapp\"]` pattern that breaks sign...", "text": "Question: A legacy image uses `ENTRYPOINT [\"/bin/sh\",\"-c\",\"exec \"$0\" \"$@\"\", \"myapp\"]` pattern that breaks signal handling and prevents graceful shutdown. Containers linger during rollout. What’s the hardened image entrypoint?\n\nSolution:\nUse a real init and avoid shell traps. \n\n1) Prefer `ENTRYPOINT [\"tini\",\"--\",\"/usr/local/bin/myapp\"]` or build with a compiled entrypoint that forwards signals. \n2) Drop shell indirection unless needed; if used, set proper trap handling and `set -euo pipefail`. \n3) Verify with `docker run --init` locally and confirm SIGTERM propagation before rollout. \n4) Add readiness gates and reasonable `terminationGracePeriodSeconds`.\n", "meta": {"source": "dataset", "import_ts": "2025-11-29T12:36:09Z", "original_len_words": 86, "main_field": "question", "has_question": true, "has_solution": true}}
