{"id": "5ea6d76bbc", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/", "title": "Active Directory integration - Ubuntu Server documentation", "text": "Active Directory integration - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nActive Directory integration\n¶\nIf you have a Microsoft Active Directory domain set up, you can join your Ubuntu Server to it. There are a number of choices you need to make about your setup before you can begin, so you may want to refer to our\nAD integration explanations\nfirst.\nPrepare to join a domain\nJoin a simple domain with the rid backend\nJoin a forest with the rid backend\nJoin a forest with the autorid backend\nSee also\n¶\nExplanation:\nIntroduction to Active Directory integration", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:11Z", "original_len_words": 120}}
{"id": "57c82ec582", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-domain-with-winbind-preparation/", "title": "Join a domain with winbind: preparation - Ubuntu Server documentation", "text": "Join a domain with winbind: preparation - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a domain with winbind: preparation\n¶\nChoosing the identity mapping backend, and planning its ranges, is the first and most important aspect of joining a domain. To actually perform the join, however, a few more configuration steps are necessary. These steps are common to both backend types, the only difference being the actual idmap configuration.\nTo continue, this is the minimum set of packages that are needed:\nsudo apt install winbind libnss-winbind libpam-winbind\nNext, it will make everything much easier if the\nDNS\nresolver is pointed at the Active Directory DNS server. If that is already the case as provided by the\nDHCP\nserver, this part can be skipped.\nFor example, for a default netplan configuration file which looks like this:\nnetwork\n:\nversion\n:\n2\nethernets\n:\neth0\n:\ndhcp4\n:\ntrue\nYou can add a\nnameservers\nblock which will override the DNS options sent by the DHCP server. For example, if the DNS server is at\n10.10.4.5\nand the domain search value is\nexample.internal\n, this would be the new configuration:\nnetwork\n:\nversion\n:\n2\nethernets\n:\neth0\n:\ndhcp4\n:\ntrue\nnameservers\n:\naddresses\n:\n[\n10.10.4.5\n]\nsearch\n:\n[\nexample.internal\n]\nTo make the changes effective, first make sure there are no syntax errors:\nsudo netplan generate\nIf there are no complaints, the changes can be applied:\nsudo netplan apply\nNote\nBe careful whenever changing network parameters over an ssh connection. If there are any mistakes, you might lose remote access!\nTo check if the resolver was updated, run\nresolvectl\nstatus\n:\nGlobal\n         Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n  resolv.conf mode: stub\n\nLink 281 (eth0)\n    Current Scopes: DNS\n         Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n       DNS Servers: 10.10.4.5 10.10.4.1\n        DNS Domain: example.internal\nNow we need to configure the system to also use the winbind NSS module to look for users and groups. In Ubuntu 24.04 LTS and later, this is done automatically, but for older LTS releases, edit the file\n/etc/nsswitch.conf\nand add\nwinbind\nto the end of the\npasswd:\nand\ngroup:\nlines:\n# /etc/nsswitch.conf\n#\n# Example configuration of GNU Name Service Switch functionality.\n# If you have the `glibc-doc-reference' and `info' packages installed, try:\n# `info libc \"Name Service Switch\"' for information about this file.\n\npasswd:         files systemd winbind\ngroup:          files systemd winbind\n(...)\nFinally, let’s enable automatic home directory creation for users as they login. Run the command:\nsudo pam-auth-update --enable mkhomedir\nNow we are set to perform the final winbind configuration depending on the identity mapping backend that was chosen, and actually join the domain.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 460}}
{"id": "125b7f8348", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-forest-with-the-autorid-backend/", "title": "Join a forest with the autorid backend - Ubuntu Server documentation", "text": "Join a forest with the autorid backend - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a forest with the autorid backend\n¶\nJoining a more complex Active Directory forest with the autorid backend is very similar to the rid backend. The only difference is in the idmap configuration in\n/etc/samba/smb.conf\n:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend = autorid\n    # 1,000,000 - 19,999,999\n    idmap config * : range   = 1000000 - 19999999\n    # 1,000,000\n    idmap config * : rangesize = 1000000\n\n    # allow logins when the DC is unreachable\n    winbind offline logon = yes\n    # this *can* be yes if there is absolute certainty that there is only a\n    # single domain involved\n    winbind use default domain = no\n    # setting these enumeration options to yes has a high performance impact\n    # and can cause instabilities\n    winbind enum groups = no\n    winbind enum users = no\n    winbind refresh tickets = yes\n    # if domain users should be allowed to login, they will need a login shell\n    template shell = /bin/bash\n    # the home directory template for domain users\n    template homedir = /home/%D/%U\n    kerberos method = secrets and keytab\nNote that there is no specific domain mentioned in the idmap configuration. That’s because the autorid backend does the allocations on demand, according to the defined slots. The configuration above defines the following:\n1 million IDs per slot\n19 slots (or domains)\nfull ID range, covering all slots, is from 1,000,000 to 19,999,999\nThat being said, the machine still needs to be joined to a specific domain of that forest, and in this example that will be\nEXAMPLE.INTERNAL\n.\nRunning the recommended\ntestparm\ncommand gives us confidence that the configuration is at least free from syntax and other logical errors:\n$ testparm\nLoad smb config files from /etc/samba/smb.conf\nLoaded services file OK.\nWeak crypto is allowed by GnuTLS (e.g. NTLM as a compatibility fallback)\n\nServer role: ROLE_DOMAIN_MEMBER\n\nPress enter to see a dump of your service definitions\nLike with the\nrid\nidmap backend, if this system is not yet in the AD\nDNS\nserver, it’s best to change its\nhostname\n(including the short hostname) to be the fully qualified domain name (FQDN), as that will allow the joining procedure to also update the DNS records, if so allowed by the AD server (normally it is).\nFor this example, the system’s hostname is\nn2\nin the\nexample.internal\ndomain, so the FQDN is\nn2.example.internal\n:\nsudo hostnamectl hostname n2.example.internal\nNow the domain join can be performed:\n$ sudo net ads join -U Administrator\nPassword for [EXAMPLE\\Administrator]:\nUsing short domain name -- EXAMPLE\nJoined 'N2' to dns domain 'example.internal'\nAnd we can revert the hostname change:\nsudo hostnamectl hostname n2\nIf the DNS server was updated correctly (and there were no errors about that in the join output above), then the hostname should now be correctly set, even though we have just the short name in\n/etc/hostname\n:\n$ hostname\nn2\n\n$ hostname -f\nn2.example.internal\nThe last step is to restart the\nwinbind\nservice:\nsudo systemctl restart winbind.service\nVerifying the join\n¶\nThe quickest way to test the integrity of the domain join is via the\nwbinfo\ncommand:\n$ sudo wbinfo -t\nchecking the trust secret for domain EXAMPLE via RPC calls succeeded\nThe next verification step should be to actually try to resolve an existing username from the domain. In the\nEXAMPLE.INTERNAL\ndomain, for example, we have some test users we can check:\n$ id jammy@example.internal\nuid=2001103(EXAMPLE\\jammy) gid=2000513(EXAMPLE\\domain users) groups=2000513(EXAMPLE\\domain users),2001103(EXAMPLE\\jammy)\nIf you compare this with the\nrid\ndomain join, note how the ID that the\njammy\nuser got is different. That’s why it’s important to correctly chose an idmap backend, and correctly assess if deterministic IDs are important for your use case or not.\nAnother valid syntax for domain users is prefixing the name with the domain, like this:\n$ id EXAMPLE\\\\jammy\nuid=2001103(EXAMPLE\\jammy) gid=2000513(EXAMPLE\\domain users) groups=2000513(EXAMPLE\\domain users),2001103(EXAMPLE\\jammy)\nAnd here we try a console login:\nn2 login: jammy@example.internal\nPassword:\nWelcome to Ubuntu 24.04 LTS (GNU/Linux 6.5.0-26-generic x86_64)\n(...)\nCreating directory '/home/EXAMPLE/jammy'.\nEXAMPLE\\jammy@n1:~$\nThe output above also shows the automatic on-demand home directory creation, according to the template defined in\n/etc/samba/smb.conf\n.\nSince we joined a forest, we should also be able to verify users from other domains in that forest. For example, in this example, the domain\nMYDOMAIN.INTERNAL\nis also part of the forest, and we can verify its users:\n$ id noble@mydomain.internal\nuid=3001104(MYDOMAIN\\noble) gid=3000513(MYDOMAIN\\domain users) groups=3000513(MYDOMAIN\\domain users),3001104(MYDOMAIN\\noble)\n\n$ id MYDOMAIN\\\\noble\nuid=3001104(MYDOMAIN\\noble) gid=3000513(MYDOMAIN\\domain users) groups=3000513(MYDOMAIN\\domain users),3001104(MYDOMAIN\\noble)\nA console login also works:\nn2 login: noble@mydomain.internal\nPassword:\nWelcome to Ubuntu 24.04 LTS (GNU/Linux 6.8.0-31-generic x86_64)\n(...)\nCreating directory '/home/MYDOMAIN/noble'.\nMYDOMAIN\\noble@n2:~$\nNotice how the domain name being part of the home directory path is useful: it separates the users from different domains, avoiding collisions for the same username.\nNote\nThe actual login name used can have multiple formats:\nDOMAIN\\user\nat the terminal login prompt,\nDOMAIN\\\\user\nwhen referred to in shell scripts (note the escaping of the ‘\n\\\n’ character), and\nuser@domain\nis also accepted.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 860}}
{"id": "717244d150", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-forest-with-the-rid-backend/", "title": "Join a forest with the rid backend - Ubuntu Server documentation", "text": "Join a forest with the rid backend - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a forest with the rid backend\n¶\nIt’s also possible to join an Active Directory forest using the\nrid\nidentity mapping backend. To better understand what is involved, and why it is tricky, let’s reuse the example where we joined a single domain with this backend:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend       = tdb\n    # 100,000 - 199,999\n    idmap config * : range         = 100000 - 199999\n    idmap config EXAMPLE : backend = rid\n    # 1,000,000 - 1,999,999\n    idmap config EXAMPLE : range   = 1000000 - 1999999\nWith this configuration, we are expected to join the\nEXAMPLE.INTERNAL\ndomain, and have given it a range of 1 million IDs starting with the ID\n1000000\n(1,000,000). There is also the mandatory reserved range for the default domain, represented by the identity mapping configuration for “\n*\n”, which has a smaller range of 100,000 IDs, starting at\n100000\n(100,000).\nThe\ntestparm\nutility is happy with this configuration, and there is no overlap of ID ranges:\n$ testparm\nLoad smb config files from /etc/samba/smb.conf\nLoaded services file OK.\nWeak crypto is allowed by GnuTLS (e.g. NTLM as a compatibility fallback)\n\nServer role: ROLE_DOMAIN_MEMBER\nWe next adjust the\nhostname\nand perform the join:\n$ sudo hostnamectl hostname n3.example.internal\n\n$ hostname\nn3.example.internal\n\n$ hostname -f\nn3.example.internal\n\n$ sudo net ads join -U Administrator\nPassword for [EXAMPLE\\Administrator]:\nUsing short domain name -- EXAMPLE\nJoined 'N3' to dns domain 'example.internal'\n\n$ sudo hostnamectl hostname n3\n$ sudo systemctl restart winbind.service\nA quick check shows that the users from\nEXAMPLE.INTERNAL\nare recognized:\n$ id jammy@example.internal\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nBut what happens if this single domain establishes a trust relationship with another domain, and we don’t modify the\n/etc/samba/smb.conf\nfile to cope with that? Where will the users from the new trusted domain get their IDs from? Since there is no specific idmap configuration for the new trusted domain, its users will get IDs from the default domain:\n$ id noble@mydomain.internal\nuid=100000(MYDOMAIN\\noble) gid=100000(MYDOMAIN\\domain users) groups=100000(MYDOMAIN\\domain users)\nOops. That is from the much smaller range 100,000 - 199,999, reserved for the catch-all default domain. Furthermore, if yet another trust relationship is established, those users will also get their IDs from this range, mixing multiple domains up in the same ID range, in whatever order they are being looked up.\nIf above we had looked up another user instead of\nnoble@mydomain.internal\n, that other user would have been given the ID 100000. There is no deterministic formula for the default domain ID allocation, like there is for the\nrid\nbackend. In the default domain, IDs are allocated on a first come, first serve basis.\nTo address this, we can add another\nidmap config\nconfiguration for the\nrid\nbackend, giving the new domain a separate range:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend       = tdb\n    # 100,000 - 199,999\n    idmap config * : range         = 100000 - 199999\n    idmap config EXAMPLE : backend = rid\n    # 1,000,000 - 1,999,999\n    idmap config EXAMPLE : range   = 1000000 - 1999999\n\n    # MYDOMAIN.INTERNAL idmap configuration\n    idmap config MYDOMAIN : backend = rid\n    # 2,000,000 - 2,999,999\n    idmap config MYDOMAIN : range   = 2000000 - 2999999\nWith this configuration, nothing changed for the\nEXAMPLE.INTERNAL\nusers, as expected:\n$ id jammy@example.internal\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nBut the users from the trusted domain\nMYDOMAIN.INTERNAL\nwill get their IDs allocated from the 2,000,000 - 2,999,999 range, instead of the default one:\n$ id noble@mydomain.internal\nuid=2001104(MYDOMAIN\\noble) gid=2000513(MYDOMAIN\\domain users) groups=2000513(MYDOMAIN\\domain users),2001104(MYDOMAIN\\noble)\nAnd this allocation, which is using the\nrid\nbackend, is deterministic.\nProblem solved! Well, until another trust relationship is established, then we have to allocate another range for it.\nSo is it possible to use the\nrid\nidentity mapping backend with an Active Directory forest, with multiple domains? Yes, but following these steps\nBEFORE\nestablishing any new trust relationship:\nplan an ID range for the new domain\nupdate\nALL\nsystems that are using the\nrid\nidmap backend with the new range configuration for the new domain\nrestart winbind on\nALL\nsuch systems\nthen the new trust relationship can be established\nIf a system is missed, and doesn’t have an idmap configuration entry for the new domain, the moment a user from that new domain is looked up, it will be assigned an ID from the default domain, which will be non-deterministic and different from the ID assigned to the same user in another system which had the new idmap configuration entry. Quite a mess. Unless the different ID is not important, in which case it’s much simpler to just use the\nautorid\nidentity mapping backend.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 813}}
{"id": "683ca3ee29", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-simple-domain-with-the-rid-backend/", "title": "Join a simple domain with the rid backend - Ubuntu Server documentation", "text": "Join a simple domain with the rid backend - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a simple domain with the rid backend\n¶\nLet’s expand on the configuration we had for the\nrid\nbackend and complete the\n/etc/samba/smb.conf\nconfiguration file with the remaining details. We are joining a single domain called\nEXAMPLE.INTERNAL\n. The new configuration options were added at the end of the\n[global]\nsection:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend       = tdb\n    idmap config * : range         = 100000 - 199999\n    idmap config EXAMPLE : backend = rid\n    idmap config EXAMPLE : range   = 1000000 - 1999999\n\n    # allow logins when the DC is unreachable\n    winbind offline logon = yes\n    # this *can* be yes if there is absolute certainty that there is only a\n    # single domain involved\n    winbind use default domain = no\n    # setting these enumeration options to yes has a high performance impact\n    # and can cause instabilities\n    winbind enum groups = no\n    winbind enum users = no\n    winbind refresh tickets = yes\n    # if domain users should be allowed to login, they will need a login shell\n    template shell = /bin/bash\n    # the home directory template for domain users\n    template homedir = /home/%D/%U\n    kerberos method = secrets and keytab\nRight after saving\n/etc/samba/smb.conf\n, it’s always good practice to run the\ntestparm\nutility. It will perform a quick syntax check on the configuration file and alert you of any issues. Here is the output we get with the above configuration settings:\nLoad smb config files from /etc/samba/smb.conf\nLoaded services file OK.\nWeak crypto is allowed by GnuTLS (e.g. NTLM as a compatibility fallback)\n\nServer role: ROLE_DOMAIN_MEMBER\n\nPress enter to see a dump of your service definitions\n(...)\nDuring the domain join process, the tooling will attempt to update the\nDNS\nserver with the\nhostname\nof this system. Since its IP is likely not yet registered in DNS, that’s kind of a chicken and egg problem. It helps to, beforehand, set the hostname manually to the\nFQDN\n. For this example, we will use a host named\nn1\nin the\nexample.internal\ndomain:\nsudo hostnamectl hostname n1.example.internal\nSo that the output of\nhostname\n-f\n(and also just\nhostname\n) is\nn1.example.internal\n.\nWith the config file in place and checked, and all the other changes we made in the previous section, the domain join can be performed:\n$ sudo net ads join -U Administrator\nPassword for [EXAMPLE\\Administrator]:\nUsing short domain name -- EXAMPLE\nJoined 'N1' to dns domain 'example.internal'\nYou can now revert the\nhostnamectl\nchange from before, and set the hostname back to the short version, i.e.,\nn1\nin this example:\nsudo hostnamectl hostname n1\nAs the last step of the process, the\nwinbind\nservice must be restarted:\nsudo systemctl restart winbind.service\nVerifying the join\n¶\nThe quickest way to test the integrity of the domain join is via the\nwbinfo\ncommand:\n$ sudo wbinfo -t\nchecking the trust secret for domain EXAMPLE via RPC calls succeeded\nThe next verification step should be to actually try to resolve an existing username from the domain. In the\nEXAMPLE.INTERNAL\ndomain, for example, we have some test users we can check:\n$ id jammy@example.internal\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nAnother valid syntax for domain users is prefixing the name with the domain, like this:\n$ id EXAMPLE\\\\jammy\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nAnd finally, attempt a console login:\nn1 login: jammy@example.internal\nPassword:\nWelcome to Ubuntu 24.04 LTS (GNU/Linux 6.5.0-26-generic x86_64)\n(...)\nCreating directory '/home/EXAMPLE/jammy'.\nEXAMPLE\\jammy@n1:~$\nThe output above also shows the automatic on-demand home directory creation, according to the template defined in\n/etc/samba/smb.conf\n.\nNote\nThe actual login name used can have multiple formats:\nDOMAIN\\user\nat the terminal login prompt,\nDOMAIN\\\\user\nwhen referred to in shell scripts (note the escaping of the ‘\n\\\n’ character), and\nuser@domain\nis also accepted.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 667}}
{"id": "91b3d276fc", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/", "title": "Backups and version control - Ubuntu Server documentation", "text": "Backups and version control - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nBackups and version control\n¶\nOn Ubuntu, two primary ways of backing up your system are\nbackup utilities\nand\nshell scripts\n. For additional protection, you can combine backup methods.\nBackup utilities\n¶\nBacula\nhas advanced features and customization support, which makes it a good choice for enterprise systems or complex setups.\nrsnapshot\nis a simple and efficient solution, well suited to individual users or small-scale organizations.\nShell scripts\n¶\nIf you are looking for full flexibility and customization, another option is to use shell scripts.\nBackup with shell scripts\nVersion control\n¶\netckeeper\nstores the contents of\n/etc\nin a Version Control System (VCS) repository\nInstall gitolite\nfor a traditional source control management server for git, including multiple users and access rights management\nSee also\n¶\nExplanation:\nIntroduction to backups", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 165}}
{"id": "038e3a0edd", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/back-up-using-shell-scripts/", "title": "How to back up using shell scripts - Ubuntu Server documentation", "text": "How to back up using shell scripts - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to back up using shell scripts\n¶\nIn general, a shell script configures which directories to backup, and passes those directories as arguments to the\ntar\nutility, which creates an archive file. The archive file can then be moved or copied to another location. The archive can also be created on a remote file system such as a\nNetwork File System (NFS)\nmount.\nThe\ntar\nutility creates one archive file out of many files or directories.\ntar\ncan also filter the files through compression utilities, thus reducing the size of the archive file.\nIn this guide, we will walk through how to use a shell script for backing up files, and how to restore files from the archive we create.\nThe shell script\n¶\nThe following shell script uses\ntar\nto create an archive file on a remotely mounted NFS file system. The archive filename is determined using additional command line utilities. For more details about the script, check out the\nbasic\nbackup\nshell\nscript\n<https://discourse.ubuntu.com/t/basic-backup-shell-script/36419>\n_.\n#!/bin/bash\n####################################\n#\n# Backup to NFS mount script.\n#\n####################################\n# What to backup.\nbackup_files\n=\n\"/home /var/spool/mail /etc /root /boot /opt\"\n# Where to backup to.\ndest\n=\n\"/mnt/backup\"\n# Create archive filename.\nday\n=\n$(\ndate\n+%A\n)\nhostname\n=\n$(\nhostname\n-s\n)\narchive_file\n=\n\"\n$hostname\n-\n$day\n.tgz\"\n# Print start status message.\necho\n\"Backing up\n$backup_files\nto\n$dest\n/\n$archive_file\n\"\ndate\necho\n# Backup the files using tar.\ntar\nczf\n$dest\n/\n$archive_file\n$backup_files\n# Print end status message.\necho\necho\n\"Backup finished\"\ndate\n# Long listing of files in $dest to check file sizes.\nls\n-lh\n$dest\nRunning the script\n¶\nRun from a terminal\n¶\nThe simplest way to use the above backup script is to copy and paste the contents into a file (called\nbackup.sh\n, for example). The file must be made executable:\nchmod\nu+x\nbackup.sh\nThen from a terminal prompt, run the following command:\nsudo\n./backup.sh\nThis is a great way to test the script to make sure everything works as expected.\nRun with\ncron\n¶\nThe\ncron\nutility can be used to automate use of the script. The\ncron\ndaemon allows scripts, or commands, to be run at a specified time and date.\ncron\nis configured through entries in a\ncrontab\nfile.\ncrontab\nfiles are separated into fields:\n# m h dom mon dow   command\nWhere:\nm\n: The minute the command executes on, between 0 and 59.\nh\n: The hour the command executes on, between 0 and 23.\ndom\n: The day of the month the command executes on.\nmon\n: The month the command executes on, between 1 and 12.\ndow\n: The day of the week the command executes on, between 0 and 7. Sunday may be specified by using 0 or 7, both values are valid.\ncommand\n: The command to run.\nTo add or change entries in a\ncrontab\nfile the\ncrontab\n-e\ncommand should be used. Also note the contents of a\ncrontab\nfile can be viewed using the\ncrontab\n-l\ncommand.\nTo run the\nbackup.sh\nscript listed above using\ncron\n, enter the following from a terminal prompt:\nsudo\ncrontab\n-e\nNote\nUsing\nsudo\nwith the\ncrontab\n-e\ncommand edits the\nroot\nuser’s\ncrontab\n. This is necessary if you are backing up directories only the root user has access to.\nAs an example, if we add the following entry to the\ncrontab\nfile:\n# m h dom mon dow   command\n0\n0\n*\n*\n*\nbash\n/usr/local/bin/backup.sh\nThe\nbackup.sh\nscript would be run every day at 12:00 pm.\nNote\nThe\nbackup.sh\nscript will need to be copied to the\n/usr/local/bin/\ndirectory in order for this entry to run properly. The script can reside anywhere on the file system, simply change the script path appropriately.\nRestoring from the archive\n¶\nOnce an archive has been created, it is important to test the archive. The archive can be tested by listing the files it contains, but the best test is to\nrestore\na file from the archive.\nTo see a listing of the archive contents, run the following command from a terminal:\ntar\n-tzvf\n/mnt/backup/host-Monday.tgz\nTo restore a file from the archive back to a different directory, enter:\ntar\n-xzvf\n/mnt/backup/host-Monday.tgz\n-C\n/tmp\netc/hosts\nThe\n-C\noption to\ntar\nredirects the extracted files to the specified directory. The above example will extract the\n/etc/hosts\nfile to\n/tmp/etc/hosts\n.\ntar\nrecreates the directory structure that it contains. Also, notice the leading “\n/\n” is left off the path of the file to restore.\nTo restore all files in the archive enter the following:\ncd\n/\nsudo\ntar\n-xzvf\n/mnt/backup/host-Monday.tgz\nNote\nThis will overwrite the files currently on the file system.\nFurther reading\n¶\nFor more information on shell scripting see the\nAdvanced Bash-Scripting Guide\nThe\nCron How-to Wiki Page\ncontains details on advanced cron options.\nSee the\nGNU tar Manual\nfor more tar options.\nThe Wikipedia\nBackup Rotation Scheme\narticle contains information on other backup rotation schemes.\nThe shell script uses tar to create the archive, but there many other command line utilities that can be used. For example:\ncpio\n: used to copy files to and from archives.\ndd\n: part of the coreutils package. A low level utility that can copy data from one format to another.\nrsnapshot\n: a file system snapshot utility used to create copies of an entire file system. Also check the\nTools - rsnapshot\nfor some information.\nrsync(1)\n: a flexible utility used to create incremental copies of files.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 962}}
{"id": "51a7070fdd", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-bacula/", "title": "How to install and configure Bacula - Ubuntu Server documentation", "text": "How to install and configure Bacula - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure Bacula\n¶\nBacula\nis a backup management tool that enables you to backup, restore, and verify data across your network. There are Bacula clients for Linux, Windows, and Mac OS X – making it a cross-platform and network-wide solution.\nBacula components\n¶\nBacula is made up of several components and services that are used to manage backup files and locations:\nBacula Director\n: A service that controls all backup, restore, verify, and archive operations.\nBacula Console\n: An application that allows communication with the Director.\nBacula File\n: This application is installed on machines to be backed up, and is responsible for handling data requested by the Director.\nBacula Storage\n: The program that performs the storage of data onto, and recovery of data from, the physical media.\nBacula Catalog\n: Responsible for maintaining the file indices and volume databases for all backed-up files. This enables rapid location and restoration of archived files. The Catalog supports three different databases: MySQL, PostgreSQL, and SQLite.\nBacula Monitor\n: This is a graphical tray monitor for the Bacula backup system.\nThese services and applications can be run on multiple servers and clients, or they can be installed on one machine if backing up a single disk or volume.\nIn this documentation, we will deploy a Bacula Director, with a backup job for the Director itself, and also install the Bacule File service on a workstation, to remotely backup its data.\nInstalling the Server Components\n¶\nThe Bacula components can be installed on multiple systems, or they can be grouped together where it makes sense. A fully distributed installation might be appealing and is more scalable, but is also harder to configure. Here we will pick something in between:\nA Bacula “server”, where we will install the following Bacula components: Director, Catalog with an SQL database, Storage, File, Console. The server itself should also be backed up, hence why components typically installed on clients are also installed here.\nA Bacula “client”, which is just a system to be backed up. It will have only the File component installed. Any system that needs to be backed up will have to have the File component installed.\nTo begin with, we have to start with installing the database that will be used by the Catalog. The choices are:\nsqlite: Should only be used for test or development deployments of Bacula.\nPostgreSQL\nMySQL\nEither SQL database is suitable. For this document, we will use PostgreSQL:\nsudo apt install postgresql\nNote\nPlease take a look at\nMySQL databases\nand\nPostgreSQL databases\nfor more details on these powerful databases.\nNext we can install Bacula. The\nbacula\npackage has the necessary dependencies and will pull in what is needed for our deployment scenario:\nsudo apt install bacula\nDuring the install process you will be asked to supply a password for the\ndatabase owner\nof the\nbacula database\n. If left blank, a random password will be used.\nConfiguring the Server\n¶\nSeveral components were installed on this system by the\nbacula\npackage. Each one has its own configuration file:\nDirector:\n/etc/bacula/bacula-dir.conf\nStorage:\n/etc/bacula/bacula-sd.conf\nFile:\n/etc/bacula/bacula-fd.conf\nConsole:\n/etc/bacula/bconsole.conf\nAll these components need to eventually talk to each other, and the authentication is performed via passwords that were automatically generated at install time. These passwords are stored in the\n/etc/bacula/common_default_passwords\nfile. If a new component is installed on this system, it can benefit from this file to automatically be ready to authenticate itself, but in general, after everything is installed and configured, this file isn’t needed anymore.\nDirector\n¶\nThe Bacula Director is the central component of the system. This is where we:\nRegister the clients;\nDefine the schedules;\nDefine the file sets to be backed up;\nDefine storage pools;\nDefine the backup jobs;\nBacula configuration files are formatted based on\nresources\ncomposed of\ndirectives\nsurrounded by curly “{}” braces. Each Bacula component has an individual file in the\n/etc/bacula\ndirectory.\nThe default installation of the several bacula components will create configuration files in\n/etc/bacula/\nwith some choices and examples. That is a good reference, but it does not apply to all cases.\nAll the following sections involve the\n/etc/bacula/bacula-dir.conf\nDirector configuration file, unless stated otherwise.\nThe\nDirector\nresource\n¶\nThis block defines the attributes of the Director service:\nDirector\n{\nName\n=\nbacula\n-\nserver\n-\ndir\nDIRport\n=\n9101\nQueryFile\n=\n\"/etc/bacula/scripts/query.sql\"\nWorkingDirectory\n=\n\"/var/lib/bacula\"\nPidDirectory\n=\n\"/run/bacula\"\nMaximum\nConcurrent\nJobs\n=\n20\nPassword\n=\n\"<randomly generated>\"\nMessages\n=\nDaemon\n#DirAddress = 127.0.0.1\n}\nWhat you should inspect and change:\nName\n: it’s common to use the format\n$hostname-dir\nfor the director. For example, if the hostname is “\nbacula-server\n”, the name here would be\nbacula-server-dir\n. By sticking to this pattern, less changes will have to be made to the config, as this is what the default installation already assumes.\nDirAddress\n: by default this is set to the localhost address. In order to be able to perform remote backups, thouch, the director needs to be accessible on the network. To do that, simply remove or comment this parameter: in that case, the service will listen on all network interfaces available on the system.\nPassword\n: a random password will have been created for this installation, so it doesn’t need to be changed, unless you would rather pick a different one.\nTip\nFor more details about all the options of the\nDirector\nresource, please check the upstream\nDirector Resource\ndocumentation.\nThe\nFileSet\nresource\n¶\nLet’s define what we want to backup. There will likely be multiple file sets defined in a production server, but as an example, here we will define a set for backing up the home directory:\nFileSet\n{\nName\n=\n\"Home Set\"\nInclude\n{\nOptions\n{\nsignature\n=\nSHA256\naclsupport\n=\nyes\nxattrsupport\n=\nyes\n}\nOptions\n{\nwilddir\n=\n\"/home/*/Downloads\"\nwildfile\n=\n\"*.iso\"\nexclude\n=\nyes\n}\nFile\n=\n/\nhome\n}\n}\nThis example illustrates some interesting points, and shows the type of flexibility he have in defining File Sets:\nName\n: This defines the name of this file set, and will be referenced by other configuration blocks.\nInclude Options\n: the\nOptions\nblock can be used several times inside the\nInclude\nblock. Here we define that we want to include POSIX ACLs support, and extended attributes, and use the SHA256 hash algorithm, but perhaps more interesting is how we select which files to exclude from the set:\nwilddir\n,\nwildfile\n: these parameters specify globbing expressions of directories and files to exclude from the set (because we also set\nexclude\n=\nyes\n). In this example, we are excluding potentially large files. Unfortunately there is no direct way to exclude files from a set based on their size, so we have to make some educated guesses using standard file extensions and directory locations.\nFile\n: This parameter can be specified multiple times, and it’s additive. In this example we have it only used once, to select the\n/home\ndirectory and its subdirectories, subject to the exclusions defined in the\nOptions\nblock.\nTip\nFor more details about all the options of the\nFileSet\nresource, please check the upstream\nFileSet Resource\ndocumentation.\nThe\nClient\nresource\n¶\nThe\nClient\nresource is used to define a system to be backed up. That system will have at least the File Daemon installed, and will be contacted by the Director whenever there is a backup job for it.\nThe default installation will have defined this resource already, and it should be similar to the following:\n# Client (File Services) to backup\nClient\n{\nName\n=\nbacula\n-\nserver\n-\nfd\nAddress\n=\nbacula\n-\nserver\n.\nlxd\n# use the real hostname instead of \"localhost\"\nFDPort\n=\n9102\nCatalog\n=\nMyCatalog\nPassword\n=\n\"<randomly generated>\"\nFile\nRetention\n=\n60\ndays\nJob\nRetention\n=\n6\nmonths\nAutoPrune\n=\nyes\n}\nOf note in this definition we have:\nName\n: As with other similar resources, the default name uses the format\n$hostname-fd\n(where\nfd\nstands for\nFile Daemon\n).\nAddress\n: The default will be\nlocalhost\n, but we should be in the habit of using a real hostname, because in a more distributed installation, these hostnames will be sent to other services in other machines, and “localhost” will then be incorrect.\nPassword\n: The password was automatically generated, and should be kept as is unless you want to use another one.\nFile\nRetention\n,\nJob\nRetention\n: these should be adjusted according to your particular needs for each client.\nBy default, the backup job named\nBackupClient1\nis configured to archive the Bacula Catalog. If you plan on using the server to back up more than one client you should change the name of this job to something more descriptive. To change the name, edit\n/etc/bacula/bacula-dir.conf\n:\nAutoPrune\n: This setting makes Bacula automatically apply the retention parameters at the end of a backup job. It is enabled by default.\nTip\nFor more details about all the options of the\nClient\nresource, please check the upstream\nClient Resource\ndocumentation.\nThe\nPool\nresource\n¶\nA\nPool\nin Bacula represents a collection of volumes. A\nVolume\nis a single physical tape, or a file on disk, and is where Bacula will write the backup data.\nThe default configuration file will have defined several\nPools\nalready. For this documentation, we are interested in the\nFile\npool:\nPool\n{\nName\n=\nFile\nPool\nType\n=\nBackup\nRecycle\n=\nyes\n# Bacula can automatically recycle Volumes\nAutoPrune\n=\nyes\n# Prune expired volumes\nVolume\nRetention\n=\n365\ndays\n# one year\nMaximum\nVolume\nBytes\n=\n50\nG\n# Limit Volume size\nMaximum\nVolumes\n=\n100\n# Limit number of Volumes in Pool\nLabel\nFormat\n=\n\"Vol-\"\n# Auto label\n}\nWe will use this pool to backup to a directory on the server (which will usually be the mount point for a big storage device). The pool resource has some definitions that affect how large the backups can become, so these have to be checked:\nName\n: The name of the pool, which will be referenced in other resources.\nVolume\nRetention\n: For how long volumes are kept.\nMaximum\nVolume\nBytes\n: What is the maximum size of each volume file.\nMaximum\nVolumes\n: How many volume files are we going to keep at most.\nLabel\nFormat\n: The prefix that each volume file will get. In this example, the files will be automatically named\nVol-0001\n,\nVol-0002\n, and so on.\nWith the values in the example above, we will be storing at most 50G * 100 = 5000GB in this pool.\nTip\nFor more details about all the options of the\nPool\nresource, please check the upstream\nPool Resource\ndocumentation.\nThe\nStorage\nresource\n¶\nThe\nStorage\nresource in the bacula Director configuration file points at the system where the Storage component is running.\nIn our current setup, that’s the same system where the Director is running, but we\nMUST NOT\nuse\nlocalhost\nin the definition, because this configuration is also sent to the File component on other systems. In another system,\nlocalhost\nwill mean itself, but the Storage daemon is not running over there.\nThis time we will have to change two configuration files: the Director one, and the Storage one. Let’s begin by defining a\nStorage\nresource on\n/etc/bacula/bacula-dir.conf\n, the Director configuration file:\nStorage\n{\nName\n=\nFileBackup\nAddress\n=\nbacula\n-\nserver\n.\nlxd\nSDPort\n=\n9103\n# For this password, use:\n# sudo grep ^SDPASSWD /etc/bacula/common_default_passwords\nPassword\n=\n\"<SDPASSWD value>\"\nDevice\n=\nFileBackup\nMedia\nType\n=\nFile\n}\nHere is what we have defined with the block above:\nName\n: The name of this Storage resource, which will be referenced in other places.\nAddress\n: The name of the system where the Storage daemon is running. Again, never use\nlocalhost\nhere, even if it’s the same system where the Director is running.\nPassword\n: The password that should be used when connecting to the Storage daemon. The installation of the packages will have generated a random password. It can be found either in the existing\nAutochanger\ndefinitions in\n/etc/bacula/bacula-dir.conf\n, or in\n/etc/bacula/common_default_passwords\nin the line for\nSDPASSWD\n, or in the Storage daemon configuration file\n/etc/bacula/bacula-sd.conf\n.\nDevice\n: This must match an existing\nDevice\ndefinition in the Storage daemon’s configuration file (which will be covered next).\nMedia\nType\n: Likewise, this must also match the same\nMedia\nType\ndefined in the Storage daemon’s configuration file.\nTip\nFor more details about all the options of the\nPool\nresource, please check the upstream\nStorage Resource\ndocumentation.\nNext we need to edit the corresponding Storage daemon configuration in\n/etc/bacula/bacula-sd.conf\n.\nFirst, remove or comment out the\nSDAddress\nconfiguration, so that the daemon will listen on all network interfaces it finds:\nStorage\n{\nName\n=\nbacula\n-\nserver\n-\nsd\nSDPort\n=\n9103\nWorkingDirectory\n=\n\"/var/lib/bacula\"\nPid\nDirectory\n=\n\"/run/bacula\"\nPlugin\nDirectory\n=\n\"/usr/lib/bacula\"\nMaximum\nConcurrent\nJobs\n=\n20\nEncryption\nCommand\n=\n\"/etc/bacula/scripts/key-manager.py getkey\"\n#SDAddress = 127.0.0.1\n}\nImportant points for the config above:\nName\n: It’s standard for Bacula systems to suffix the name of the system where a component is running with the abbreviation of that component. In this case, the name of the system is\nbacule-server\n, and the component we are defining is the Storage Daemon, hence the\n-sd\nsuffix.\nSDAddress\n: We need this daemon to listen on all interfaces so it’s reachable from other systems, so we comment this line out and rely on the default which it to listen on all interfaces.\nNext, let’s define a\nDevice\n, also in\n/etc/bacula/bacula-sd.conf\n:\nDevice\n{\nName\n=\nFileBackup\nMedia\nType\n=\nFile\nArchive\nDevice\n=\n/\nstorage\n/\nbackups\nRandom\nAccess\n=\nyes\nAutomatic\nMount\n=\nyes\nRemovable\nMedia\n=\nno\nAlways\nOpen\n=\nno\nLabel\nMedia\n=\nyes\n}\nWhat we need to pay close attention to here is:\nName\n: This has to match the name this device will be referred to in other services. In our case, it matches the name we are using in the\nDevice\nentry of the\nStorage\ndefinition we added to the Director configuration file\n/etc/bacula/bacula-dir.conf\nearlier.\nMedia\nType\n: Likewise, this has to match the entry we used in the\nStorage\ndefinition in the Director.\nArchive\nDevice\n: Since we are going to store backups as files, and not as tapes, the\nArchive\nDevice\nconfiguration points to a directory. Here we are using\n/storage/backups\n, which can be the mount point of an external storage for example. This is the target directory of all backup jobs what will refer to this device of this storage server.\nLabel\nMedia\n: Since we are using files and not real tapes, we want the Storage daemon to actually name the files for us. This configuration option allows it to do so.\nLastly, the Storage component needs to be told about the Director. This is done with a\nDirector\nresource in\n/etc/bacula/bacula-sd.conf\n. No changes should be needed here because we installed the Storage component on the same host as the Director, but it’s best to check:\nDirector\n{\nName\n=\nbacula\n-\nserver\n-\ndir\nPassword\n=\n\"<randomly generated>\"\n}\nThese two options need to match the following:\nName\n: This names which Director is allowed to use this Storage component, and therefore needs to match the\nName\ndefined in the\nDirector\nresource in\n/etc/bacula/bacula-dir.conf\non the Director system.\nPassword\n: The password that the Director needs to use to authenticate against this Storage component. This needs to match the\nPassword\nset in the\nStorage\nresource in\n/etc/bacula/bacula-dir.conf\non the Directory system.\nAfter making all changes to the\nbacula-sd.conf\nconfiguration file, restart the Storage Daemon:\nsudo systemctl restart bacula-sd.service\nThe\nJob\nresource\n¶\nThe\nJob\nresource is the basic unit in Bacula, and ties everything together:\nWho is being backed up (\nClient\n).\nWhat should be backed up (\nFileSet\n).\nWhere should the data be stored (\nStorage\n,\nPool\n), and where to record the job (\nCatalog\n)\nWhen thouls the job run (\nSchedule\n)\nThe default Director configuration file includes a default Job resource, and more Jobs can inherit from that.\nLet’s go over the Default Job resource first in\n/etc/bacula/bacula-dir.conf\nand change it a little bit:\nJobDefs\n{\nName\n=\n\"DefaultJob\"\nType\n=\nBackup\nLevel\n=\nIncremental\nClient\n=\nbacula\n-\nserver\n-\nfd\nFileSet\n=\n\"Home Set\"\nSchedule\n=\n\"WeeklyCycle\"\nStorage\n=\nFileBackup\nMessages\n=\nStandard\nPool\n=\nFile\nSpoolAttributes\n=\nyes\nPriority\n=\n10\nWrite\nBootstrap\n=\n\"/var/lib/bacula/\n%c\n.bsr\"\n}\nThis configuration is selecting some defaults:\nName\n: The name of this Job.\nClient\n: To which client it applies. This must match an existing\nClient\n{}\nresource definition.\nFileSet\n: The name of the\nFileSet\nresource that defines the data to be backed up. We changed it to\nHome\nSet\nin this example.\nSchedule\n: The name of the\nSchedule\nresource that defines when this job should run.\nStorage\n: Which\nStorage\nresource this job should use. We changed it to\nFileBackup\nin this example.\nPool\n: Which\nPool\nresource this job should use.\nWe can now take advantage of this set of defaults, and define a new Job resource with minimal config:\nJob\n{\nName\n=\n\"DirectorHomeBackup\"\nJobDefs\n=\n\"DefaultJob\"\n}\nTip\nA job has many attributes that can only be specified once. This means that jobs are pretty much specific to what is being backed up, and from where, among other things. It therefore helps to come up with a naming convention.\nThe upstream documentation has a section about\nNaming Resources\nwith some suggestions.\nWe also need a Job definition for the restore task. The default configuration file will have a definition for this already, but it needs to be changed:\nJob\n{\nName\n=\n\"RestoreFiles\"\nType\n=\nRestore\nClient\n=\nbacula\n-\nserver\n-\nfd\nStorage\n=\nFileBackup\n# The FileSet and Pool directives are not used by Restore Jobs\n# but must not be removed\nFileSet\n=\n\"Home Set\"\nPool\n=\nFile\nMessages\n=\nStandard\nWhere\n=\n/\nstorage\n/\nrestore\n}\nImportant parameters defined above:\nName\n: The name of this job.\nType\n: This is a job that restores backups (\nRestore\n).\nClient\n: Where the files should be restored to. This can be overridden when the job is invoked.\nStorage\n: The storage from where the backup should be restored. We changed it to\nFileBackup\nin this example.\nFileSet\nand\nPool\n: These are not used, but must be present and point to valid resources. We changed\nFileSet\nto\nHome\nSet\nin this example.\nWhere\n: The path where the restored files should be placed. This can also be overridden when the job is invoked. We changed it to\n/storage/restore\nin this example.\nTip\nFor more details about all the options of the\nJob\nresource, please check the upstream\nJob Resource\ndocumentation.\nStorage daemon\n¶\nThere isn’t much more to configure for the Storage daemon after the Director configuration steps done earlier, but we still need to create the directories for the backup and restore jobs:\nsudo mkdir -m 0700 /storage /storage/backups /storage/restore\nsudo chown bacula: -R /storage\nThis will allow bacula, and only bacula, to read and write to the storage path. You can, of course, adjust the permissions and ownership to something that suits your deployment. Just be mindful that the bacula user needs to be able to create and remove files from the\n/storage/backups\nand\n/storage/restore\npaths, and that regular users should not be allowed to read those.\nTip\nFor more details about the Storage daemon configuration options, please check the upstream\nStorage Daemon\ndocumentation.\nFile daemon\n¶\nThe File daemon configuration is located in the\n/etc/bacula/bacula-fd.conf\nfile, and the only remaining task is to make sure it listens on the network. To be fair, in this particular deployment layout, this is not strictly needed, as both the Director and Storage daemons are located on the same system, but making this change allows for those components to be split off to different systems should that need arise.\nTo make this change, we are going to remove or comment out the\nFDAddress\noption in the\nFileDaemon\nresource in\n/etc/bacula/bacula-fd.conf\nfile:\nFileDaemon\n{\nName\n=\nbacula\n-\nserver\n-\nfd\nFDport\n=\n9102\nWorkingDirectory\n=\n/\nvar\n/\nlib\n/\nbacula\nPid\nDirectory\n=\n/\nrun\n/\nbacula\nMaximum\nConcurrent\nJobs\n=\n20\nPlugin\nDirectory\n=\n/\nusr\n/\nlib\n/\nbacula\n#FDAddress = 127.0.0.1\n}\nAfter making the change and saving the file, restart the File daemon service:\nsudo systemctl restart bacula-fd.service\nTip\nFor more details about the File daemon configuration, please check the upstream\nFile Daemon\ndocumentation.\nConsole\n¶\nThere is no further configuration to be done for the Console at this time. The defaults selected and adjusted by the package install are sufficient. The configuration file is\n/etc/bacula/bconsole.conf\n, and more details are available in the upstream\nConsole Configuration\ndocumentation.\nThe Console can be used to query the Director about jobs, but to use the Console with a\nnon-root\nuser, the user needs to be in the\nBacula group\n. To add a user to the Bacula group, run the following command from a terminal:\nsudo adduser <username> bacula\nReplace\n<username>\nwith the actual username. Also, if you are adding the current user to the group you should log out and back in for the new permissions to take effect.\nWarning\nBe mindful of who is added to the\nbacula\ngroup: members of this group are able to read all the data that is being backed up!\nCleaning up\n¶\nWe have added new resources to some Bacula components, and changed some existing ones. There are also resources we didn’t touch, but they will show up in the console or logs. Optionally, we can remove them to cleanup our config files.\nIn\n/etc/bacula/bacula-dir.conf\n:\nall the\nAutochanger\nresources can be removed, since they are not referred to by any other resource.\nIn\n/etc/bacula/bacula-sd.conf\n:\nThe\nAutochanger\nresources can be removed, as it’s not being used.\nThe\nFileChgr1-Dev1\n,\nFileChgr1-Dev2\n,\nFileChgr2-Dev1\n, and\nFileChgr2-Dev2\nDevices, referred to by the Autochangers above, should then also be removed.\nWe made many changes to a few configuration files, so let’s restart all the related services:\nsudo systemctl restart bacula-dir.service bacula-fd.service bacula-sd.service\nRunning a backup job\n¶\nWe now have everything in place to run our first backup job.\nOn the Bacula Director system, run the\nbconsole\ncommand as root to enter the Bacula Console:\nsudo bconsole\nThe command will connect to the the local Director, and open up an interactive prompt:\nConnecting to Director localhost:9101\n1000 OK: 10002 bacula-server-dir Version: 15.0.3 (25 March 2025)\nEnter a period to cancel a command.\n*\nYou can type\nhelp\nfor a full list of all the available commands, and\nhelp\n<command>\nfor more detailed information about the specific\n<command>\n.\nFor example, to obtain help text about the\nrun\ncommand, type\nhelp\nrun\nto obtain the following output:\nCommand       Description\n  =======       ===========\n  run           Run a job\n\nArguments:\n  job=<job-name> client=<client-name>\n  fileset=<FileSet-name> level=<level-keyword>\n  storage=<storage-name> where=<directory-prefix>\n  when=<universal-time-specification> pool=<pool-name>\n  nextpool=<next-pool-name> comment=<text> accurate=<bool> spooldata=<bool> yes\n\nWhen at a prompt, entering a period cancels the command.\nLet’s interactively run a backup job. The output below will show the\nrun\ncommand and all the replies that were typed in response to the console prompts:\n*run\nUsing Catalog \"MyCatalog\"\nA job name must be specified.\nThe defined Job resources are:\n     1: HomeBackup\n     2: BackupCatalog\n     3: RestoreFiles\nSelect Job resource (1-3): 1\nRun Backup job\nJobName:  HomeBackup\nLevel:    Incremental\nClient:   bacula-server-fd\nFileSet:  Home Set\nPool:     File (From Job resource)\nStorage:  FileBackup (From Job resource)\nWhen:     2025-10-20 20:21:03\nPriority: 10\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=7\nLet’s unpack this:\nrun\n: This is the command. Since no parameters were given, Bacula will ask for what’s missing.\nWhich job should be run:\nHomeBackup\nis the job we defined in this how-to, and we select it by its index number.\nSummary: at the end, we are given a summary of the job. Here we can still change values via the\nmod\nreply, but for now let’s just accept those values and reply\nyes\n.\nJobId: the job is accepted, and we are given an ID. In this case, it was “\n7\n”.\nTo check the result of a job, there are several methods:\nMessages: right after scheduling the job, it’s likely something will be logged. You can run the\nmessages\ncommand, and it will show the latest unread messages (and also mark them as read, so you can only benefit from this once).\nThe\nlist\njobs\ncommand, to list all jobs, or, more specifically,\nlist\njobid=7\nto list a particular job.\nInspect the full log of that particular job, via the\nlist\njoblog\njobid=<N>\ncommand.\nServer log: you can inspect the server log at\n/var/log/bacula/bacula.log\n.\nFor example, if we run\nlist\njobid=7\n, this is the output:\n+-------+------------+---------------------+------+-------+----------+----------+-----------+\n| jobid | name       | starttime           | type | level | jobfiles | jobbytes | jobstatus |\n+-------+------------+---------------------+------+-------+----------+----------+-----------+\n|     7 | HomeBackup | 2025-10-20 20:21:11 | B    | I     |        0 |        0 | T         |\n+-------+------------+---------------------+------+-------+----------+----------+-----------+\nThat tells us some details about this job, in particular that it finished correctly (the\nT\ncode).\nTip\nFor a list of status and error codes, check the upstream\nJob status and Error codes tables\n.\nTo see the full log of this specific job, we can use the\nlist\njoblog\njobid=7\ncommand. This is quite detailed, and the output below is truncated for brevity:\n+----------------------------------------------------------------------------------------------------+\n| logtext                                                                                              |\n+----------------------------------------------------------------------------------------------------+\n| bacula-server-dir JobId 7: Start Backup JobId 7, Job=HomeBackup.2025-10-20_20.21.08_03               |\n| bacula-server-dir JobId 7: Connected to Storage \"FileBackup\" at bacula-server.lxd:9103 with TLS      |\n| bacula-server-dir JobId 7: Using Device \"FileBackup\" to write.                                       |\n...\n  Build OS:               x86_64-pc-linux-gnu ubuntu 25.10\n  JobId:                  7\n  Job:                    HomeBackup.2025-10-20_20.21.08_03\n  Backup Level:           Incremental, since=2025-10-20 18:11:00\n  Client:                 \"bacula-server-fd\" 15.0.3 (25Mar25) x86_64-pc-linux-gnu,ubuntu,25.10\n  FileSet:                \"Home Set\" 2025-10-20 16:27:31\n  Pool:                   \"File\" (From Job resource)\n  Catalog:                \"MyCatalog\" (From Client resource)\n  Storage:                \"FileBackup\" (From Job resource)\n...\n  Non-fatal FD errors:    0\n  SD Errors:              0\n  FD termination status:  OK\n  SD termination status:  OK\n  Termination:            Backup OK                                                                    |\n...\nIf we inspect the backup target location on the Storage server (which in this deployment is the same as the Director), we can see that a volume file was created:\n-\nrw\n-\nr\n-----\n1\nbacula\ntape\n345\nK\nOct\n20\n20\n:\n21\n/\nstorage\n/\nbackups\n/\nVol\n-\n0001\nRestoring a backup\n¶\nSo what is it that was backed up? This job used the\nHome\nSet\n, so we expect to see files from the\n/home\ndirectory. To see what are the contents of that backup job, we can use the\nrestore\ncommand (the\nRestoreFiles\njob should never be executed directly). Below is the output of an interactive\nrestore\nsession where we selected the option “Select the most recent backup for a client”:\nFirst you select one or more JobIds that contain files\nto be restored. You will be presented several methods\nof specifying the JobIds. Then you will be allowed to\nselect which files from those JobIds are to be restored.\n\nTo select the JobIds, you have the following choices:\n     1: List last 20 Jobs run\n...\n     5: Select the most recent backup for a client\n...\nSelect item:  (1-14): 5\nAutomatically selected Client: bacula-server-fd\nAutomatically selected FileSet: Home Set\n+-------+-------+----------+----------+---------------------+------------+\n| jobid | level | jobfiles | jobbytes | starttime           | volumename |\n+-------+-------+----------+----------+---------------------+------------+\n|     4 | F     |      266 |  312,756 | 2025-10-20 17:34:43 | Vol-0001   |\n|     5 | I     |        3 |       32 | 2025-10-20 18:11:00 | Vol-0001   |\n+-------+-------+----------+----------+---------------------+------------+\nYou have selected the following JobIds: 4,5\n...\nYou are now entering file selection mode where you add (mark) and\nremove (unmark) files to be restored. No files are initially added, unless\nyou used the \"all\" keyword on the command line.\nEnter \"done\" to leave this mode.\n\ncwd is: /\n$\nHere we can navigate the filesystem and inspect which files are part of the backup:\n$ dir\ndrwxr-xr-x   1 root     root              12  2025-10-20 14:03:44  /home/\n$ cd home/ubuntu\ncwd is: /home/ubuntu/\n$ dir\n-rw-------   1 ubuntu   ubuntu            32  2025-10-20 18:10:51  /home/ubuntu/.bash_history\n-rw-r--r--   1 ubuntu   ubuntu           220  2025-10-20 14:03:50  /home/ubuntu/.bash_logout\n-rw-r--r--   1 ubuntu   ubuntu          3830  2025-10-20 14:03:50  /home/ubuntu/.bashrc\n-rw-r--r--   1 ubuntu   ubuntu           807  2025-10-20 14:03:44  /home/ubuntu/.profile\ndrwx------   1 ubuntu   ubuntu            30  2025-10-20 14:03:45  /home/ubuntu/.ssh/\n-rw-rw-r--   1 ubuntu   ubuntu             0  2025-10-20 18:10:50  /home/ubuntu/this-is-on-the-server.txt\nTo restore a file, we use the\nmark\ncommand on it. For example, let’s restore\n/home/ubuntu/.tmux.conf\n:\n$ mark .tmux.conf\n1 file marked.\n$ done\nBootstrap records written to /var/lib/bacula/bacula-server-dir.restore.1.bsr\n\nThe Job will require the following (*=>InChanger):\n   Volume(s)                 Storage(s)                SD Device(s)\n===========================================================================\n\n    Vol-0001                  FileBackup                FileBackup\n\nVolumes marked with \"*\" are in the Autochanger.\n\n\n1 file selected to be restored.\n\nRun Restore job\nJobName:         RestoreFiles\nBootstrap:       /var/lib/bacula/bacula-server-dir.restore.1.bsr\nWhere:           /storage/restore\nReplace:         Always\nFileSet:         Home Set\nBackup Client:   bacula-server-fd\nRestore Client:  bacula-server-fd\nStorage:         FileBackup\nWhen:            2025-10-20 21:02:37\nCatalog:         MyCatalog\nPriority:        10\nPlugin Options:  *None*\nOK to run? (Yes/mod/no):\nNow we have some choices. Notice how the\nRestoreFiles\njob was automatically selected. That’s the only job of the type\nRestore\nthat we defined in the Director configuration earlier. It has certain default values, and we can either accept those (by replying\nyes\n), or modify them (by replying\nmod\n).\nIf we accept these default, the marked files will be restored to the\n/storage/restore\npath on the\nbacula-server-fd\nsystem:\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=8\n*\nAnd indeed, if we inspect that location, we see the file that we marked for restoration:\n-rw-r--r-- 1 ubuntu ubuntu 2.4K Oct 20 14:03 /storage/restore/home/ubuntu/.tmux.conf\nIf we wanted to restore it to its original place, for example, if the user mistakenly deleted it and wanted it back, we would select the\nmod\noption to change where the file should be placed:\nOK to run? (Yes/mod/no): mod\nParameters to modify:\n     1: Level\n     2: Storage\n     3: Job\n     4: FileSet\n     5: Restore Client\n     6: When\n     7: Priority\n     8: Bootstrap\n     9: Where\n    10: File Relocation\n    11: Replace\n    12: JobId\n    13: Plugin Options\nSelect parameter to modify (1-13): 9\nPlease enter the full path prefix for restore (/ for none): /\nRun Restore job\nJobName:         RestoreFiles\nBootstrap:       /var/lib/bacula/bacula-server-dir.restore.2.bsr\nWhere:\nReplace:         Always\nFileSet:         Home Set\nBackup Client:   bacula-server-fd\nRestore Client:  bacula-server-fd\nStorage:         FileBackup\nWhen:            2025-10-20 21:11:55\nCatalog:         MyCatalog\nPriority:        10\nPlugin Options:  *None*\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=9\nBy giving a restoration prefix of\n/\n, we are essentially asking to restore the file at its original full path.\nAdding a client\n¶\nIf we want to start backing up a new system, we need to install the File Daemon on that system and include it in the Bacula Director. In this example, the new system we are adding is called\nworkstation1\n.\nFirst, on the system that we want to add, let’s install the client portion of Bacula, which is the File Daemon component:\nsudo apt install bacula-fd\nNext, update the Director resource in\n/etc/bacula/bacula-fd.conf\nto point at the existing Director we have already deployed:\nDirector\n{\nName\n=\nbacula\n-\nserver\n-\ndir\n# same as Director's Name on the Director server\nPassword\n=\n\"<randomly generated>\"\n}\nNotes:\nName\n: This has to be the same name set on the Director’s\n/etc/bacula/bacula-dir.conf\nfile, in the\nDirector\nresource over there. It’s not the hostname.\nPassword\n: The password was randomly generated when the\nbacula-fd\npackage was installed. This password has to match the password in the new\nClient\nresource that we will add to the Director next, so keep note of it.\nAlso in\n/etc/bacula/bacula-fd.conf\n, we have to remove or comment out the\nFDAddress\nparameter in the\nFileDaemon\nresource, so that this service will listen on all available network interfaces, and not just localhost:\nFileDaemon\n{\nName\n=\nworkstation1\n-\nfd\nFDport\n=\n9102\n# where we listen for the director\nWorkingDirectory\n=\n/\nvar\n/\nlib\n/\nbacula\nPid\nDirectory\n=\n/\nrun\n/\nbacula\nMaximum\nConcurrent\nJobs\n=\n20\nPlugin\nDirectory\n=\n/\nusr\n/\nlib\n/\nbacula\n#FDAddress = 127.0.0.1  # default is to listen on all interfaces\n}\nAnd finally, in the same file, update the\nMessages\nresource and update the Director name in there as well:\nMessages {\n    Name = Standard\n    director = bacula-server-dir = all, !skipped, !restored, !verified, !saved\n}\nWith these changes done, restart the File Daemon:\nsudo systemctl restart bacula-fd.service\nNow we switch to the Director system, where we have to let it know about this new Client that we just provisioned.\nIn\n/etc/bacula/bacula-dir.conf\n, add a new\nClient\nresource:\nClient\n{\nName\n=\nworkstation1\n-\nfd\nAddress\n=\nworkstation1\n.\nlxd\nFDPort\n=\n9102\nCatalog\n=\nMyCatalog\nPassword\n=\n\"<to be filled in>\"\n# password from bacula-fd.conf on workstation1-fd\nFile\nRetention\n=\n60\ndays\nJob\nRetention\n=\n6\nmonths\nAutoPrune\n=\nyes\n}\nNotes:\nName\n: The name has to match the name defined in the\nFileDaemon\nresource from\n/etc/bacula/bacula-fd.conf\nof the system we just added.\nPassword\n: The password has to be the same as the one defined in the\nDirector\nresource from\n/etc/bacula/bacula-fd.conf\nof that system.\nAddress\n: The hostname or IP of the system we added.\nThis makes the Director know how to reach the new client.\nNow we have to define a new job to backup files from this new client. Again on\n/etc/bacula/bacula-dir.conf\non the Director, let’s add a new\nJob\nresource:\nJob\n{\nName\n=\n\"BackupWorkstation\"\nJobDefs\n=\n\"DefaultJob\"\nClient\n=\nworkstation1\n-\nfd\n}\nThis job inherits all parameters from the\nDefaultJob\n, and just overrides the client.\nWith this done, we can restart the Director:\nsudo systemctl restart bacula-dir.service\nIf we now enter the Bacula console, we should be able to list the new client, and run its new backup job:\n*list clients\nAutomatically selected Catalog: MyCatalog\nUsing Catalog \"MyCatalog\"\n+----------+------------------+---------------+--------------+\n| clientid | name             | fileretention | jobretention |\n+----------+------------------+---------------+--------------+\n|        1 | bacula-server-fd |     5,184,000 |   15,552,000 |\n|        2 | workstation1-fd  |     5,184,000 |   15,552,000 |\n+----------+------------------+---------------+--------------+\nLet’s run the new\nBackupWorkstation\njob:\n*run\nUsing Catalog \"MyCatalog\"\nA job name must be specified.\nThe defined Job resources are:\n     1: HomeBackup\n     2: BackupWorkstation\n     3: BackupCatalog\n     4: RestoreFiles\nSelect Job resource (1-4): 2\nRun Backup job\nJobName:  BackupWorkstation\nLevel:    Incremental\nClient:   workstation1-fd\nFileSet:  Home Set\nPool:     File (From Job resource)\nStorage:  FileBackup (From Job resource)\nWhen:     2025-10-21 21:02:48\nPriority: 10\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=14\nYou have messages.\nAnd for a quick check of the contents (for testing, there was a file called\nthis-is-workstation1.txt\nin\n/home/ubuntu\non that system):\n*restore\n...\n     5: Select the most recent backup for a client\n...\nSelect item:  (1-14): 5\nDefined Clients:\n     1: bacula-server-fd\n     2: workstation1-fd\nSelect the Client (1-2): 2\n...\n$ cd home/ubuntu\ncwd is: /home/ubuntu/\n$ dir this*\n-rw-rw-r--   1 ubuntu   ubuntu             0  2025-10-21 21:02:08  /home/ubuntu/this-is-workstation1.txt\nFurther reading\n¶\nFor more Bacula configuration options, refer to the\nBacula documentation\n.\nThe\nBacula home page\ncontains the latest Bacula news and developments.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 5863}}
{"id": "fcd71d4d11", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-etckeeper/", "title": "etckeeper - Ubuntu Server documentation", "text": "etckeeper - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\netckeeper\n¶\netckeeper\nallows the contents of\n/etc\nto be stored in a Version Control System (VCS) repository. It integrates with APT and automatically commits changes to\n/etc\nwhen packages are installed or upgraded.\nPlacing\n/etc\nunder version control is considered an industry best practice, and the goal of etckeeper is to make this process as painless as possible.\nInstall etckeeper\n¶\nInstall etckeeper by entering the following in a terminal:\nsudo\napt\ninstall\netckeeper\nInitialize etckeeper\n¶\nThe main configuration file,\n/etc/etckeeper/etckeeper.conf\n, is fairly simple. The main option defines which VCS to use, and by default etckeeper is configured to use git.\nThe repository is automatically initialized (and committed for the first time) during package installation. It is possible to undo this by entering the following command:\nsudo\netckeeper\nuninit\nConfigure autocommit frequency\n¶\nBy default, etckeeper will commit uncommitted changes made to\n/etc\non a daily basis. This can be disabled using the\nAVOID_DAILY_AUTOCOMMITS\nconfiguration option.\nIt will also automatically commit changes before and after package installation. For a more precise tracking of changes, it is recommended to commit your changes manually, together with a commit message, using:\nsudo\netckeeper\ncommit\n\"Reason for configuration change\"\nThe\nvcs\netckeeper command provides access to any subcommand of the VCS that etckeeper is configured to run. It will be run in\n/etc\n. For example, in the case of git:\nsudo\netckeeper\nvcs\nlog\n/etc/passwd\nTo demonstrate the integration with the package management system (APT), install\npostfix\n:\nsudo\napt\ninstall\npostfix\nWhen the installation is finished, all the\npostfix\nconfiguration files should be committed to the repository:\n[master 5a16a0d] committing changes in /etc made by \"apt install postfix\"\n Author: Your Name <xyz@example.com>\n 36 files changed, 2987 insertions(+), 4 deletions(-)\n create mode 100755 init.d/postfix\n create mode 100644 insserv.conf.d/postfix\n create mode 100755 network/if-down.d/postfix\n create mode 100755 network/if-up.d/postfix\n create mode 100644 postfix/dynamicmaps.cf\n create mode 100644 postfix/main.cf\n create mode 100644 postfix/main.cf.proto\n create mode 120000 postfix/makedefs.out\n create mode 100644 postfix/master.cf\n create mode 100644 postfix/master.cf.proto\n create mode 100755 postfix/post-install\n create mode 100644 postfix/postfix-files\n create mode 100755 postfix/postfix-script\n create mode 100755 ppp/ip-down.d/postfix\n create mode 100755 ppp/ip-up.d/postfix\n create mode 120000 rc0.d/K01postfix\n create mode 120000 rc1.d/K01postfix\n create mode 120000 rc2.d/S01postfix\n create mode 120000 rc3.d/S01postfix\n create mode 120000 rc4.d/S01postfix\n create mode 120000 rc5.d/S01postfix\n     create mode 120000 rc6.d/K01postfix\n     create mode 100755 resolvconf/update-libc.d/postfix\n     create mode 100644 rsyslog.d/postfix.conf\n     create mode 120000 systemd/system/multi-user.target.wants/postfix.service\n     create mode 100644 ufw/applications.d/postfix\nFor an example of how\netckeeper\ntracks manual changes, add new a host to\n/etc/hosts\n. Using git you can see which files have been modified:\nsudo\netckeeper\nvcs\nstatus\nand how:\nsudo\netckeeper\nvcs\ndiff\nIf you are happy with the changes you can now commit them:\nsudo\netckeeper\ncommit\n\"added new host\"\nResources\n¶\nSee the\netckeeper\nsite for more details on using etckeeper.\nFor documentation on the git VCS tool see\nthe Git website\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 503}}
{"id": "0ae63b7f60", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-gitolite/", "title": "How to set up gitolite - Ubuntu Server documentation", "text": "How to set up gitolite - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to set up gitolite\n¶\nGitolite\nGitolite allows you to setup git hosting on a central server, with fine-grained access control and many more powerful features.\nYou can use your served repositories as\ngit\nremote\nin the form of\ngit@yourserver:some/repo/path\n.\nGitolite stores\n“bare git”\nrepositories at a location of your choice, usually\n/home/git\n.\nIt has its independent user realm, each user is created by assigning their\nSSH-key\n.\nThe repos itself are owned by one system user of your choice, usually\ngit\n.\nInstall a gitolite server\n¶\nGitolite can be installed with the following command.\nThe install automation will ask for a path or content of your\nadmin\nssh key.\nFor a better understanding of your setup we recommend to leave the prompt empty for full control of your setup (so you can use\ngit\nas the username, and customize the storage path).\nsudo\napt\ninstall\ngitolite3\nConfigure gitolite\n¶\nGitolite stores its configuration in a git repository (called\ngitolite-admin\n), so there’s no configuration in\n/etc\n.\nThis configuration repository manages all other git repos, users and their permissions.\nCreate a\ngit\nuser for gitolite to use for the service (you can adjust the git repo storage path as the\n--home\ndirectory):\nsudo\nuseradd\n--system\n--home\n/home/git\ngit\nTo access the config repository, we now add the administrator’s public\nSSH-key\n.\nIf you have not yet configured an SSH key, refer to the section on\nSSH keys in our OpenSSH guide\n.\nWe copy it to\n/tmp\nso our\ngit\nuser can read the file to import it.\nPlease adjust the path to the desired admin user’s\nSSH-key\n(and algorithm, like\nid_rsa.pub\n).\ncp\n~/.ssh/id_ed25519.pub\n/tmp/admin.pub\nAs the\ngit\nuser, let’s import the administrator’s key into gitolite (it will get the\nadmin\nusername due to that key’s filename).\nsudo\n-i\n-u\ngit\ngitolite\nsetup\n-pk\n/tmp/admin.pub\nWhat this creates:\nthe management repo in\n~git/repositories/gitolite-admin.git\na global config in\n~git/.gitolite.rc\n~git/projects.list\nas repo overview\n~git/.ssh/authorized_keys\nwith\ncommand=\nto force gitolite over\nssh\nlater it will contain the\nssh\npublic key for each user you configured\nTo try if the setup worked, try\nssh\nas the user owning the admin key we just added, so see the\ngitolite repo overview\n:\nssh\ngit@yourserver\nhello\nadmin\n,\nthis\nis\ngit\n@your\n-\ngitolite\n-\nserver\nrunning\ngitolite3\nR\nW\ngitolite\n-\nadmin\nR\nW\ntesting\nManaging gitolite users and repositories\n¶\nTo configure gitolite users, repositories and permissions, clone the configuration repository.\n$yourserver\ncan be an ip-address, hostname, or just\nlocalhost\nfor your current machine.\ngit\nclone\ngit@\n$yourserver\n:gitolite-admin.git\nTo apply configuration change, commit them in the repo and\npush the changes\nback to the server with:\ngit\ncommit\n-a\ngit\npush\norigin\nmaster\nThe\ngitolite-admin\ncontains two subdirectories:\nkeydir\n(which contains the list of users’ public SSH keys) and\nconf\n(which contains configuration files).\nTo\nadd a gitolite user\n(it’s virtual - not a system username), obtain their SSH public key (from\n~user/.ssh/id_<name>.pub\n) and add it to the\nkeydir\ndirectory as\n<desired-username>.pub\n.\nTo\ndelete a gitolite user\n, you only need to delete their public key files.\nTo manage repositories and groups in\nconf/gitolite.conf\n, specify the the list of repositories followed by some access rules.\nHave an example:\n# gitolite config\n# users are created by their public key in keydir/$username.pub\n\n# group creation\n@bestproject          = name1 name2\n@projectwatchers      = name3 @bestproject\n\n# this repo itself\nrepo    gitolite-admin\n        RW+     =   admin\n        R       =   alice\n\n# a repo with access to anybody\nrepo    testing\n        RW+     = @all\n\n# a repo with special privileges, to tags and branches\nrepo    some/awesome/project\n        RW                      =   alice @bestproject\n        RW+                     =   bob\n        RW+   dev/              =   @bestproject\n        R                       =   @projectwatchers carol\n# bestproject members and alice can push code (but not force-push)\n# bestproject members can force-push branches starting with dev/\n# bob can forcepush anything\n# projectwatchers and carol have readonly access\nFor more advanced permission configuration (restricting tags, branches, …), please see the examples in the upstream documentation\npage 1\nand\npage 2\n.\nUsing your server\n¶\nNow you can use your newly set up gitolite server as a regular\ngit\nremote\n.\nOnce a user is created and has permissions, they can access the repositories.\nAs a fresh clone:\ngit\nclone\ngit@\n$server\n:some/awesome/project.git\nOr as a remote to an existing repository:\ngit\nremote\nadd\ngitolite\ngit@\n$server\n:some/awesome/project.git\nFurther reading\n¶\nGitolite’s code repository\nprovides access to source code\nGitolite’s documentation\nincludes more detailed configuration guides and a “fool-proof setup”, with how-tos for common tasks\nGitolite’s maintainer has written a book,\nGitolite Essentials\n, for more in-depth information about the software\nGeneral information about\ngit\nitself can be found at the\nGit homepage", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:14Z", "original_len_words": 822}}
{"id": "d211a5afa7", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-rsnapshot/", "title": "How to install and configure rsnapshot - Ubuntu Server documentation", "text": "How to install and configure rsnapshot - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure rsnapshot\n¶\nrsnapshot\nis an rsync-based\nfilesystem\nsnapshot utility. It can take incremental backups of local and remote filesystems for any number of machines. rsnapshot makes extensive use of hard links, so disk space is only used when absolutely necessary. It leverages the power of rsync to create scheduled, incremental backups.\nInstall rsnapshot\n¶\nTo install\nrsnapshot\nopen a terminal shell and run:\nsudo\napt-get\ninstall\nrsnapshot\nIf you want to backup a remote filesystem, the rsnapshot server needs to be able to access the target machine over SSH without password. For more information on how to enable this please see\nOpenSSH documentation\n. If the backup target is a local filesystem there is no need to set up OpenSSH.\nConfigure rsnapshot\n¶\nThe\nrsnapshot\nconfiguration resides in\n/etc/rsnapshot.conf\n. Below you can find some of the options available there.\nThe root directory where all snapshots will be stored is found at:\nsnapshot_root\n/var/cache/rsnapshot/\nNumber of backups to keep\n¶\nSince\nrsnapshot\nuses incremental backups, we can afford to keep older backups for a while before removing them. You set these up under the\nBACKUP\nLEVELS\n/\nINTERVALS\nsection. You can tell\nrsnapshot\nto retain a specific number of backups of each kind of interval.\nretain\ndaily\n6\nretain\nweekly\n7\nretain\nmonthly\n4\nIn this example we will keep 6 snapshots of our daily strategy, 7 snapshots of our weekly strategy, and 4 snapshots of our monthly strategy. These data will guide the rotation made by\nrsnapshot\n.\nRemote machine access\n¶\nIf you are accessing a remote machine over SSH and the port to bind is not the default (port\n22\n), you need to set the following variable with the port number:\nssh_args\n-p\n22222\nWhat to backup\n¶\nNow the most important part; you need to decide what you would like to backup.\nIf you are backing up locally to the same machine, this is as easy as specifying the directories that you want to save and following it with\nlocalhost/\nwhich will be a sub-directory in the\nsnapshot_root\nthat you set up earlier.\nbackup\n/home/\nlocalhost/\nbackup\n/etc/\nlocalhost/\nbackup\n/usr/local/\nlocalhost/\nIf you are backing up a remote machine you just need to tell\nrsnapshot\nwhere the server is and which directories you would like to back up.\nbackup\nroot@example.com:/home/\nexample.com/\n+rsync_long_args\n=\n--bwlimit\n=\n16\n,exclude\n=\ncore\nbackup\nroot@example.com:/etc/\nexample.com/\nexclude\n=\nmtab,exclude\n=\ncore\nAs you can see, you can pass extra rsync parameters (the\n+\nappends the parameter to the default list – if you remove the\n+\nsign you override it) and also exclude directories.\nYou can check the comments in\n/etc/rsnapshot.conf\nand the\nrsnapshot(1)\nmanual page for more options.\nTest configuration\n¶\nAfter modifying the configuration file, it is good practice to check if the syntax is OK:\nsudo\nrsnapshot\nconfigtest\nYou can also test your backup levels with the following command:\nsudo\nrsnapshot\n-t\ndaily\nIf you are happy with the output and want to see it in action you can run:\nsudo\nrsnapshot\ndaily\nScheduling backups\n¶\nWith\nrsnapshot\nworking correctly with the current configuration, the only thing left to do is schedule it to run at certain intervals. We will use cron to make this happen since\nrsnapshot\nincludes a default cron file in\n/etc/cron.d/rsnapshot\n. If you open this file there are some entries commented out as reference.\n0 4  * * *           root    /usr/bin/rsnapshot daily\n0 3  * * 1           root    /usr/bin/rsnapshot weekly\n0 2  1 * *           root    /usr/bin/rsnapshot monthly\nThe settings above added to\n/etc/cron.d/rsnapshot\nrun:\nThe\ndaily snapshot\neveryday at 4:00 am\nThe\nweekly snapshot\nevery Monday at 3:00 am\nThe\nmonthly snapshot\non the first of every month at 2:00 am\nFor more information on how to schedule a backup using cron please take a look at the\nExecuting\nwith\ncron\nsection in\nBackups - Shell Scripts\n.\nFurther reading\n¶\nrsnapshot official web page\nrsnapshot(1)\nmanual page\nrsync(1)\nmanual page", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:14Z", "original_len_words": 703}}
{"id": "c0e20c2499", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/", "title": "Containers - Ubuntu Server documentation", "text": "Containers - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nContainers\n¶\nContainers are a lightweight, portable virtualization technology. They package software together with its dependencies so that applications can run consistently even across different environments.\nHow to use LXD\nDocker for sysadmins\nHow to run rocks on your server\nSee also\n¶\nExplanation:\nVirtualisation and containers", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:14Z", "original_len_words": 78}}
{"id": "7a72012fd4", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/docker-for-system-admins/", "title": "Docker for system admins - Ubuntu Server documentation", "text": "Docker for system admins - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nDocker for system admins\n¶\nWe are going to explore set-ups for configuring storage, networking, and logging in the subsequent sections. This will also help you get familiarized with Docker command line interface (CLI).\nInstallation\n¶\nFirst, install Docker if it’s not already installed:\n$\nsudo\napt-get\ninstall\n-y\ndocker.io\ndocker-compose-v2\nConfiguring storage\n¶\nHow to configure volumes\n¶\nCreate a volume\n$\ndocker\nvolume\ncreate\nmy-vol\n\nmy-vol\nList volumes\n$\ndocker\nvolume\nls\n\nDRIVER\nVOLUME\nNAME\nlocal\nmy-vol\nInspect volume\n$\ndocker\nvolume\ninspect\nmy-vol\n[\n{\n\"CreatedAt\"\n:\n\"2023-10-25T00:53:24Z\"\n,\n\"Driver\"\n:\n\"local\"\n,\n\"Labels\"\n:\nnull\n,\n\"Mountpoint\"\n:\n\"/var/lib/docker/volumes/my-vol/_data\"\n,\n\"Name\"\n:\n\"my-vol\"\n,\n\"Options\"\n:\nnull\n,\n\"Scope\"\n:\n\"local\"\n}\n]\nRun a container and mount a volume\n$\ndocker\nrun\n--name\nweb-server\n-d\n\\\n--mount\nsource\n=\nmy-vol,target\n=\n/app\n\\\nubuntu/apache2\n\n0709c1b632801fddd767deddda0d273289ba423e9228cc1d77b2194989e0a882\nInspect your container to make sure the volume is mounted correctly:\ndocker\ninspect\nweb-server\n--format\n'{{ json .Mounts }}'\n|\njq\n.\n[\n{\n\"Type\"\n:\n\"volume\"\n,\n\"Name\"\n:\n\"my-vol\"\n,\n\"Source\"\n:\n\"/var/lib/docker/volumes/my-vol/_data\"\n,\n\"Destination\"\n:\n\"/app\"\n,\n\"Driver\"\n:\n\"local\"\n,\n\"Mode\"\n:\n\"z\"\n,\n\"RW\"\n:\ntrue\n,\n\"Propagation\"\n:\n\"\"\n}\n]\nBy default, all your volumes will be stored in\n/var/lib/docker/volumes\n.\nStop and remove the container, then remove its volume.\ndocker\nstop\nweb-server\ndocker\nrm\nweb-server\ndocker\nvolume\nrm\nmy-vol\nHow to configure bind mounts\n¶\nCreate a Docker container and bind mount your host directory:\n$\ndocker\nrun\n-d\n\\\n--name\nweb-server\n\\\n--mount\ntype\n=\nbind,source\n=\n\"\n$(\npwd\n)\n\"\n,target\n=\n/app\n\\\nubuntu/apache2\n\n6f5378e34d6c6811702e16d047a5a80f18adbd9d8a14b11050ae3c3353bf8d2a\nInspect your container to check for the bind mount:\ndocker\ninspect\nweb-server\n--format\n'{{ json .Mounts }}'\n|\njq\n.\n[\n{\n\"Type\"\n:\n\"bind\"\n,\n\"Source\"\n:\n\"/root\"\n,\n\"Destination\"\n:\n\"/app\"\n,\n\"Mode\"\n:\n\"\"\n,\n\"RW\"\n:\ntrue\n,\n\"Propagation\"\n:\n\"rprivate\"\n}\n]\nStop and remove the container\ndocker\nstop\nweb-server\ndocker\nrm\nweb-server\nHow to configure Tmpfs\n¶\nCreate a Docker container and mount a tmpfs:\n$\ndocker\nrun\n--name\nweb-server\n-d\n\\\n--mount\ntype\n=\ntmpfs,target\n=\n/app\n\\\nubuntu/apache2\n\n03483cc28166fc5c56317e4ee71904941ec5942071e7c936524f74d732b6a24c\nInspect your container to check for the tmpfs mount:\ndocker\ninspect\nweb-server\n--format\n'{{ json .Mounts }}'\n|\njq\n.\n[\n{\n\"Type\"\n:\n\"tmpfs\"\n,\n\"Source\"\n:\n\"\"\n,\n\"Destination\"\n:\n\"/app\"\n,\n\"Mode\"\n:\n\"\"\n,\n\"RW\"\n:\ntrue\n,\n\"Propagation\"\n:\n\"\"\n}\n]\nChoosing the right storage drivers\n¶\nBefore changing the configuration and restarting the daemon, make sure that the specified filesystem (zfs, btrfs, or device mapper) is mounted at\n/var/lib/docker\n.\nOtherwise, if you configure the Docker daemon to use a storage driver different from the filesystem mounted at\n/var/lib/docker\n, a failure will happen. The Docker daemon expects that\n/var/lib/docker\nis correctly set up when it starts.\nCheck the current storage driver\n$\ndocker\ninfo\n|\ngrep\n\"Storage Driver\"\nStorage\nDriver:\noverlay2\nEnsure the required Filesystem is available. We will be using the ZFS Filesystem.\n$\napt\ninstall\nzfsutils-linux\n-y\n# Install ZFS\n$\nfallocate\n-l\n5G\n/zfs-pool.img\n# Create a 5GB file\n$\nzpool\ncreate\nmypool\n/zfs-pool.img\n# Create a ZFS pool\n$\nzfs\ncreate\n-o\nmountpoint\n=\n/var/lib/docker\nmypool/docker\n# Create a ZFS dataset and mount it to dockers directory, \"/var/lib/docker\".\n$\nzfs\nlist\n# Verify that it mounted successfully\nNAME\nUSED\nAVAIL\nREFER\nMOUNTPOINT\nmypool\n162K\n4\n.36G\n24K\n/mypool\nmypool/docker\n39K\n4\n.36G\n39K\n/var/lib/docker\nChange the storage driver\nStop the docker daemon\nsystemctl\nstop\ndocker\nEdit\n/etc/docker/daemon.json\nusing your favorite editor, then update the storage driver value to\nzfs\n.\nvim\n/etc/docker/daemon.json\n{\n\"storage-driver\"\n:\n\"zfs\"\n}\nRestart the docker daemon\nsystemctl\nrestart\ndocker\nVerify the change\n$\ndocker\ninfo\n|\ngrep\n\"Storage Driver\"\nStorage\nDriver:\nzfs\nConfiguring networking\n¶\nThis is how you can create a user-defined network using the Docker CLI:\nCreate a network\n$\ndocker\nnetwork\ncreate\n--driver\nbridge\nmy-net\n\nD84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\nList networks\n$\ndocker\nnetwork\nls\n\nNETWORK\nID\nNAME\nDRIVER\nSCOPE\n1f55a8891c4a\nbridge\nbridge\nlocal\n9ca94be2c1a0\nhost\nhost\nlocal\nd84efaca11d6\nmy-net\nbridge\nlocal\n5d300e6a07b1\nnone\nnull\nlocal\nInspect the network we created\ndocker\nnetwork\ninspect\nmy-net\n[\n{\n\"Name\"\n:\n\"my-net\"\n,\n\"Id\"\n:\n\"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\"\n,\n\"Created\"\n:\n\"2023-10-25T22:18:52.972569338Z\"\n,\n\"Scope\"\n:\n\"local\"\n,\n\"Driver\"\n:\n\"bridge\"\n,\n\"EnableIPv6\"\n:\nfalse\n,\n\"IPAM\"\n:\n{\n\"Driver\"\n:\n\"default\"\n,\n\"Options\"\n:\n{},\n\"Config\"\n:\n[\n{\n\"Subnet\"\n:\n\"172.18.0.0/16\"\n,\n\"Gateway\"\n:\n\"172.18.0.1\"\n}\n]\n},\n\"Internal\"\n:\nfalse\n,\n\"Attachable\"\n:\nfalse\n,\n\"Ingress\"\n:\nfalse\n,\n\"ConfigFrom\"\n:\n{\n\"Network\"\n:\n\"\"\n},\n\"ConfigOnly\"\n:\nfalse\n,\n\"Containers\"\n:\n{},\n\"Options\"\n:\n{},\n\"Labels\"\n:\n{}\n}\n]\nContainers can connect to a defined network when they are created (via\ndocker\nrun\n) or at any time of its lifecycle.\nConnecting a new container to an existing network\n¶\n$\ndocker\nrun\n-d\n--name\nc1\n--network\nmy-net\nubuntu/apache2\n\nC7aa78f45ce3474a276ca3e64023177d5984b3df921aadf97e221da8a29a891e\nView the network connected to the container\ndocker\ninspect\nc1\n--format\n'{{ json .NetworkSettings }}'\n|\njq\n.\n{\n\"Bridge\"\n:\n\"\"\n,\n\"SandboxID\"\n:\n\"ee1cc10093fdfdf5d4a30c056cef47abbfa564e770272e1e5f681525fdd85555\"\n,\n\"HairpinMode\"\n:\nfalse\n,\n\"LinkLocalIPv6Address\"\n:\n\"\"\n,\n\"LinkLocalIPv6PrefixLen\"\n:\n0\n,\n\"Ports\"\n:\n{\n\"80/tcp\"\n:\nnull\n},\n\"SandboxKey\"\n:\n\"/var/run/docker/netns/ee1cc10093fd\"\n,\n\"SecondaryIPAddresses\"\n:\nnull\n,\n\"SecondaryIPv6Addresses\"\n:\nnull\n,\n\"EndpointID\"\n:\n\"\"\n,\n\"Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"IPAddress\"\n:\n\"\"\n,\n\"IPPrefixLen\"\n:\n0\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"MacAddress\"\n:\n\"\"\n,\n\"Networks\"\n:\n{\n\"my-net\"\n:\n{\n\"IPAMConfig\"\n:\nnull\n,\n\"Links\"\n:\nnull\n,\n\"Aliases\"\n:\n[\n\"c7aa78f45ce3\"\n],\n\"NetworkID\"\n:\n\"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\"\n,\n\"EndpointID\"\n:\n\"1cb76d44a484d302137bb4b042c8142db8e931e0c63f44175a1aa75ae8af9cb5\"\n,\n\"Gateway\"\n:\n\"172.18.0.1\"\n,\n\"IPAddress\"\n:\n\"172.18.0.2\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"MacAddress\"\n:\n\"02:42:ac:12:00:02\"\n,\n\"DriverOpts\"\n:\nnull\n}\n}\n}\nConnecting a running container to an existing network\n¶\nMake a running container connect to the existing network\nCreate the container\n$\ndocker\nrun\n-d\n--name\nc2\nubuntu/nginx\n\nFea22fbb6e3685eae28815f3ad8c8a655340ebcd6a0c13f3aad0b45d71a20935\nConnect the running container to the network and verify that it’s connected.\ndocker\nnetwork\nconnect\nmy-net\nc2\ndocker\ninspect\nc2\n--format\n'{{ json .NetworkSettings }}'\n|\njq\n.\n{\n\"Bridge\"\n:\n\"\"\n,\n\"SandboxID\"\n:\n\"82a7ea6efd679dffcc3e4392e0e5da61a8ccef33dd78eb5381c9792a4c01f366\"\n,\n\"HairpinMode\"\n:\nfalse\n,\n\"LinkLocalIPv6Address\"\n:\n\"\"\n,\n\"LinkLocalIPv6PrefixLen\"\n:\n0\n,\n\"Ports\"\n:\n{\n\"80/tcp\"\n:\nnull\n},\n\"SandboxKey\"\n:\n\"/var/run/docker/netns/82a7ea6efd67\"\n,\n\"SecondaryIPAddresses\"\n:\nnull\n,\n\"SecondaryIPv6Addresses\"\n:\nnull\n,\n\"EndpointID\"\n:\n\"490c15cf3bcb149dd8649e3ac96f71addd13f660b4ec826dc39e266184b3f65b\"\n,\n\"Gateway\"\n:\n\"172.17.0.1\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"IPAddress\"\n:\n\"172.17.0.3\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"MacAddress\"\n:\n\"02:42:ac:11:00:03\"\n,\n\"Networks\"\n:\n{\n\"bridge\"\n:\n{\n\"IPAMConfig\"\n:\nnull\n,\n\"Links\"\n:\nnull\n,\n\"Aliases\"\n:\nnull\n,\n\"NetworkID\"\n:\n\"1f55a8891c4a523a288aca8881dae0061f9586d5d91c69b3a74e1ef3ad1bfcf4\"\n,\n\"EndpointID\"\n:\n\"490c15cf3bcb149dd8649e3ac96f71addd13f660b4ec826dc39e266184b3f65b\"\n,\n\"Gateway\"\n:\n\"172.17.0.1\"\n,\n\"IPAddress\"\n:\n\"172.17.0.3\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"MacAddress\"\n:\n\"02:42:ac:11:00:03\"\n,\n\"DriverOpts\"\n:\nnull\n},\n\"my-net\"\n:\n{\n\"IPAMConfig\"\n:\n{},\n\"Links\"\n:\nnull\n,\n\"Aliases\"\n:\n[\n\"fea22fbb6e36\"\n],\n\"NetworkID\"\n:\n\"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\"\n,\n\"EndpointID\"\n:\n\"17856b7f6902db39ff6ab418f127d75d8da597fdb8af0a6798f35a94be0cb805\"\n,\n\"Gateway\"\n:\n\"172.18.0.1\"\n,\n\"IPAddress\"\n:\n\"172.18.0.3\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"MacAddress\"\n:\n\"02:42:ac:12:00:03\"\n,\n\"DriverOpts\"\n:\n{}\n}\n}\n}\nThe container c2 is connected to two networks\nbridge\nand\nmy-net\n.\nThe default network created by the Docker daemon is called\nbridge\nusing the\nbridge network driver\n.\nModifying the default network “bridge”\n¶\nA system administrator can modify this default networks IP address by editing\n/etc/docker/daemon.json\nand including the below into the JSON object\nvim\n/etc/docker/daemon.json\n{\n\"bip\"\n:\n\"192.168.1.1/24\"\n,\n\"fixed-cidr\"\n:\n\"192.168.1.0/25\"\n,\n\"fixed-cidr-v6\"\n:\n\"2001:db8::/64\"\n,\n\"mtu\"\n:\n1500\n,\n\"default-gateway\"\n:\n\"192.168.1.254\"\n,\n\"default-gateway-v6\"\n:\n\"2001:db8:abcd::89\"\n,\n\"dns\"\n:\n[\n\"10.20.1.2\"\n,\n\"10.20.1.3\"\n]\n}\nRestart the Docker daemon\nsystemctl\nrestart\ndocker\nVerify your changes\ndocker\nnetwork\ninspect\nbridge\nExposing a container port to the host\n¶\nAfter deciding how you are going to manage the network and selecting the most appropriate driver, there are some specific deployment details that a system administrator has to bear in mind when running containers.\nExposing ports of any system is always a concern, since it increases the surface for malicious attacks. For containers, we also need to be careful, analysing whether we really need to publish ports to the host. For instance, if the goal is to allow containers to access a specific port from another container, there is no need to publish any port to the host. This can be solved by connecting all the containers to the same network. You should publish ports of a container to the host only if you want to make it available to non-Docker workloads. When a container is created no port is published to the host, the option\n--publish\n(or\n-p\n) should be passed to\ndocker\nrun\nor\ndocker\ncreate\nlisting which port will be exposed and how.\nThe\n--publish\noption of Docker CLI accepts the following options:\nFirst, the host port that will be used to publish the container’s port. It can also contain the IP address of the host. For example,\n0.0.0.0:8080\n.\nSecond, the container’s port to be published. For example,\n80\n.\nThird (optional), the type of port that will be published which can be TCP or UDP. For example,\n80/tcp\nor\n80/udp\n.\nAn example of how to publish port\n80\nof a container to port\n8080\nof the host:\nCreate a container and expose it’s port to the host\n$\ndocker\nrun\n-d\n--name\nweb-server\n--publish\n8080\n:80\nubuntu/nginx\n\nf451aa1990db7d2c9b065c6158e2315997a56a764b36a846a19b1b96ce1f3910\nView the containers network settings\ndocker\ninspect\nweb-server\n--format\n'{{ json .NetworkSettings.Ports }}'\n|\njq\n.\n{\n\"80/tcp\"\n:\n[\n{\n\"HostIp\"\n:\n\"0.0.0.0\"\n,\n\"HostPort\"\n:\n\"8080\"\n},\n{\n\"HostIp\"\n:\n\"::\"\n,\n\"HostPort\"\n:\n\"8080\"\n}\n]\n}\nThe\nHostIp\nvalues are\n0.0.0.0\n(IPv4) and\n::\n(IPv6), and the service running in the container is accessible to everyone in the network (reaching the host), if you want to publish the port from the container and let the service be available just to the host you can use\n--publish\n127.0.0.1:8080:80\ninstead. The published port can be TCP or UDP and one can specify that passing\n--publish\n8080:80/tcp\nor\n--publish\n8080:80/udp\n.\nThe system administrator might also want to manually set the IP address or the\nhostname\nof the container. To achieve this, one can use the\n--ip\n(IPv4),\n--ip6\n(IPv6), and\n--hostname\noptions of the\ndocker\nnetwork\nconnect\ncommand to specify the desired values.\nAnother important aspect of networking with containers is the\nDNS\nservice. By default containers will use the DNS setting of the host, defined in\n/etc/resolv.conf\n. Therefore, if a container is created and connected to the default\nbridge\nnetwork it will get a copy of host’s\n/etc/resolv.conf\n. If the container is connected to a user-defined network, then it will use Docker’s embedded DNS server. The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host. In case the system administrator wants to configure the DNS service, the\ndocker\nrun\nand\ndocker\ncreate\ncommands have options to allow that, such as\n--dns\n(IP address of a DNS server) and\n--dns-opt\n(key-value pair representing a DNS option and its value). For more information, check the manpages of those commands.\nManaging logs\n¶\nThe default logging driver is called\njson\nfile\n, and the system administrator can change it to suite their needs.\nModifying the logging driver via the docker daemon file\n¶\nEdit the docker daemon file and update the logging driver\nvim\n/etc/docker/daemon.json\n{\n\"log-driver\"\n:\n\"journald\"\n}\nModifying the logging driver when creating a container\n¶\nAnother option is specifying the logging driver during container creation time:\nSpecify a log driver when executing a\ndocker\nrun\n$\ndocker\nrun\n-d\n--name\nweb-server\n--log-driver\n=\njournald\nubuntu/nginx\n\n1c08b667f32d8b834f0d9d6320721e07de5f22168cfc8a024d6e388daf486dfa\nVerify your configuration\ndocker\ninspect\nweb-server\n--format\n'{{ json .HostConfig.LogConfig }}'\n|\njq\n.\n{\n\"Type\"\n:\n\"journald\"\n,\n\"Config\"\n:\n{}\n}\nView logs\n$\ndocker\nlogs\nweb-server\n\n/docker-entrypoint.sh:\n/docker-entrypoint.d/\nis\nnot\nempty,\nwill\nattempt\nto\nperform\nconfiguration\n/docker-entrypoint.sh:\nLooking\nfor\nshell\nscripts\nin\n/docker-entrypoint.d/\n/docker-entrypoint.sh:\nLaunching\n/docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh:\nConfiguration\ncomplete\n;\nready\nfor\nstart\nup\nDepending on the driver you might also want to pass some options. You can do that via the CLI, passing\n--log-opt\nor in the daemon config file adding the key\nlog-opts\n. For more information check the\nlogging driver documentation\n.\nDocker CLI also provides the\ndocker\nlogs\nand\ndocker\nservice\nlogs\ncommands which allows one to check for the logs produced by a given container or service (set of containers) in the host. However, those two commands are functional only if the logging driver for the containers is\njson-file\n,\nlocal\nor\njournald\n. They are useful for debugging in general, but there is the downside of increasing the storage needed in the host.\nThe remote logging drivers are useful to store data in an external service/host, and they also avoid spending more disk space in the host to store log files. Nonetheless, sometimes, for debugging purposes, it is important to have log files locally. Considering that, Docker has a feature called “dual logging”, which is enabled by default, and even if the system administrator configures a logging driver different from\njson-file\n,\nlocal\nand\njournald\n, the logs will be available locally to be accessed via the Docker CLI. If this is not the desired behavior, the feature can be disabled in the\n/etc/docker/daemon.json\nfile:\n{\n\"log-driver\"\n:\n\"syslog\"\n,\n\"log-opts\"\n:\n{\n\"cache-disabled\"\n:\n\"true\"\n,\n\"syslog-address\"\n:\n\"udp://1.2.3.4:1111\"\n}\n}\nThe option\ncache-disabled\nis used to disable the “dual logging” feature. If you try to run\ndocker\nlogs\nwith that configuration you will get the following error:\n$\ndocker\nlogs\nweb-server\n\nError\nresponse\nfrom\ndaemon:\nconfigured\nlogging\ndriver\ndoes\nnot\nsupport\nreading\nResources\n¶\nTo read an explanatory guide to Docker storage, networking, and logging see:\nDocker storage, networking, and logging", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 2358}}
{"id": "6a7cce1a14", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/lxd-containers/", "title": "LXD containers - Ubuntu Server documentation", "text": "LXD containers - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nLXD containers\n¶\nLXD\n(pronounced lex-dee) is a modern, secure, and powerful system container and virtual machine manager.\nIt provides a unified experience for running and managing full Linux systems inside containers or virtual machines. You can access it via the command line, its\nbuilt-in graphical user interface\n, or a set of powerful\nREST APIs\n.\nLXD scales from one instance on a single machine\nto a cluster\nin a full data center rack, making it suitable for both development and production workloads. You can even use LXD to set up a small, scalable private cloud,\nsuch as a MicroCloud\n.\nThis document will focus on how to configure and administer LXD on Ubuntu systems using the command line. On Ubuntu Server Cloud images, LXD comes pre-installed.\nOnline resources\n¶\nYou can visit\nthe official LXD documentation\n, or get in touch with the LXD team in their\nUbuntu Discourse forum\n. The team also maintains a\nYouTube channel\nwith helpful videos.\nInstallation\n¶\nLXD is pre-installed on Ubuntu Server cloud images. On other systems, the\nlxd\npackage can be installed using:\nsudo\nsnap\ninstall\nlxd\nThis will install the self-contained LXD snap package.\nKernel preparation\n¶\nIn general, Ubuntu should have all the desired features enabled by default. One exception to this is that in order to enable swap accounting, the boot argument\nswapaccount=1\nmust be set. This can be done by appending it to the\nGRUB_CMDLINE_LINUX_DEFAULT=\nvariable in /etc/default/grub, then running ‘update-grub’ as root and rebooting.\nConfiguration\n¶\nIn order to use LXD, some basic settings need to be configured first. This is done by running\nlxd\ninit\n, which will allow you to choose:\nDirectory or\nZFS\ncontainer backend. If you choose ZFS, you can choose which block devices to use, or the size of a file to use as backing store.\nAvailability over the network.\nA ‘trust password’ used by remote clients to vouch for their client certificate.\nYou must run ‘lxd init’ as root. ‘lxc’ commands can be run as any user who is a member of group lxd. If user joe is not a member of group ‘lxd’, you may run:\nadduser\njoe\nlxd\nas root to change it. The new membership will take effect on the next login, or after running\nnewgrp\nlxd\nfrom an existing login.\nSee\nHow to initialize LXD\nin the LXD documentation for more information on the configuration settings. Also, refer to the definitive configuration provided with the source code for the server, container, profile, and device configuration.\nCreating your first container\n¶\nThis section will describe the simplest container tasks.\nCreating a container\n¶\nEvery new container is created based on either an image, an existing container, or a container snapshot. At install time, LXD is configured with the following image servers:\nubuntu\n: this serves official Ubuntu cloud image releases.\nubuntu-daily\n: this serves official Ubuntu cloud images of the daily development releases.\nubuntu-minimal\n: this serves official Ubuntu Minimal cloud image releases.\nimages\n: this server provides unofficial images for a variety of Linux distributions. This is not the recommended server for Ubuntu images.\nThe command to create and start a container is\nlxc\nlaunch\nremote\n:\nimage\ncontainername\nImages are identified by their hash, but are also aliased. The\nubuntu\nremote knows many aliases such as\n18.04\nand\nbionic\n. A list of all images available from the Ubuntu Server can be seen using:\nlxc\nimage\nlist\nubuntu\n:\nTo see more information about a particular image, including all the aliases it is known by, you can use:\nlxc\nimage\ninfo\nubuntu\n:\nbionic\nYou can generally refer to an Ubuntu image using the release name (\nbionic\n) or the release number (\n18.04\n). In addition,\nlts\nis an alias for the latest supported LTS release. To choose a different architecture, you can specify the desired architecture:\nlxc\nimage\ninfo\nubuntu\n:\nlts\n/\narm64\nNow, let’s start our first container:\nlxc\nlaunch\nubuntu\n:\nbionic\nb1\nThis will download the official current Bionic cloud image for your current architecture, then create a container named\nb1\nusing that image, and finally start it. Once the command returns, you can see it using:\nlxc\nlist\nlxc\ninfo\nb1\nand open a shell in it using:\nlxc\nexec\nb1\n--\nbash\nA convenient alias for the command above is:\nlxc\nshell\nb1\nThe try-it page mentioned above gives a full synopsis of the commands you can use to administer containers.\nNow that the\nbionic\nimage has been downloaded, it will be kept in sync until no new containers have been created based on it for (by default) 10 days. After that, it will be deleted.\nLXD server configuration\n¶\nBy default, LXD is socket activated and configured to listen only on a local UNIX socket. While LXD may not be running when you first look at the process listing, any LXC command will start it up. For instance:\nlxc\nlist\nThis will create your client certificate and contact the LXD server for a list of containers. To make the server accessible over the network you can set the http port using:\nlxc\nconfig\nset\ncore\n.\nhttps_address\n:\n8443\nThis will tell LXD to listen to port 8443 on all addresses.\nAuthentication\n¶\nBy default, LXD will allow all members of group\nlxd\nto talk to it over the UNIX socket. Communication over the network is authorized using server and client certificates.\nBefore client\nc1\nwishes to use remote\nr1\n,\nr1\nmust be registered using:\nlxc\nremote\nadd\nr1\nr1\n.\nexample\n.\ncom\n:\n8443\nThe fingerprint of r1’s certificate will be shown, to allow the user at c1 to reject a false certificate. The server in turn will verify that c1 may be trusted in one of two ways. The first is to register it in advance from any already-registered client, using:\nlxc\nconfig\ntrust\nadd\nr1\ncertfile\n.\ncrt\nNow when the client adds r1 as a known remote, it will not need to provide a password as it is already trusted by the server.\nThe other step is to configure a ‘trust password’ with\nr1\n, either at initial configuration using\nlxd\ninit\n, or after the fact using:\nlxc\nconfig\nset\ncore\n.\ntrust_password\nPASSWORD\nThe password can then be provided when the client registers\nr1\nas a known remote.\nBacking store\n¶\nLXD supports several backing stores. The recommended and the default backing store is\nzfs\n. If you already have a ZFS pool configured, you can tell LXD to use it during the\nlxd\ninit\nprocedure, otherwise a file-backed zpool will be created automatically. With ZFS, launching a new container is fast because the\nfilesystem\nstarts as a copy on write clone of the images’ filesystem. Note that unless the container is privileged (see below) LXD will need to change ownership of all files before the container can start, however this is fast and change very little of the actual filesystem data.\nThe other supported backing stores are described in detail in the\nStorage configuration\nsection of the LXD documentation.\nContainer configuration\n¶\nContainers are configured according to a set of profiles, described in the next section, and a set of container-specific configuration. Profiles are applied first, so that container specific configuration can override profile configuration.\nContainer configuration includes properties like the architecture, limits on resources such as CPU and RAM, security details including apparmor restriction overrides, and devices to apply to the container.\nDevices can be of several types, including UNIX character, UNIX block, network interface, or disk. In order to insert a host mount into a container, a ‘disk’ device type would be used. For instance, to mount\n/opt\nin container\nc1\nat\n/opt\n, you could use:\nlxc\nconfig\ndevice\nadd\nc1\nopt\ndisk\nsource\n=/\nopt\npath\n=\nopt\nSee:\nlxc\nhelp\nconfig\nfor more information about editing container configurations. You may also use:\nlxc\nconfig\nedit\nc1\nto edit the whole of\nc1\n’s configuration. Comments at the top of the configuration will show examples of correct syntax to help administrators hit the ground running. If the edited configuration is not valid when the editor is exited, then the editor will be restarted.\nProfiles\n¶\nProfiles are named collections of configurations which may be applied to more than one container. For instance, all containers created with\nlxc\nlaunch\n, by default, include the\ndefault\nprofile, which provides a network interface\neth0\n.\nTo mask a device which would be inherited from a profile but which should not be in the final container, define a device by the same name but of type ‘none’:\nlxc\nconfig\ndevice\nadd\nc1\neth1\nnone\nNesting\n¶\nContainers all share the same host kernel. This means that there is always an inherent trade-off between features exposed to the container and host security from malicious containers. Containers by default are therefore restricted from features needed to nest child containers. In order to run lxc or lxd containers under a lxd container, the\nsecurity.nesting\nfeature must be set to true:\nlxc\nconfig\nset\ncontainer1\nsecurity\n.\nnesting\ntrue\nOnce this is done,\ncontainer1\nwill be able to start sub-containers.\nIn order to run unprivileged (the default in LXD) containers nested under an unprivileged container, you will need to ensure a wide enough UID mapping. Please see the ‘UID mapping’ section below.\nLimits\n¶\nLXD supports flexible constraints on the resources which containers can consume. The limits come in the following categories:\nCPU: limit cpu available to the container in several ways.\nDisk: configure the priority of I/O requests under load\nRAM: configure memory and swap availability\nNetwork: configure the network priority under load\nProcesses: limit the number of concurrent processes in the container.\nFor a full list of limits known to LXD, see\nthe configuration documentation\n.\nUID mappings and privileged containers\n¶\nBy default, LXD creates unprivileged containers. This means that root in the container is a non-root UID on the host. It is privileged against the resources owned by the container, but unprivileged with respect to the host, making root in a container roughly equivalent to an unprivileged user on the host. (The main exception is the increased attack surface exposed through the system call interface)\nBriefly, in an unprivileged container, 65536 UIDs are ‘shifted’ into the container. For instance, UID 0 in the container may be 100000 on the host, UID 1 in the container is 100001, etc, up to 165535. The starting value for UIDs and\nGIDs\n, respectively, is determined by the ‘root’ entry the\n/etc/subuid\nand\n/etc/subgid\nfiles. (See the\nsubuid(5)\n) manual page.)\nIt is possible to request a container to run without a UID mapping by setting the\nsecurity.privileged\nflag to true:\nlxc\nconfig\nset\nc1\nsecurity\n.\nprivileged\ntrue\nNote however that in this case the root user in the container is the root user on the host.\nApparmor\n¶\nLXD confines containers by default with an apparmor profile which protects containers from each other and the host from containers. For instance this will prevent root in one container from signaling root in another container, even though they have the same uid mapping. It also prevents writing to dangerous, un-namespaced files such as many sysctls and\n/proc/sysrq-trigger\n.\nIf the apparmor policy for a container needs to be modified for a container\nc1\n, specific apparmor policy lines can be added in the\nraw.apparmor\nconfiguration key.\nSeccomp\n¶\nAll containers are confined by a default seccomp policy. This policy prevents some dangerous actions such as forced umounts, kernel module loading and unloading, kexec, and the\nopen_by_handle_at\nsystem call. The seccomp configuration cannot be modified, however a completely different seccomp policy – or none – can be requested using\nraw.lxc\n(see below).\nRaw LXC configuration\n¶\nLXD configures containers for the best balance of host safety and container usability. Whenever possible it is highly recommended to use the defaults, and use the LXD configuration keys to request LXD to modify as needed. Sometimes, however, it may be necessary to talk to the underlying lxc driver itself. This can be done by specifying LXC configuration items in the ‘raw.lxc’ LXD configuration key. These must be valid items as documented in the\nlxc.container.conf(5)\nmanual page.\nSnapshots\n¶\nContainers can be renamed and live-migrated using the\nlxc\nmove\ncommand:\nlxc\nmove\nc1\nfinal\n-\nbeta\nThey can also be snapshotted:\nlxc\nsnapshot\nc1\nYYYY\n-\nMM\n-\nDD\nLater changes to c1 can then be reverted by restoring the snapshot:\nlxc\nrestore\nu1\nYYYY\n-\nMM\n-\nDD\nNew containers can also be created by copying a container or snapshot:\nlxc\ncopy\nu1\n/\nYYYY\n-\nMM\n-\nDD\ntestcontainer\nPublishing images\n¶\nWhen a container or container snapshot is ready for consumption by others, it can be published as a new image using;\nlxc\npublish\nu1\n/\nYYYY\n-\nMM\n-\nDD\n--\nalias\nfoo\n-\n2.0\nThe published image will be private by default, meaning that LXD will not allow clients without a trusted certificate to see them. If the image is safe for public viewing (i.e. contains no private information), then the ‘public’ flag can be set, either at publish time using\nlxc\npublish\nu1\n/\nYYYY\n-\nMM\n-\nDD\n--\nalias\nfoo\n-\n2.0\npublic\n=\ntrue\nor after the fact using\nlxc\nimage\nedit\nfoo\n-\n2.0\nand changing the value of the public field.\nImage export and import\n¶\nImage can be exported as, and imported from, tarballs:\nlxc\nimage\nexport\nfoo\n-\n2.0\nfoo\n-\n2.0\n.\ntar\n.\ngz\nlxc\nimage\nimport\nfoo\n-\n2.0\n.\ntar\n.\ngz\n--\nalias\nfoo\n-\n2.0\n--\npublic\nTroubleshooting\n¶\nTo view debug information about LXD itself, on a systemd based host use\njournalctl\n-\nu\nlxd\nContainer logfiles for container c1 may be seen using:\nlxc\ninfo\nc1\n--\nshow\n-\nlog\nThe configuration file which was used may be found under\n/var/log/lxd/c1/lxc.conf\nwhile apparmor profiles can be found in\n/var/lib/lxd/security/apparmor/profiles/c1\nand seccomp profiles in\n/var/lib/lxd/security/seccomp/c1\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 2374}}
{"id": "a30bba0977", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/run-rocks-on-your-server/", "title": "How to run rocks on your server - Ubuntu Server documentation", "text": "How to run rocks on your server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to run rocks on your server\n¶\nDeploying rocks with Docker\n¶\nAs with any other OCI-compliant container image,\nrocks\ncan be deployed with your favorite container management tool. This section depicts a typical deployment workflow for a generic Grafana rock, using\nDocker\n.\nFirst, install Docker if it’s not already installed:\n$\nsudo\napt-get\ninstall\n-y\ndocker.io\ndocker-compose-v2\nWe can deploy a container with the\ndocker\nrun\ncommand. This command has a number of\npossible parameters\n. The\n“Usage” section\nof the Grafana rock’s documentation has a table with an overview of parameters specific to the image.\n$ sudo docker run -d --name grafana-container -e TZ=UTC -p 3000:3000 ubuntu/grafana:10.3.3-22.04_stable\nUnable to find image 'ubuntu/grafana:10.3.3-22.04_stable' locally\n10.3.3-22.04_stable: Pulling from ubuntu/grafana\nbccd10f490ab: Already exists \n549078d9d057: Pull complete \n6ef870aa8500: Pull complete \n2b475da7ccbd: Pull complete \nDigest: sha256:df566ef90ecb14267a459081949ee7b6693fa573b97a7134a9a6722207275caa\nStatus: Downloaded newer image for ubuntu/grafana:10.3.3-22.04_stable\n356e623ef2c16bc7d810bddccad8d7980f9c633aefc3a88bc8761eac4e1b1c50\nIn this particular case, we’re using:\n-d\nto run the container in the background.\nWe are also specifying a well-defined name for the container, with the\n--name\nparameter.\nWith\n-e\nwe are setting the container’s timezone (\nTZ\n) environment variable to\nUTC\n.\nWe also use\n-p\nto map port 3000 of the container to 3000 on localhost.\nThe last parameter indicates the name of the rock, as listed in Docker Hub. Notice that the image tag we requested has the\n_stable\nsuffix to indicate the image’s risk. This is called a\nChannel Tag\nand it follows a similar convention to\nsnap “channels”\n.\nThis container, named\ngrafana-container\n, serves Grafana 10.3.3 in an Ubuntu 22.04 LTS environment and can be accessed via local port 3000. Load the website up in your local web browser:\nIf you don’t have Firefox handy,\ncurl\ncan be used instead:\n$\ncurl\n-s\nhttp://localhost:3000/login\n|\ngrep\n\"<title>\"\n<title>Grafana</title>\nNow that we’ve tested the deployment of the Grafana rock as a single container, let’s clean it up:\n$\nsudo\ndocker\nps\nCONTAINER\nID\nIMAGE\nCOMMAND\nCREATED\nSTATUS\nPORTS\nNAMES\n356e623ef2c1\nubuntu/grafana:10.3.3-22.04_stable\n\"/bin/pebble enter -…\"\n17\nminutes\nago\nUp\n17\nminutes\n0\n.0.0.0:3000->3000/tcp,\n:::3000->3000/tcp\ngrafana-container\nWe can stop and remove the container as follows:\n$\nsudo\ndocker\nstop\ngrafana-container\n$\nsudo\ndocker\nrm\ngrafana-container\nThe\nGrafana rock’s documentation\nwill also show you how to use Docker’s\n-v\nbind mounts to configure Grafana’s provisioning directory and data persistence.\nMulti-container deployment\n¶\nThe section above explained the use of a single container for running a single software instance, but one of the benefits of using rocks is the ability to easily create and architecturally organize (or “orchestrate”) them to operate together in a modular fashion.\nThis section will demonstrate use of\ndocker-compose\nto set up two container services that inter-operate to implement a trivial observability stack with the\nPrometheus\nand\nGrafana\nrocks.\nStart by creating a Prometheus configuration file called\nprometheus.yml\nwith the following contents:\nglobal\n:\nscrape_interval\n:\n1m\nscrape_configs\n:\n-\njob_name\n:\n'prometheus'\nscrape_interval\n:\n1m\nstatic_configs\n:\n-\ntargets\n:\n[\n'localhost:9090'\n]\nNote that this is a very simplistic example, where Prometheus only collects metrics about itself. You could expand the above configuration to tell Prometheus to scrape metrics from other sources.\nThen, create the Compose file\ndocker-compose.yml\nand define both services:\nservices\n:\ngrafana\n:\nimage\n:\nubuntu/grafana:10.3.3-22.04_stable\ncontainer_name\n:\ngrafana-container\nenvironment\n:\nTZ\n:\nUTC\nports\n:\n-\n\"3000:3000\"\nprometheus\n:\nimage\n:\nubuntu/prometheus:2.49.1-22.04_stable\ncontainer_name\n:\nprometheus-container\nenvironment\n:\nTZ\n:\nUTC\nports\n:\n-\n\"9090:9090\"\nvolumes\n:\n-\n./prometheus.yml:/etc/prometheus/prometheus.yml\nNote that the Prometheus configuration file is being given to the container via a Docker volume (of type “bind mount”). The above sample could also be improved to also use another volume for persisting data, and even a Grafana default configuration for the Prometheus datasource.\nSince we already installed Docker in the section above, all that is needed is to create and start the containers defined in this Compose file. This can be achieved with:\n$\nsudo\ndocker\ncompose\nup\n-d\n[\n+\n]\nRunning\n10\n/10\n✔\ngrafana\nPulled\n✔\nbccd10f490ab\nAlready\nexists\n✔\n549078d9d057\nPull\ncomplete\n✔\n6ef870aa8500\nPull\ncomplete\n✔\n2b475da7ccbd\nPull\ncomplete\n✔\nprometheus\nPulled\n✔\na8b1c5f80c2d\nAlready\nexists\n✔\nf021062473aa\nPull\ncomplete\n✔\n9c6122d12d1d\nPull\ncomplete\n✔\n274b56f68abe\nPull\ncomplete\n[\n+\n]\nRunning\n3\n/3\n✔\nNetwork\ncompose_default\nCreated\n✔\nContainer\nprometheus-container\nStarted\n✔\nContainer\ngrafana-container\nStarted\nAs before, the\n-d\nindicates that all containers in this stack should be started in the background. You can confirm they are live and running with:\n$\nsudo\ndocker\ncompose\nps\nNAME\nIMAGE\nCOMMAND\nSERVICE\nCREATED\nSTATUS\nPORTS\ngrafana-container\nubuntu/grafana:10.3.3-22.04_stable\n\"/bin/pebble enter -…\"\ngrafana\n3\nseconds\nago\nUp\n3\nseconds\n0\n.0.0.0:3000->3000/tcp,\n:::3000->3000/tcp\nprometheus-container\nubuntu/prometheus:2.49.1-22.04_stable\n\"/bin/pebble enter -…\"\nprometheus\n3\nseconds\nago\nUp\n3\nseconds\n0\n.0.0.0:9090->9090/tcp,\n:::9090->9090/tcp\nOpening\nhttp://localhost:3000\nwill give you the same Grafana login page as before:\nUse the default username\nadmin\nand password\nadmin\nto login:\nBy clicking on “Data Sources” you can then add Prometheus and provide the server URL\nhttp://prometheus:9090\n:\nThis URL works because Docker Compose ensures both containers are on the same Docker network and that they can be discovered via their service name.\nFinally, click on “Explore” from the Grafana menu, and select the\nprometheus\ndatasource. You can now query and visualize the Prometheus metrics. For example:\nNext Steps\n¶\nAs you can see,\ndocker-compose\nmakes it convenient to set up multi-container applications without needing to perform runtime changes to the containers. As you can imagine, this can permit building a more sophisticated management system to handle fail-over, load-balancing, scaling, upgrading old nodes, and monitoring status. But rather than needing to implement all of this directly on top of\ndocker-container\n, you may want to investigate Kubernetes-style cluster management software such as\nmicrok8s\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 985}}
{"id": "f00b9c1888", "source_url": "https://documentation.ubuntu.com/server/how-to/data-and-storage/", "title": "Data and storage - Ubuntu Server documentation", "text": "Data and storage - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nData and storage\n¶\nThe following sections provide details on various topics related to storing, managing and accessing data.\nData management\n¶\nOpenLDAP\nshows how to set up and configure OpenLDAP\nDatabases\nprovides details on two of the most common databases found in Ubuntu: MySQL and PostgreSQL\nStorage and backups\n¶\nStorage\nshows how to set up and manage Logical Volumes\nBackups and version control\npresents common options for backing up your data and your system\nSee also\n¶\nExplanation:\nData and storage", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 116}}
{"id": "87e0d07c71", "source_url": "https://documentation.ubuntu.com/server/how-to/databases/", "title": "Databases - Ubuntu Server documentation", "text": "Databases - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nDatabases\n¶\nUbuntu provides two popular database servers: MySQL and PostgreSQL. Both are popular choices with similar feature sets, and both are equally supported in Ubuntu.\nThese guides show you how to install and configure them.\nMySQL\nPostgreSQL\nSee also\n¶\nExplanation:\nIntroduction to databases", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 76}}
{"id": "95620926f1", "source_url": "https://documentation.ubuntu.com/server/how-to/databases/install-mysql/", "title": "Install and configure a MySQL server - Ubuntu Server documentation", "text": "Install and configure a MySQL server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure a MySQL server\n¶\nMySQL\nis a fast, multi-threaded, multi-user, and robust SQL database server. It is intended for mission-critical, heavy-load production systems and mass-deployed software.\nInstall MySQL\n¶\nTo install MySQL, run the following command from a terminal prompt:\nsudo\napt\ninstall\nmysql-server\nOnce the installation is complete, the MySQL server should be started automatically. You can quickly check its current status via systemd:\nsudo\nservice\nmysql\nstatus\nWhich should provide an output like the following:\n● mysql.service - MySQL Community Server\n   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\n   Active: active (running) since Tue 2019-10-08 14:37:38 PDT; 2 weeks 5 days ago\n Main PID: 2028 (mysqld)\n    Tasks: 28 (limit: 4915)\n   CGroup: /system.slice/mysql.service\n           └─2028 /usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid\n Oct 08 14:37:36 db.example.org systemd[1]: Starting MySQL Community Server...\nOct 08 14:37:38 db.example.org systemd[1]: Started MySQL Community Server.\nThe network status of the MySQL service can also be checked by running the\nss\ncommand at the terminal prompt:\nsudo\nss\n-tap\n|\ngrep\nmysql\nWhen you run this command, you should see something similar to the following:\nLISTEN    0         151              127.0.0.1:mysql             0.0.0.0:*       users:((\"mysqld\",pid=149190,fd=29))\nLISTEN    0         70                       *:33060                   *:*       users:((\"mysqld\",pid=149190,fd=32))\nIf the server is not running correctly, you can type the following command to start it:\nsudo\nservice\nmysql\nrestart\nA good starting point for troubleshooting problems is the systemd journal, which can be accessed from the terminal prompt with this command:\nsudo\njournalctl\n-u\nmysql\nConfigure MySQL\n¶\nYou can edit the files in\n/etc/mysql/\nto configure the basic settings – log file, port number, etc. For example, to configure MySQL to listen for connections from network hosts, in the file\n/etc/mysql/mysql.conf.d/mysqld.cnf\n, change the\nbind-address\ndirective to the server’s IP address:\nbind\n-\naddress\n=\n192.168.0.5\nNote\nReplace\n192.168.0.5\nwith the appropriate address, which can be determined via the\nip\naddress\nshow\ncommand.\nAfter making a configuration change, the MySQL daemon will need to be restarted with the following command:\nsudo\nsystemctl\nrestart\nmysql.service\nUser setup\n¶\nBy default,\nmysql-server\ninitially provides a\n'root'@'localhost'\nuser for managing the server locally. You can enter the MySQL command-line as this user by running:\nsudo mysql -u root\nNo password is required by MySQL as it authenticates with\nauth_socket\n.\nCreate a new user\n¶\nFrom the command-line, you can create additional MySQL users with different privileges using the\nCREATE\nUSER\ncommand. For authentication, the two main options are to use a password or use a socket like the root user.\nTo create a user authenticated with a password, you can use MySQL’s provided\ncaching_sha2_password\nplugin. It can be invoked in the following way, providing the password in plaintext:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH caching_sha2_password BY 'password';\nA random password can also be generated here with:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH caching_sha2_password BY RANDOM PASSWORD;\nMySQL’s upstream documentation\nprovides an overview of additional options when creating accounts with passwords.\nSocket-based authentication is used to allow a local system user to access an account without entering a password. Invoke this with:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH auth_socket;\nBy default, only the system user with the matching username can access this account. If you want the MySQL account username to differ from the system user username, then use the\nAS\noption:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH auth_socket AS 'system-user-username';\nAdding user permissions\n¶\nA newly created user will require privilege updates to interact with databases in any way. These are provided by the\nGRANT\ncommand alongside specified roles or operations. For example, to give your user the ability to view table entries using the\nSELECT\noperation on all databases, run the following:\nGRANT SELECT on *.* TO 'username'@'localhost';\nDatabase engines\n¶\nWhilst the default configuration of MySQL provided by the Ubuntu packages is perfectly functional and performs well there are things you may wish to consider before you proceed.\nMySQL is designed to allow data to be stored in different ways. These methods are referred to as either database or storage engines. There are two main storage engines that you’ll be interested in:\nInnoDB\nand\nMyISAM\n. Storage engines are transparent to the end user. MySQL will handle things differently under the surface, but regardless of which storage engine is in use, you will interact with the database in the same way.\nEach engine has its own advantages and disadvantages.\nWhile it is possible (and may be advantageous) to mix and match database engines on a table level, doing so reduces the effectiveness of the performance tuning you can do as you’ll be splitting the resources between two engines instead of dedicating them to one.\nInnoDB\n¶\nAs of MySQL 5.5, InnoDB is the default engine, and is highly recommended over MyISAM unless you have specific needs for features unique to that engine.\nInnoDB is a more modern database engine, designed to be\nACID compliant\nwhich guarantees database transactions are processed reliably. To meet ACID compliance all transactions are journaled independently of the main tables. This allows for much more reliable data recovery as data consistency can be checked.\nWrite locking can occur on a row-level basis within a table. That means multiple updates can occur on a single table simultaneously. Data caching is also handled in memory within the database engine, allowing caching on a more efficient row-level basis rather than file block.\nMyISAM\n¶\nMyISAM is the older of the two. It can be faster than InnoDB under certain circumstances and favors a read-only workload. Some web applications have been tuned around MyISAM (though that’s not to imply that they will be slower under InnoDB).\nMyISAM also supports the\nFULLTEXT\nindex type, which allows very fast searches of large quantities of text data. However MyISAM is only capable of locking an entire table for writing. This means only one process can update a table at a time. As any application that uses the table scales this may prove to be a hindrance.\nIt also lacks journaling, which makes it harder for data to be recovered after a crash. The following link provides some points for consideration about using\nMyISAM on a production database\n.\nBackups\n¶\nMySQL databases should be backed up regularly. Backups can be accomplished through several methods, of which we’ll discuss three here.\nmysqldump\nis included with\nmysql-server\n. It is useful for backing up smaller databases, allows backups to be edited prior to a restore, and can be used for exporting to CSV and XML.\nMySQL Shell’s Dump Utility\nallows for backups of specific schema and tables, both to local files and remote secure servers. It is recommended for creating partial backups, and for integration with Python programs.\nPercona Xtrabackup\ncreates full backups with far greater performance than the former options. However, it lacks the ability to customize schema and tables. It is the recommended option for backing up large databases in a production environment.\nmysqldump\n¶\nmysqldump\nis a built-in tool that performs\nlogical backups\nfor MySQL.\nTo dump the data of a publicly available database on the local MySQL server into a file, run the following:\nmysqldump\n[\ndatabase\nname\n]\n>\ndump.sql\nFor restricted databases, specify a user with the proper permissions using\n-u\n:\nmysqldump\n-u\nroot\n[\ndatabase\nname\n]\n>\ndump.sql\nTo restore a database from the backup file, run the\nmysql\ncommand and pipe the file through stdin:\nmysql\n-u\nroot\n[\ndatabase\nname\n]\n<\ndump.sql\nSee the\nupstream documentation\nfor more information.\nMySQL Shell Dump Utility\n¶\nMySQL Shell, supported in Ubuntu 24.04 LTS and later, contains a set of utilities for dumping, backing up, and restoring MySQL data. It provides a programmatic option for logical backups with filtering options.\nTo install MySQL Shell, run the following:\nsudo\napt\ninstall\nmysql-shell\nRun the following to connect to the local MySQL server on Ubuntu with MySQL Shell in Python mode:\nmysqlsh\n--socket\n=\n/var/run/mysqld/mysqld.sock\n--no-password\n--python\nInitiate a local backup of all data in Python mode with:\nutil\n.\ndump_instance\n(\n\"/tmp/worlddump\"\n)\nDump a specific set of tables with\ndump_tables\n:\nutil\n.\ndump_tables\n(\n\"database name\"\n,\n[\n\"table 1\"\n,\n\"table 2\"\n],\n\"/tmp/tabledump\"\n)\nTo restore dumped data, use the\ndump loading utility\n.\nutil\n.\nload_dump\n(\n\"/tmp/worlddump\"\n)\nNote\nTo restore data from a local file,\nlocal_infile\nneeds to be enabled on the MySQL server. Activate this by accessing the server with the\nmysql\ncommand and entering\nSET\nGLOBAL\nlocal_infile=1;\n.\nSee the\nMySQL Shell dump documentation\nfor more information.\nPercona Xtrabackup\n¶\nAlso supported in Ubuntu 24.04 LTS and later, Percona Xtrabackup is a tool for creating\nphysical backups\n. It is similar to the commercial offering of\nMySQL Enterprise Backup\n.\nTo install Xtrabackup, run the following command from a terminal prompt:\nsudo\napt\ninstall\npercona-xtrabackup\nCreate a new backup with the\nxtrabackup\ncommand. This can be done while the server is running.\nxtrabackup\n--backup\n--target-dir\n=\n/tmp/worlddump\nTo restore from a backup, service will need to be interrupted. This can be achieved with the following:\nsudo\nsystemctl\nstop\nmysql\nxtrabackup\n--prepare\n--target-dir\n=\n/tmp/worlddump\nsudo\nrm\n-rf\n/var/lib/mysql\nsudo\nxtrabackup\n--copy-back\n--target-dir\n=\n/tmp/worlddump\n--datadir\n=\n/var/lib/mysql\nsudo\nchown\n-R\nmysql:mysql\n/var/lib/mysql\nsudo\nsystemctl\nstart\nmysql\nFor more information, see\nPercona’s upstream documentation\n.\nAdvanced configuration\n¶\nCreating a tuned configuration\n¶\nThere are a number of parameters that can be adjusted within MySQL’s configuration files. This will allow you to improve the server’s performance over time.\nMany parameters can be adjusted with the existing database, however some may affect the data layout and thus need more care to apply.\nFirst, if you have existing data, you will first need to carry out a\nmysqldump\nand reload:\nmysqldump\n--all-databases\n--routines\n-u\nroot\n-p\n>\n~/fulldump.sql\nThis will then prompt you for the root password before creating a copy of the data. It is advisable to make sure there are no other users or processes using the database while this takes place. Depending on how much data you’ve got in your database, this may take a while. You won’t see anything on the screen during the process.\nOnce the dump has been completed, shut down MySQL:\nsudo\nservice\nmysql\nstop\nIt’s also a good idea to backup the original configuration:\nsudo\nrsync\n-avz\n/etc/mysql\n/root/mysql-backup\nNext, make any desired configuration changes. Then, delete and re-initialize the database space and make sure ownership is correct before restarting MySQL:\nsudo\nrm\n-rf\n/var/lib/mysql/*\nsudo\nmysqld\n--initialize\nsudo\nchown\n-R\nmysql:\n/var/lib/mysql\nsudo\nservice\nmysql\nstart\nThe final step is re-importation of your data by piping your SQL commands to the database.\ncat\n~/fulldump.sql\n|\nmysql\nFor large data imports, the ‘Pipe Viewer’ utility can be useful to track import progress. Ignore any ETA times produced by\npv\n; they’re based on the average time taken to handle each row of the file, but the speed of inserting can vary wildly from row to row with\nmysqldumps\n:\nsudo\napt\ninstall\npv\npv\n~/fulldump.sql\n|\nmysql\nOnce this step is complete, you are good to go!\nNote\nThis is not necessary for all\nmy.cnf\nchanges. Most of the variables you can change to improve performance are adjustable even whilst the server is running. As with anything, make sure to have a good backup copy of your config files and data before making changes.\nMySQL Tuner\n¶\nMySQL Tuner\nis a Perl script that connects to a running MySQL instance and offers configuration suggestions for optimising the database for your workload. The longer the server has been running, the better the advice\nmysqltuner\ncan provide. In a production environment, consider waiting for at least 24 hours before running the tool. You can install\nmysqltuner\nwith the following command:\nsudo\napt\ninstall\nmysqltuner\nThen once it has been installed, simply run:\nmysqltuner\n– and wait for its final report.\nThe top section provides general information about the database server, and the bottom section provides tuning suggestions to alter in your\nmy.cnf\n. Most of these can be altered live on the server without restarting; look through the\nofficial MySQL documentation\nfor the relevant variables to change in production.\nThe following example is part of a report from a production database showing potential benefits from increasing the query cache:\n-------- Recommendations -----------------------------------------------------\nGeneral recommendations:\n    Run OPTIMIZE TABLE to defragment tables for better performance\n    Increase table_cache gradually to avoid file descriptor limits\nVariables to adjust:\n    key_buffer_size (> 1.4G)\n    query_cache_size (> 32M)\n    table_cache (> 64)\n    innodb_buffer_pool_size (>= 22G)\nObviously, performance optimisation strategies vary from application to application; what works best for WordPress might not be the best for Drupal or Joomla. Performance can depend on the types of queries, use of indexes, how efficient the database design is and so on.\nYou may find it useful to spend some time searching for database tuning tips based on the applications you’re using. Once you’ve reached the point of diminishing returns from database configuration adjustments, look to the application itself for improvements, or invest in more powerful hardware and/or scale up the database environment.\nFurther reading\n¶\nFull documentation is available in both online and offline formats from the\nMySQL Developers portal\nFor general SQL information see the O’Reilly books\nGetting Started with SQL: A Hands-On Approach for Beginners\nby Thomas Nield as an entry point and\nSQL in a Nutshell\nas a quick reference.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 2249}}
{"id": "60b68e8d2c", "source_url": "https://documentation.ubuntu.com/server/how-to/databases/install-postgresql/", "title": "Install and configure PostgreSQL - Ubuntu Server documentation", "text": "Install and configure PostgreSQL - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure PostgreSQL\n¶\nPostgreSQL\n(commonly referred to as “Postgres”) is an object-relational database system that has all the features of traditional commercial database systems, but with enhancements to be found in next-generation database management systems (DBMS).\nInstall PostgreSQL\n¶\nTo install PostgreSQL, run the following command in the command prompt:\nsudo\napt\ninstall\npostgresql\nThe database service is automatically configured with viable defaults, but can be customized based on your specific needs.\nConfigure PostgreSQL\n¶\nPostgreSQL supports multiple client authentication methods. In Ubuntu,\npeer\nis the default authentication method used for\nlocal\nconnections, while\nscram-sha-256\nis the default for\nhost\nconnections (this used to be\nmd5\nuntil Ubuntu 21.10). Please refer to the\nPostgreSQL Administrator’s Guide\nif you would like to configure alternatives like Kerberos.\nThe following discussion assumes that you wish to enable TCP/IP connections and use the\nscram-sha-256\nmethod for client authentication. PostgreSQL configuration files are stored in the\n/etc/postgresql/<version>/main\ndirectory. For example, if you install PostgreSQL 14, the configuration files are stored in the\n/etc/postgresql/14/main\ndirectory.\nTip\nTo configure\nIDENT\nauthentication, add entries to the\n/etc/postgresql/*/main/pg_ident.conf\nfile. There are detailed comments in the file to guide you.\nBy default, only connections from the local system are allowed. To enable all other computers to connect to your PostgreSQL server, edit the file\n/etc/postgresql/*/main/postgresql.conf\n. Locate the line:\n#listen_addresses = ‘localhost’\nand change it to\n*\n:\nlisten_addresses = '*'\nNote\nTo listen on all IPv4 interfaces, set\nlisten_addresses\nto ‘\n0.0.0.0\n’, while ‘\n::\n’ will listen on all IPv6 interfaces. ‘\n*\n’ will cause PostgreSQL to listen on all available network interfaces, both IPv4 and IPv6.\nFor details on other parameters, refer to the configuration file or to the\nPostgreSQL documentation\nfor information on how they can be edited.\nNow that we can connect to our PostgreSQL server, the next step is to set a password for the\npostgres\nuser. Run the following command at a terminal prompt to connect to the default PostgreSQL template database:\nsudo\n-u\npostgres\npsql\ntemplate1\nThe above command connects to PostgreSQL database\ntemplate1\nas user\npostgres\n. Once you connect to the PostgreSQL server, you will be at an SQL prompt. You can run the following SQL command at the\npsql\nprompt to configure the password for the user\npostgres\n:\nALTER\nUSER\npostgres\nwith\nencrypted\npassword\n'your_password'\n;\nAfter configuring the password, edit the file\n/etc/postgresql/*/main/pg_hba.conf\nto use\nscram-sha-256\nauthentication with the\npostgres\nuser, allowed for the\ntemplate1\ndatabase, from any system in the local network (which in the example is\n192.168.1.1/24\n) :\nhostssl template1       postgres        192.168.1.1/24        scram-sha-256\nNote\nThe config statement\nhostssl\nused here will reject TCP connections that would not use SSL. PostgreSQL in Ubuntu has the SSL feature built in and configured by default, so it works right away. On your PostgreSQL server this uses the certificate created by\nssl-cert\npackage which is great, but for production use you should consider updating that with a proper certificate from a recognized Certificate Authority (CA).\nFinally, you should restart the PostgreSQL service to initialize the new configuration. From a terminal prompt enter the following to restart PostgreSQL:\nsudo\nsystemctl\nrestart\npostgresql.service\nWarning\nThe above configuration is not complete by any means. Please refer to the\nPostgreSQL Administrator’s Guide\nto configure more parameters.\nYou can test server connections from other machines by using the PostgreSQL client as follows, replacing the domain name with your actual server domain name or IP address:\nsudo\napt\ninstall\npostgresql-client\npsql\n--host\nyour-servers-dns-or-ip\n--username\npostgres\n--password\n--dbname\ntemplate1\nStreaming replication\n¶\nPostgreSQL has a nice feature called\nstreaming replication\nwhich provides the ability to continuously ship and apply the Write-Ahead Log\n(WAL) XLOG\nrecords to some number of standby servers to keep them current. Here is a simple way to replicate a PostgreSQL server (main) to a standby server.\nFirst, create a replication user in the main server, to be used from the standby server:\nsudo\n-u\npostgres\ncreateuser\n--replication\n-P\n-e\nreplicator\nLet’s configure the main server to turn on the streaming replication. Open the file\n/etc/postgresql/*/main/postgresql.conf\nand make sure you have the following lines:\nlisten_addresses = '*'\nwal_level = replica\nAlso edit the file\n/etc/postgresql/*/main/pg_hba.conf\nto add an extra line to allow the standby server connection for replication (that is a special keyword) using the\nreplicator\nuser:\nhost  replication   replicator   <IP address of the standby>      scram-sha-256\nRestart the service to apply changes:\nsudo\nsystemctl\nrestart\npostgresql\nNow, in the standby server, let’s stop the PostgreSQL service:\nsudo\nsystemctl\nstop\npostgresql\nEdit the\n/etc/postgresql/*/main/postgresql.conf\nto set up hot standby:\nhot_standby = on\nBack up the current state of the main server (those commands are still issued on the standby system):\nsudo\nsu\n-\npostgres\n# backup the current content of the standby server (update the version of your postgres accordingly)\ncp\n-R\n/var/lib/postgresql/14/main\n/var/lib/postgresql/14/main_bak\n# remove all the files in the data directory\nrm\n-rf\n/var/lib/postgresql/14/main/*\npg_basebackup\n-h\n<IP\naddress\nof\nthe\nmain\nserver>\n-D\n/var/lib/postgresql/14/main\n-U\nreplicator\n-P\n-v\n-R\nAfter this, a full single pass will have been completed, copying the content of the main database onto the local system being the standby. In the\npg_basebackup\ncommand the flags represent the following:\n-h\n: The\nhostname\nor IP address of the main server\n-D\n: The data directory\n-U\n: The user to be used in the operation\n-P\n: Turns on progress reporting\n-v\n: Enables verbose mode\n-R\n: Creates a\nstandby.signal\nfile and appends connection settings to\npostgresql.auto.conf\nFinally, let’s start the PostgreSQL service on standby server:\nsudo\nsystemctl\nstart\npostgresql\nTo make sure it is working, go to the main server and run the following command:\nsudo\n-u\npostgres\npsql\n-c\n\"select * from pg_stat_replication;\"\nAs mentioned, this is a very simple introduction, there are way more great details in the upstream documentation about the configuration of\nreplication\nas well as further\nHigh Availability, Load Balancing, and Replication\n.\nTo test the replication you can now create a test database in the main server and check if it is replicated in the standby server:\nsudo\n-u\npostgres\ncreatedb\ntest\n# on the main server\nsudo\n-u\npostgres\npsql\n-c\n\"\\l\"\n# on the standby server\nYou need to be able to see the\ntest\ndatabase, that was created on the main server, in the standby server.\nBackups\n¶\nPostgreSQL databases should be backed up regularly. Refer to the\nPostgreSQL Administrator’s Guide\nfor different approaches.\nFurther reading\n¶\nAs mentioned above, the\nPostgreSQL Administrator’s Guide\nis an excellent resource. The guide is also available in the\npostgresql-doc\npackage. Execute the following in a terminal to install the package:\nsudo\napt\ninstall\npostgresql-doc\nThis package provides further manpages on PostgreSQL\ndblink\nand “server programming interface” as well as the upstream HTML guide. To view the guide enter\nxdg-open\n/usr/share/doc/postgresql-doc-*/html/index.html\nor point your browser at it.\nFor general SQL information see the O’Reilly books\nGetting Started with SQL: A Hands-On Approach for Beginners\nby Thomas Nield as an entry point and\nSQL in a Nutshell\nas a quick reference.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 1207}}
{"id": "f251cdb5f0", "source_url": "https://documentation.ubuntu.com/server/how-to/graphics/", "title": "Graphics - Ubuntu Server documentation", "text": "Graphics - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nGraphics\n¶\nOn-system GPU\n¶\nInstall NVIDIA drivers\nVirtual GPU\n¶\nA virtual GPU (vGPU) partitions a physical GPU to enable GPU-accelerated workloads in virtualized environments.\nVirtual GPU (vGPU) with QEMU/KVM", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 62}}
