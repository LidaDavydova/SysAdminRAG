{"id": "5ea6d76bbc", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/", "title": "Active Directory integration - Ubuntu Server documentation", "text": "Active Directory integration - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nActive Directory integration\n¶\nIf you have a Microsoft Active Directory domain set up, you can join your Ubuntu Server to it. There are a number of choices you need to make about your setup before you can begin, so you may want to refer to our\nAD integration explanations\nfirst.\nPrepare to join a domain\nJoin a simple domain with the rid backend\nJoin a forest with the rid backend\nJoin a forest with the autorid backend\nSee also\n¶\nExplanation:\nIntroduction to Active Directory integration", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:11Z", "original_len_words": 120}}
{"id": "57c82ec582", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-domain-with-winbind-preparation/", "title": "Join a domain with winbind: preparation - Ubuntu Server documentation", "text": "Join a domain with winbind: preparation - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a domain with winbind: preparation\n¶\nChoosing the identity mapping backend, and planning its ranges, is the first and most important aspect of joining a domain. To actually perform the join, however, a few more configuration steps are necessary. These steps are common to both backend types, the only difference being the actual idmap configuration.\nTo continue, this is the minimum set of packages that are needed:\nsudo apt install winbind libnss-winbind libpam-winbind\nNext, it will make everything much easier if the\nDNS\nresolver is pointed at the Active Directory DNS server. If that is already the case as provided by the\nDHCP\nserver, this part can be skipped.\nFor example, for a default netplan configuration file which looks like this:\nnetwork\n:\nversion\n:\n2\nethernets\n:\neth0\n:\ndhcp4\n:\ntrue\nYou can add a\nnameservers\nblock which will override the DNS options sent by the DHCP server. For example, if the DNS server is at\n10.10.4.5\nand the domain search value is\nexample.internal\n, this would be the new configuration:\nnetwork\n:\nversion\n:\n2\nethernets\n:\neth0\n:\ndhcp4\n:\ntrue\nnameservers\n:\naddresses\n:\n[\n10.10.4.5\n]\nsearch\n:\n[\nexample.internal\n]\nTo make the changes effective, first make sure there are no syntax errors:\nsudo netplan generate\nIf there are no complaints, the changes can be applied:\nsudo netplan apply\nNote\nBe careful whenever changing network parameters over an ssh connection. If there are any mistakes, you might lose remote access!\nTo check if the resolver was updated, run\nresolvectl\nstatus\n:\nGlobal\n         Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n  resolv.conf mode: stub\n\nLink 281 (eth0)\n    Current Scopes: DNS\n         Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n       DNS Servers: 10.10.4.5 10.10.4.1\n        DNS Domain: example.internal\nNow we need to configure the system to also use the winbind NSS module to look for users and groups. In Ubuntu 24.04 LTS and later, this is done automatically, but for older LTS releases, edit the file\n/etc/nsswitch.conf\nand add\nwinbind\nto the end of the\npasswd:\nand\ngroup:\nlines:\n# /etc/nsswitch.conf\n#\n# Example configuration of GNU Name Service Switch functionality.\n# If you have the `glibc-doc-reference' and `info' packages installed, try:\n# `info libc \"Name Service Switch\"' for information about this file.\n\npasswd:         files systemd winbind\ngroup:          files systemd winbind\n(...)\nFinally, let’s enable automatic home directory creation for users as they login. Run the command:\nsudo pam-auth-update --enable mkhomedir\nNow we are set to perform the final winbind configuration depending on the identity mapping backend that was chosen, and actually join the domain.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 460}}
{"id": "125b7f8348", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-forest-with-the-autorid-backend/", "title": "Join a forest with the autorid backend - Ubuntu Server documentation", "text": "Join a forest with the autorid backend - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a forest with the autorid backend\n¶\nJoining a more complex Active Directory forest with the autorid backend is very similar to the rid backend. The only difference is in the idmap configuration in\n/etc/samba/smb.conf\n:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend = autorid\n    # 1,000,000 - 19,999,999\n    idmap config * : range   = 1000000 - 19999999\n    # 1,000,000\n    idmap config * : rangesize = 1000000\n\n    # allow logins when the DC is unreachable\n    winbind offline logon = yes\n    # this *can* be yes if there is absolute certainty that there is only a\n    # single domain involved\n    winbind use default domain = no\n    # setting these enumeration options to yes has a high performance impact\n    # and can cause instabilities\n    winbind enum groups = no\n    winbind enum users = no\n    winbind refresh tickets = yes\n    # if domain users should be allowed to login, they will need a login shell\n    template shell = /bin/bash\n    # the home directory template for domain users\n    template homedir = /home/%D/%U\n    kerberos method = secrets and keytab\nNote that there is no specific domain mentioned in the idmap configuration. That’s because the autorid backend does the allocations on demand, according to the defined slots. The configuration above defines the following:\n1 million IDs per slot\n19 slots (or domains)\nfull ID range, covering all slots, is from 1,000,000 to 19,999,999\nThat being said, the machine still needs to be joined to a specific domain of that forest, and in this example that will be\nEXAMPLE.INTERNAL\n.\nRunning the recommended\ntestparm\ncommand gives us confidence that the configuration is at least free from syntax and other logical errors:\n$ testparm\nLoad smb config files from /etc/samba/smb.conf\nLoaded services file OK.\nWeak crypto is allowed by GnuTLS (e.g. NTLM as a compatibility fallback)\n\nServer role: ROLE_DOMAIN_MEMBER\n\nPress enter to see a dump of your service definitions\nLike with the\nrid\nidmap backend, if this system is not yet in the AD\nDNS\nserver, it’s best to change its\nhostname\n(including the short hostname) to be the fully qualified domain name (FQDN), as that will allow the joining procedure to also update the DNS records, if so allowed by the AD server (normally it is).\nFor this example, the system’s hostname is\nn2\nin the\nexample.internal\ndomain, so the FQDN is\nn2.example.internal\n:\nsudo hostnamectl hostname n2.example.internal\nNow the domain join can be performed:\n$ sudo net ads join -U Administrator\nPassword for [EXAMPLE\\Administrator]:\nUsing short domain name -- EXAMPLE\nJoined 'N2' to dns domain 'example.internal'\nAnd we can revert the hostname change:\nsudo hostnamectl hostname n2\nIf the DNS server was updated correctly (and there were no errors about that in the join output above), then the hostname should now be correctly set, even though we have just the short name in\n/etc/hostname\n:\n$ hostname\nn2\n\n$ hostname -f\nn2.example.internal\nThe last step is to restart the\nwinbind\nservice:\nsudo systemctl restart winbind.service\nVerifying the join\n¶\nThe quickest way to test the integrity of the domain join is via the\nwbinfo\ncommand:\n$ sudo wbinfo -t\nchecking the trust secret for domain EXAMPLE via RPC calls succeeded\nThe next verification step should be to actually try to resolve an existing username from the domain. In the\nEXAMPLE.INTERNAL\ndomain, for example, we have some test users we can check:\n$ id jammy@example.internal\nuid=2001103(EXAMPLE\\jammy) gid=2000513(EXAMPLE\\domain users) groups=2000513(EXAMPLE\\domain users),2001103(EXAMPLE\\jammy)\nIf you compare this with the\nrid\ndomain join, note how the ID that the\njammy\nuser got is different. That’s why it’s important to correctly chose an idmap backend, and correctly assess if deterministic IDs are important for your use case or not.\nAnother valid syntax for domain users is prefixing the name with the domain, like this:\n$ id EXAMPLE\\\\jammy\nuid=2001103(EXAMPLE\\jammy) gid=2000513(EXAMPLE\\domain users) groups=2000513(EXAMPLE\\domain users),2001103(EXAMPLE\\jammy)\nAnd here we try a console login:\nn2 login: jammy@example.internal\nPassword:\nWelcome to Ubuntu 24.04 LTS (GNU/Linux 6.5.0-26-generic x86_64)\n(...)\nCreating directory '/home/EXAMPLE/jammy'.\nEXAMPLE\\jammy@n1:~$\nThe output above also shows the automatic on-demand home directory creation, according to the template defined in\n/etc/samba/smb.conf\n.\nSince we joined a forest, we should also be able to verify users from other domains in that forest. For example, in this example, the domain\nMYDOMAIN.INTERNAL\nis also part of the forest, and we can verify its users:\n$ id noble@mydomain.internal\nuid=3001104(MYDOMAIN\\noble) gid=3000513(MYDOMAIN\\domain users) groups=3000513(MYDOMAIN\\domain users),3001104(MYDOMAIN\\noble)\n\n$ id MYDOMAIN\\\\noble\nuid=3001104(MYDOMAIN\\noble) gid=3000513(MYDOMAIN\\domain users) groups=3000513(MYDOMAIN\\domain users),3001104(MYDOMAIN\\noble)\nA console login also works:\nn2 login: noble@mydomain.internal\nPassword:\nWelcome to Ubuntu 24.04 LTS (GNU/Linux 6.8.0-31-generic x86_64)\n(...)\nCreating directory '/home/MYDOMAIN/noble'.\nMYDOMAIN\\noble@n2:~$\nNotice how the domain name being part of the home directory path is useful: it separates the users from different domains, avoiding collisions for the same username.\nNote\nThe actual login name used can have multiple formats:\nDOMAIN\\user\nat the terminal login prompt,\nDOMAIN\\\\user\nwhen referred to in shell scripts (note the escaping of the ‘\n\\\n’ character), and\nuser@domain\nis also accepted.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 860}}
{"id": "717244d150", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-forest-with-the-rid-backend/", "title": "Join a forest with the rid backend - Ubuntu Server documentation", "text": "Join a forest with the rid backend - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a forest with the rid backend\n¶\nIt’s also possible to join an Active Directory forest using the\nrid\nidentity mapping backend. To better understand what is involved, and why it is tricky, let’s reuse the example where we joined a single domain with this backend:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend       = tdb\n    # 100,000 - 199,999\n    idmap config * : range         = 100000 - 199999\n    idmap config EXAMPLE : backend = rid\n    # 1,000,000 - 1,999,999\n    idmap config EXAMPLE : range   = 1000000 - 1999999\nWith this configuration, we are expected to join the\nEXAMPLE.INTERNAL\ndomain, and have given it a range of 1 million IDs starting with the ID\n1000000\n(1,000,000). There is also the mandatory reserved range for the default domain, represented by the identity mapping configuration for “\n*\n”, which has a smaller range of 100,000 IDs, starting at\n100000\n(100,000).\nThe\ntestparm\nutility is happy with this configuration, and there is no overlap of ID ranges:\n$ testparm\nLoad smb config files from /etc/samba/smb.conf\nLoaded services file OK.\nWeak crypto is allowed by GnuTLS (e.g. NTLM as a compatibility fallback)\n\nServer role: ROLE_DOMAIN_MEMBER\nWe next adjust the\nhostname\nand perform the join:\n$ sudo hostnamectl hostname n3.example.internal\n\n$ hostname\nn3.example.internal\n\n$ hostname -f\nn3.example.internal\n\n$ sudo net ads join -U Administrator\nPassword for [EXAMPLE\\Administrator]:\nUsing short domain name -- EXAMPLE\nJoined 'N3' to dns domain 'example.internal'\n\n$ sudo hostnamectl hostname n3\n$ sudo systemctl restart winbind.service\nA quick check shows that the users from\nEXAMPLE.INTERNAL\nare recognized:\n$ id jammy@example.internal\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nBut what happens if this single domain establishes a trust relationship with another domain, and we don’t modify the\n/etc/samba/smb.conf\nfile to cope with that? Where will the users from the new trusted domain get their IDs from? Since there is no specific idmap configuration for the new trusted domain, its users will get IDs from the default domain:\n$ id noble@mydomain.internal\nuid=100000(MYDOMAIN\\noble) gid=100000(MYDOMAIN\\domain users) groups=100000(MYDOMAIN\\domain users)\nOops. That is from the much smaller range 100,000 - 199,999, reserved for the catch-all default domain. Furthermore, if yet another trust relationship is established, those users will also get their IDs from this range, mixing multiple domains up in the same ID range, in whatever order they are being looked up.\nIf above we had looked up another user instead of\nnoble@mydomain.internal\n, that other user would have been given the ID 100000. There is no deterministic formula for the default domain ID allocation, like there is for the\nrid\nbackend. In the default domain, IDs are allocated on a first come, first serve basis.\nTo address this, we can add another\nidmap config\nconfiguration for the\nrid\nbackend, giving the new domain a separate range:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend       = tdb\n    # 100,000 - 199,999\n    idmap config * : range         = 100000 - 199999\n    idmap config EXAMPLE : backend = rid\n    # 1,000,000 - 1,999,999\n    idmap config EXAMPLE : range   = 1000000 - 1999999\n\n    # MYDOMAIN.INTERNAL idmap configuration\n    idmap config MYDOMAIN : backend = rid\n    # 2,000,000 - 2,999,999\n    idmap config MYDOMAIN : range   = 2000000 - 2999999\nWith this configuration, nothing changed for the\nEXAMPLE.INTERNAL\nusers, as expected:\n$ id jammy@example.internal\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nBut the users from the trusted domain\nMYDOMAIN.INTERNAL\nwill get their IDs allocated from the 2,000,000 - 2,999,999 range, instead of the default one:\n$ id noble@mydomain.internal\nuid=2001104(MYDOMAIN\\noble) gid=2000513(MYDOMAIN\\domain users) groups=2000513(MYDOMAIN\\domain users),2001104(MYDOMAIN\\noble)\nAnd this allocation, which is using the\nrid\nbackend, is deterministic.\nProblem solved! Well, until another trust relationship is established, then we have to allocate another range for it.\nSo is it possible to use the\nrid\nidentity mapping backend with an Active Directory forest, with multiple domains? Yes, but following these steps\nBEFORE\nestablishing any new trust relationship:\nplan an ID range for the new domain\nupdate\nALL\nsystems that are using the\nrid\nidmap backend with the new range configuration for the new domain\nrestart winbind on\nALL\nsuch systems\nthen the new trust relationship can be established\nIf a system is missed, and doesn’t have an idmap configuration entry for the new domain, the moment a user from that new domain is looked up, it will be assigned an ID from the default domain, which will be non-deterministic and different from the ID assigned to the same user in another system which had the new idmap configuration entry. Quite a mess. Unless the different ID is not important, in which case it’s much simpler to just use the\nautorid\nidentity mapping backend.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 813}}
{"id": "683ca3ee29", "source_url": "https://documentation.ubuntu.com/server/how-to/active-directory/join-a-simple-domain-with-the-rid-backend/", "title": "Join a simple domain with the rid backend - Ubuntu Server documentation", "text": "Join a simple domain with the rid backend - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nJoin a simple domain with the rid backend\n¶\nLet’s expand on the configuration we had for the\nrid\nbackend and complete the\n/etc/samba/smb.conf\nconfiguration file with the remaining details. We are joining a single domain called\nEXAMPLE.INTERNAL\n. The new configuration options were added at the end of the\n[global]\nsection:\n[global]\n    security = ads\n    realm = EXAMPLE.INTERNAL\n    workgroup = EXAMPLE\n\n    idmap config * : backend       = tdb\n    idmap config * : range         = 100000 - 199999\n    idmap config EXAMPLE : backend = rid\n    idmap config EXAMPLE : range   = 1000000 - 1999999\n\n    # allow logins when the DC is unreachable\n    winbind offline logon = yes\n    # this *can* be yes if there is absolute certainty that there is only a\n    # single domain involved\n    winbind use default domain = no\n    # setting these enumeration options to yes has a high performance impact\n    # and can cause instabilities\n    winbind enum groups = no\n    winbind enum users = no\n    winbind refresh tickets = yes\n    # if domain users should be allowed to login, they will need a login shell\n    template shell = /bin/bash\n    # the home directory template for domain users\n    template homedir = /home/%D/%U\n    kerberos method = secrets and keytab\nRight after saving\n/etc/samba/smb.conf\n, it’s always good practice to run the\ntestparm\nutility. It will perform a quick syntax check on the configuration file and alert you of any issues. Here is the output we get with the above configuration settings:\nLoad smb config files from /etc/samba/smb.conf\nLoaded services file OK.\nWeak crypto is allowed by GnuTLS (e.g. NTLM as a compatibility fallback)\n\nServer role: ROLE_DOMAIN_MEMBER\n\nPress enter to see a dump of your service definitions\n(...)\nDuring the domain join process, the tooling will attempt to update the\nDNS\nserver with the\nhostname\nof this system. Since its IP is likely not yet registered in DNS, that’s kind of a chicken and egg problem. It helps to, beforehand, set the hostname manually to the\nFQDN\n. For this example, we will use a host named\nn1\nin the\nexample.internal\ndomain:\nsudo hostnamectl hostname n1.example.internal\nSo that the output of\nhostname\n-f\n(and also just\nhostname\n) is\nn1.example.internal\n.\nWith the config file in place and checked, and all the other changes we made in the previous section, the domain join can be performed:\n$ sudo net ads join -U Administrator\nPassword for [EXAMPLE\\Administrator]:\nUsing short domain name -- EXAMPLE\nJoined 'N1' to dns domain 'example.internal'\nYou can now revert the\nhostnamectl\nchange from before, and set the hostname back to the short version, i.e.,\nn1\nin this example:\nsudo hostnamectl hostname n1\nAs the last step of the process, the\nwinbind\nservice must be restarted:\nsudo systemctl restart winbind.service\nVerifying the join\n¶\nThe quickest way to test the integrity of the domain join is via the\nwbinfo\ncommand:\n$ sudo wbinfo -t\nchecking the trust secret for domain EXAMPLE via RPC calls succeeded\nThe next verification step should be to actually try to resolve an existing username from the domain. In the\nEXAMPLE.INTERNAL\ndomain, for example, we have some test users we can check:\n$ id jammy@example.internal\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nAnother valid syntax for domain users is prefixing the name with the domain, like this:\n$ id EXAMPLE\\\\jammy\nuid=1001103(EXAMPLE\\jammy) gid=1000513(EXAMPLE\\domain users) groups=1000513(EXAMPLE\\domain users),1001103(EXAMPLE\\jammy)\nAnd finally, attempt a console login:\nn1 login: jammy@example.internal\nPassword:\nWelcome to Ubuntu 24.04 LTS (GNU/Linux 6.5.0-26-generic x86_64)\n(...)\nCreating directory '/home/EXAMPLE/jammy'.\nEXAMPLE\\jammy@n1:~$\nThe output above also shows the automatic on-demand home directory creation, according to the template defined in\n/etc/samba/smb.conf\n.\nNote\nThe actual login name used can have multiple formats:\nDOMAIN\\user\nat the terminal login prompt,\nDOMAIN\\\\user\nwhen referred to in shell scripts (note the escaping of the ‘\n\\\n’ character), and\nuser@domain\nis also accepted.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:12Z", "original_len_words": 667}}
{"id": "91b3d276fc", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/", "title": "Backups and version control - Ubuntu Server documentation", "text": "Backups and version control - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nBackups and version control\n¶\nOn Ubuntu, two primary ways of backing up your system are\nbackup utilities\nand\nshell scripts\n. For additional protection, you can combine backup methods.\nBackup utilities\n¶\nBacula\nhas advanced features and customization support, which makes it a good choice for enterprise systems or complex setups.\nrsnapshot\nis a simple and efficient solution, well suited to individual users or small-scale organizations.\nShell scripts\n¶\nIf you are looking for full flexibility and customization, another option is to use shell scripts.\nBackup with shell scripts\nVersion control\n¶\netckeeper\nstores the contents of\n/etc\nin a Version Control System (VCS) repository\nInstall gitolite\nfor a traditional source control management server for git, including multiple users and access rights management\nSee also\n¶\nExplanation:\nIntroduction to backups", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 165}}
{"id": "038e3a0edd", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/back-up-using-shell-scripts/", "title": "How to back up using shell scripts - Ubuntu Server documentation", "text": "How to back up using shell scripts - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to back up using shell scripts\n¶\nIn general, a shell script configures which directories to backup, and passes those directories as arguments to the\ntar\nutility, which creates an archive file. The archive file can then be moved or copied to another location. The archive can also be created on a remote file system such as a\nNetwork File System (NFS)\nmount.\nThe\ntar\nutility creates one archive file out of many files or directories.\ntar\ncan also filter the files through compression utilities, thus reducing the size of the archive file.\nIn this guide, we will walk through how to use a shell script for backing up files, and how to restore files from the archive we create.\nThe shell script\n¶\nThe following shell script uses\ntar\nto create an archive file on a remotely mounted NFS file system. The archive filename is determined using additional command line utilities. For more details about the script, check out the\nbasic\nbackup\nshell\nscript\n<https://discourse.ubuntu.com/t/basic-backup-shell-script/36419>\n_.\n#!/bin/bash\n####################################\n#\n# Backup to NFS mount script.\n#\n####################################\n# What to backup.\nbackup_files\n=\n\"/home /var/spool/mail /etc /root /boot /opt\"\n# Where to backup to.\ndest\n=\n\"/mnt/backup\"\n# Create archive filename.\nday\n=\n$(\ndate\n+%A\n)\nhostname\n=\n$(\nhostname\n-s\n)\narchive_file\n=\n\"\n$hostname\n-\n$day\n.tgz\"\n# Print start status message.\necho\n\"Backing up\n$backup_files\nto\n$dest\n/\n$archive_file\n\"\ndate\necho\n# Backup the files using tar.\ntar\nczf\n$dest\n/\n$archive_file\n$backup_files\n# Print end status message.\necho\necho\n\"Backup finished\"\ndate\n# Long listing of files in $dest to check file sizes.\nls\n-lh\n$dest\nRunning the script\n¶\nRun from a terminal\n¶\nThe simplest way to use the above backup script is to copy and paste the contents into a file (called\nbackup.sh\n, for example). The file must be made executable:\nchmod\nu+x\nbackup.sh\nThen from a terminal prompt, run the following command:\nsudo\n./backup.sh\nThis is a great way to test the script to make sure everything works as expected.\nRun with\ncron\n¶\nThe\ncron\nutility can be used to automate use of the script. The\ncron\ndaemon allows scripts, or commands, to be run at a specified time and date.\ncron\nis configured through entries in a\ncrontab\nfile.\ncrontab\nfiles are separated into fields:\n# m h dom mon dow   command\nWhere:\nm\n: The minute the command executes on, between 0 and 59.\nh\n: The hour the command executes on, between 0 and 23.\ndom\n: The day of the month the command executes on.\nmon\n: The month the command executes on, between 1 and 12.\ndow\n: The day of the week the command executes on, between 0 and 7. Sunday may be specified by using 0 or 7, both values are valid.\ncommand\n: The command to run.\nTo add or change entries in a\ncrontab\nfile the\ncrontab\n-e\ncommand should be used. Also note the contents of a\ncrontab\nfile can be viewed using the\ncrontab\n-l\ncommand.\nTo run the\nbackup.sh\nscript listed above using\ncron\n, enter the following from a terminal prompt:\nsudo\ncrontab\n-e\nNote\nUsing\nsudo\nwith the\ncrontab\n-e\ncommand edits the\nroot\nuser’s\ncrontab\n. This is necessary if you are backing up directories only the root user has access to.\nAs an example, if we add the following entry to the\ncrontab\nfile:\n# m h dom mon dow   command\n0\n0\n*\n*\n*\nbash\n/usr/local/bin/backup.sh\nThe\nbackup.sh\nscript would be run every day at 12:00 pm.\nNote\nThe\nbackup.sh\nscript will need to be copied to the\n/usr/local/bin/\ndirectory in order for this entry to run properly. The script can reside anywhere on the file system, simply change the script path appropriately.\nRestoring from the archive\n¶\nOnce an archive has been created, it is important to test the archive. The archive can be tested by listing the files it contains, but the best test is to\nrestore\na file from the archive.\nTo see a listing of the archive contents, run the following command from a terminal:\ntar\n-tzvf\n/mnt/backup/host-Monday.tgz\nTo restore a file from the archive back to a different directory, enter:\ntar\n-xzvf\n/mnt/backup/host-Monday.tgz\n-C\n/tmp\netc/hosts\nThe\n-C\noption to\ntar\nredirects the extracted files to the specified directory. The above example will extract the\n/etc/hosts\nfile to\n/tmp/etc/hosts\n.\ntar\nrecreates the directory structure that it contains. Also, notice the leading “\n/\n” is left off the path of the file to restore.\nTo restore all files in the archive enter the following:\ncd\n/\nsudo\ntar\n-xzvf\n/mnt/backup/host-Monday.tgz\nNote\nThis will overwrite the files currently on the file system.\nFurther reading\n¶\nFor more information on shell scripting see the\nAdvanced Bash-Scripting Guide\nThe\nCron How-to Wiki Page\ncontains details on advanced cron options.\nSee the\nGNU tar Manual\nfor more tar options.\nThe Wikipedia\nBackup Rotation Scheme\narticle contains information on other backup rotation schemes.\nThe shell script uses tar to create the archive, but there many other command line utilities that can be used. For example:\ncpio\n: used to copy files to and from archives.\ndd\n: part of the coreutils package. A low level utility that can copy data from one format to another.\nrsnapshot\n: a file system snapshot utility used to create copies of an entire file system. Also check the\nTools - rsnapshot\nfor some information.\nrsync(1)\n: a flexible utility used to create incremental copies of files.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 962}}
{"id": "51a7070fdd", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-bacula/", "title": "How to install and configure Bacula - Ubuntu Server documentation", "text": "How to install and configure Bacula - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure Bacula\n¶\nBacula\nis a backup management tool that enables you to backup, restore, and verify data across your network. There are Bacula clients for Linux, Windows, and Mac OS X – making it a cross-platform and network-wide solution.\nBacula components\n¶\nBacula is made up of several components and services that are used to manage backup files and locations:\nBacula Director\n: A service that controls all backup, restore, verify, and archive operations.\nBacula Console\n: An application that allows communication with the Director.\nBacula File\n: This application is installed on machines to be backed up, and is responsible for handling data requested by the Director.\nBacula Storage\n: The program that performs the storage of data onto, and recovery of data from, the physical media.\nBacula Catalog\n: Responsible for maintaining the file indices and volume databases for all backed-up files. This enables rapid location and restoration of archived files. The Catalog supports three different databases: MySQL, PostgreSQL, and SQLite.\nBacula Monitor\n: This is a graphical tray monitor for the Bacula backup system.\nThese services and applications can be run on multiple servers and clients, or they can be installed on one machine if backing up a single disk or volume.\nIn this documentation, we will deploy a Bacula Director, with a backup job for the Director itself, and also install the Bacule File service on a workstation, to remotely backup its data.\nInstalling the Server Components\n¶\nThe Bacula components can be installed on multiple systems, or they can be grouped together where it makes sense. A fully distributed installation might be appealing and is more scalable, but is also harder to configure. Here we will pick something in between:\nA Bacula “server”, where we will install the following Bacula components: Director, Catalog with an SQL database, Storage, File, Console. The server itself should also be backed up, hence why components typically installed on clients are also installed here.\nA Bacula “client”, which is just a system to be backed up. It will have only the File component installed. Any system that needs to be backed up will have to have the File component installed.\nTo begin with, we have to start with installing the database that will be used by the Catalog. The choices are:\nsqlite: Should only be used for test or development deployments of Bacula.\nPostgreSQL\nMySQL\nEither SQL database is suitable. For this document, we will use PostgreSQL:\nsudo apt install postgresql\nNote\nPlease take a look at\nMySQL databases\nand\nPostgreSQL databases\nfor more details on these powerful databases.\nNext we can install Bacula. The\nbacula\npackage has the necessary dependencies and will pull in what is needed for our deployment scenario:\nsudo apt install bacula\nDuring the install process you will be asked to supply a password for the\ndatabase owner\nof the\nbacula database\n. If left blank, a random password will be used.\nConfiguring the Server\n¶\nSeveral components were installed on this system by the\nbacula\npackage. Each one has its own configuration file:\nDirector:\n/etc/bacula/bacula-dir.conf\nStorage:\n/etc/bacula/bacula-sd.conf\nFile:\n/etc/bacula/bacula-fd.conf\nConsole:\n/etc/bacula/bconsole.conf\nAll these components need to eventually talk to each other, and the authentication is performed via passwords that were automatically generated at install time. These passwords are stored in the\n/etc/bacula/common_default_passwords\nfile. If a new component is installed on this system, it can benefit from this file to automatically be ready to authenticate itself, but in general, after everything is installed and configured, this file isn’t needed anymore.\nDirector\n¶\nThe Bacula Director is the central component of the system. This is where we:\nRegister the clients;\nDefine the schedules;\nDefine the file sets to be backed up;\nDefine storage pools;\nDefine the backup jobs;\nBacula configuration files are formatted based on\nresources\ncomposed of\ndirectives\nsurrounded by curly “{}” braces. Each Bacula component has an individual file in the\n/etc/bacula\ndirectory.\nThe default installation of the several bacula components will create configuration files in\n/etc/bacula/\nwith some choices and examples. That is a good reference, but it does not apply to all cases.\nAll the following sections involve the\n/etc/bacula/bacula-dir.conf\nDirector configuration file, unless stated otherwise.\nThe\nDirector\nresource\n¶\nThis block defines the attributes of the Director service:\nDirector\n{\nName\n=\nbacula\n-\nserver\n-\ndir\nDIRport\n=\n9101\nQueryFile\n=\n\"/etc/bacula/scripts/query.sql\"\nWorkingDirectory\n=\n\"/var/lib/bacula\"\nPidDirectory\n=\n\"/run/bacula\"\nMaximum\nConcurrent\nJobs\n=\n20\nPassword\n=\n\"<randomly generated>\"\nMessages\n=\nDaemon\n#DirAddress = 127.0.0.1\n}\nWhat you should inspect and change:\nName\n: it’s common to use the format\n$hostname-dir\nfor the director. For example, if the hostname is “\nbacula-server\n”, the name here would be\nbacula-server-dir\n. By sticking to this pattern, less changes will have to be made to the config, as this is what the default installation already assumes.\nDirAddress\n: by default this is set to the localhost address. In order to be able to perform remote backups, thouch, the director needs to be accessible on the network. To do that, simply remove or comment this parameter: in that case, the service will listen on all network interfaces available on the system.\nPassword\n: a random password will have been created for this installation, so it doesn’t need to be changed, unless you would rather pick a different one.\nTip\nFor more details about all the options of the\nDirector\nresource, please check the upstream\nDirector Resource\ndocumentation.\nThe\nFileSet\nresource\n¶\nLet’s define what we want to backup. There will likely be multiple file sets defined in a production server, but as an example, here we will define a set for backing up the home directory:\nFileSet\n{\nName\n=\n\"Home Set\"\nInclude\n{\nOptions\n{\nsignature\n=\nSHA256\naclsupport\n=\nyes\nxattrsupport\n=\nyes\n}\nOptions\n{\nwilddir\n=\n\"/home/*/Downloads\"\nwildfile\n=\n\"*.iso\"\nexclude\n=\nyes\n}\nFile\n=\n/\nhome\n}\n}\nThis example illustrates some interesting points, and shows the type of flexibility he have in defining File Sets:\nName\n: This defines the name of this file set, and will be referenced by other configuration blocks.\nInclude Options\n: the\nOptions\nblock can be used several times inside the\nInclude\nblock. Here we define that we want to include POSIX ACLs support, and extended attributes, and use the SHA256 hash algorithm, but perhaps more interesting is how we select which files to exclude from the set:\nwilddir\n,\nwildfile\n: these parameters specify globbing expressions of directories and files to exclude from the set (because we also set\nexclude\n=\nyes\n). In this example, we are excluding potentially large files. Unfortunately there is no direct way to exclude files from a set based on their size, so we have to make some educated guesses using standard file extensions and directory locations.\nFile\n: This parameter can be specified multiple times, and it’s additive. In this example we have it only used once, to select the\n/home\ndirectory and its subdirectories, subject to the exclusions defined in the\nOptions\nblock.\nTip\nFor more details about all the options of the\nFileSet\nresource, please check the upstream\nFileSet Resource\ndocumentation.\nThe\nClient\nresource\n¶\nThe\nClient\nresource is used to define a system to be backed up. That system will have at least the File Daemon installed, and will be contacted by the Director whenever there is a backup job for it.\nThe default installation will have defined this resource already, and it should be similar to the following:\n# Client (File Services) to backup\nClient\n{\nName\n=\nbacula\n-\nserver\n-\nfd\nAddress\n=\nbacula\n-\nserver\n.\nlxd\n# use the real hostname instead of \"localhost\"\nFDPort\n=\n9102\nCatalog\n=\nMyCatalog\nPassword\n=\n\"<randomly generated>\"\nFile\nRetention\n=\n60\ndays\nJob\nRetention\n=\n6\nmonths\nAutoPrune\n=\nyes\n}\nOf note in this definition we have:\nName\n: As with other similar resources, the default name uses the format\n$hostname-fd\n(where\nfd\nstands for\nFile Daemon\n).\nAddress\n: The default will be\nlocalhost\n, but we should be in the habit of using a real hostname, because in a more distributed installation, these hostnames will be sent to other services in other machines, and “localhost” will then be incorrect.\nPassword\n: The password was automatically generated, and should be kept as is unless you want to use another one.\nFile\nRetention\n,\nJob\nRetention\n: these should be adjusted according to your particular needs for each client.\nBy default, the backup job named\nBackupClient1\nis configured to archive the Bacula Catalog. If you plan on using the server to back up more than one client you should change the name of this job to something more descriptive. To change the name, edit\n/etc/bacula/bacula-dir.conf\n:\nAutoPrune\n: This setting makes Bacula automatically apply the retention parameters at the end of a backup job. It is enabled by default.\nTip\nFor more details about all the options of the\nClient\nresource, please check the upstream\nClient Resource\ndocumentation.\nThe\nPool\nresource\n¶\nA\nPool\nin Bacula represents a collection of volumes. A\nVolume\nis a single physical tape, or a file on disk, and is where Bacula will write the backup data.\nThe default configuration file will have defined several\nPools\nalready. For this documentation, we are interested in the\nFile\npool:\nPool\n{\nName\n=\nFile\nPool\nType\n=\nBackup\nRecycle\n=\nyes\n# Bacula can automatically recycle Volumes\nAutoPrune\n=\nyes\n# Prune expired volumes\nVolume\nRetention\n=\n365\ndays\n# one year\nMaximum\nVolume\nBytes\n=\n50\nG\n# Limit Volume size\nMaximum\nVolumes\n=\n100\n# Limit number of Volumes in Pool\nLabel\nFormat\n=\n\"Vol-\"\n# Auto label\n}\nWe will use this pool to backup to a directory on the server (which will usually be the mount point for a big storage device). The pool resource has some definitions that affect how large the backups can become, so these have to be checked:\nName\n: The name of the pool, which will be referenced in other resources.\nVolume\nRetention\n: For how long volumes are kept.\nMaximum\nVolume\nBytes\n: What is the maximum size of each volume file.\nMaximum\nVolumes\n: How many volume files are we going to keep at most.\nLabel\nFormat\n: The prefix that each volume file will get. In this example, the files will be automatically named\nVol-0001\n,\nVol-0002\n, and so on.\nWith the values in the example above, we will be storing at most 50G * 100 = 5000GB in this pool.\nTip\nFor more details about all the options of the\nPool\nresource, please check the upstream\nPool Resource\ndocumentation.\nThe\nStorage\nresource\n¶\nThe\nStorage\nresource in the bacula Director configuration file points at the system where the Storage component is running.\nIn our current setup, that’s the same system where the Director is running, but we\nMUST NOT\nuse\nlocalhost\nin the definition, because this configuration is also sent to the File component on other systems. In another system,\nlocalhost\nwill mean itself, but the Storage daemon is not running over there.\nThis time we will have to change two configuration files: the Director one, and the Storage one. Let’s begin by defining a\nStorage\nresource on\n/etc/bacula/bacula-dir.conf\n, the Director configuration file:\nStorage\n{\nName\n=\nFileBackup\nAddress\n=\nbacula\n-\nserver\n.\nlxd\nSDPort\n=\n9103\n# For this password, use:\n# sudo grep ^SDPASSWD /etc/bacula/common_default_passwords\nPassword\n=\n\"<SDPASSWD value>\"\nDevice\n=\nFileBackup\nMedia\nType\n=\nFile\n}\nHere is what we have defined with the block above:\nName\n: The name of this Storage resource, which will be referenced in other places.\nAddress\n: The name of the system where the Storage daemon is running. Again, never use\nlocalhost\nhere, even if it’s the same system where the Director is running.\nPassword\n: The password that should be used when connecting to the Storage daemon. The installation of the packages will have generated a random password. It can be found either in the existing\nAutochanger\ndefinitions in\n/etc/bacula/bacula-dir.conf\n, or in\n/etc/bacula/common_default_passwords\nin the line for\nSDPASSWD\n, or in the Storage daemon configuration file\n/etc/bacula/bacula-sd.conf\n.\nDevice\n: This must match an existing\nDevice\ndefinition in the Storage daemon’s configuration file (which will be covered next).\nMedia\nType\n: Likewise, this must also match the same\nMedia\nType\ndefined in the Storage daemon’s configuration file.\nTip\nFor more details about all the options of the\nPool\nresource, please check the upstream\nStorage Resource\ndocumentation.\nNext we need to edit the corresponding Storage daemon configuration in\n/etc/bacula/bacula-sd.conf\n.\nFirst, remove or comment out the\nSDAddress\nconfiguration, so that the daemon will listen on all network interfaces it finds:\nStorage\n{\nName\n=\nbacula\n-\nserver\n-\nsd\nSDPort\n=\n9103\nWorkingDirectory\n=\n\"/var/lib/bacula\"\nPid\nDirectory\n=\n\"/run/bacula\"\nPlugin\nDirectory\n=\n\"/usr/lib/bacula\"\nMaximum\nConcurrent\nJobs\n=\n20\nEncryption\nCommand\n=\n\"/etc/bacula/scripts/key-manager.py getkey\"\n#SDAddress = 127.0.0.1\n}\nImportant points for the config above:\nName\n: It’s standard for Bacula systems to suffix the name of the system where a component is running with the abbreviation of that component. In this case, the name of the system is\nbacule-server\n, and the component we are defining is the Storage Daemon, hence the\n-sd\nsuffix.\nSDAddress\n: We need this daemon to listen on all interfaces so it’s reachable from other systems, so we comment this line out and rely on the default which it to listen on all interfaces.\nNext, let’s define a\nDevice\n, also in\n/etc/bacula/bacula-sd.conf\n:\nDevice\n{\nName\n=\nFileBackup\nMedia\nType\n=\nFile\nArchive\nDevice\n=\n/\nstorage\n/\nbackups\nRandom\nAccess\n=\nyes\nAutomatic\nMount\n=\nyes\nRemovable\nMedia\n=\nno\nAlways\nOpen\n=\nno\nLabel\nMedia\n=\nyes\n}\nWhat we need to pay close attention to here is:\nName\n: This has to match the name this device will be referred to in other services. In our case, it matches the name we are using in the\nDevice\nentry of the\nStorage\ndefinition we added to the Director configuration file\n/etc/bacula/bacula-dir.conf\nearlier.\nMedia\nType\n: Likewise, this has to match the entry we used in the\nStorage\ndefinition in the Director.\nArchive\nDevice\n: Since we are going to store backups as files, and not as tapes, the\nArchive\nDevice\nconfiguration points to a directory. Here we are using\n/storage/backups\n, which can be the mount point of an external storage for example. This is the target directory of all backup jobs what will refer to this device of this storage server.\nLabel\nMedia\n: Since we are using files and not real tapes, we want the Storage daemon to actually name the files for us. This configuration option allows it to do so.\nLastly, the Storage component needs to be told about the Director. This is done with a\nDirector\nresource in\n/etc/bacula/bacula-sd.conf\n. No changes should be needed here because we installed the Storage component on the same host as the Director, but it’s best to check:\nDirector\n{\nName\n=\nbacula\n-\nserver\n-\ndir\nPassword\n=\n\"<randomly generated>\"\n}\nThese two options need to match the following:\nName\n: This names which Director is allowed to use this Storage component, and therefore needs to match the\nName\ndefined in the\nDirector\nresource in\n/etc/bacula/bacula-dir.conf\non the Director system.\nPassword\n: The password that the Director needs to use to authenticate against this Storage component. This needs to match the\nPassword\nset in the\nStorage\nresource in\n/etc/bacula/bacula-dir.conf\non the Directory system.\nAfter making all changes to the\nbacula-sd.conf\nconfiguration file, restart the Storage Daemon:\nsudo systemctl restart bacula-sd.service\nThe\nJob\nresource\n¶\nThe\nJob\nresource is the basic unit in Bacula, and ties everything together:\nWho is being backed up (\nClient\n).\nWhat should be backed up (\nFileSet\n).\nWhere should the data be stored (\nStorage\n,\nPool\n), and where to record the job (\nCatalog\n)\nWhen thouls the job run (\nSchedule\n)\nThe default Director configuration file includes a default Job resource, and more Jobs can inherit from that.\nLet’s go over the Default Job resource first in\n/etc/bacula/bacula-dir.conf\nand change it a little bit:\nJobDefs\n{\nName\n=\n\"DefaultJob\"\nType\n=\nBackup\nLevel\n=\nIncremental\nClient\n=\nbacula\n-\nserver\n-\nfd\nFileSet\n=\n\"Home Set\"\nSchedule\n=\n\"WeeklyCycle\"\nStorage\n=\nFileBackup\nMessages\n=\nStandard\nPool\n=\nFile\nSpoolAttributes\n=\nyes\nPriority\n=\n10\nWrite\nBootstrap\n=\n\"/var/lib/bacula/\n%c\n.bsr\"\n}\nThis configuration is selecting some defaults:\nName\n: The name of this Job.\nClient\n: To which client it applies. This must match an existing\nClient\n{}\nresource definition.\nFileSet\n: The name of the\nFileSet\nresource that defines the data to be backed up. We changed it to\nHome\nSet\nin this example.\nSchedule\n: The name of the\nSchedule\nresource that defines when this job should run.\nStorage\n: Which\nStorage\nresource this job should use. We changed it to\nFileBackup\nin this example.\nPool\n: Which\nPool\nresource this job should use.\nWe can now take advantage of this set of defaults, and define a new Job resource with minimal config:\nJob\n{\nName\n=\n\"DirectorHomeBackup\"\nJobDefs\n=\n\"DefaultJob\"\n}\nTip\nA job has many attributes that can only be specified once. This means that jobs are pretty much specific to what is being backed up, and from where, among other things. It therefore helps to come up with a naming convention.\nThe upstream documentation has a section about\nNaming Resources\nwith some suggestions.\nWe also need a Job definition for the restore task. The default configuration file will have a definition for this already, but it needs to be changed:\nJob\n{\nName\n=\n\"RestoreFiles\"\nType\n=\nRestore\nClient\n=\nbacula\n-\nserver\n-\nfd\nStorage\n=\nFileBackup\n# The FileSet and Pool directives are not used by Restore Jobs\n# but must not be removed\nFileSet\n=\n\"Home Set\"\nPool\n=\nFile\nMessages\n=\nStandard\nWhere\n=\n/\nstorage\n/\nrestore\n}\nImportant parameters defined above:\nName\n: The name of this job.\nType\n: This is a job that restores backups (\nRestore\n).\nClient\n: Where the files should be restored to. This can be overridden when the job is invoked.\nStorage\n: The storage from where the backup should be restored. We changed it to\nFileBackup\nin this example.\nFileSet\nand\nPool\n: These are not used, but must be present and point to valid resources. We changed\nFileSet\nto\nHome\nSet\nin this example.\nWhere\n: The path where the restored files should be placed. This can also be overridden when the job is invoked. We changed it to\n/storage/restore\nin this example.\nTip\nFor more details about all the options of the\nJob\nresource, please check the upstream\nJob Resource\ndocumentation.\nStorage daemon\n¶\nThere isn’t much more to configure for the Storage daemon after the Director configuration steps done earlier, but we still need to create the directories for the backup and restore jobs:\nsudo mkdir -m 0700 /storage /storage/backups /storage/restore\nsudo chown bacula: -R /storage\nThis will allow bacula, and only bacula, to read and write to the storage path. You can, of course, adjust the permissions and ownership to something that suits your deployment. Just be mindful that the bacula user needs to be able to create and remove files from the\n/storage/backups\nand\n/storage/restore\npaths, and that regular users should not be allowed to read those.\nTip\nFor more details about the Storage daemon configuration options, please check the upstream\nStorage Daemon\ndocumentation.\nFile daemon\n¶\nThe File daemon configuration is located in the\n/etc/bacula/bacula-fd.conf\nfile, and the only remaining task is to make sure it listens on the network. To be fair, in this particular deployment layout, this is not strictly needed, as both the Director and Storage daemons are located on the same system, but making this change allows for those components to be split off to different systems should that need arise.\nTo make this change, we are going to remove or comment out the\nFDAddress\noption in the\nFileDaemon\nresource in\n/etc/bacula/bacula-fd.conf\nfile:\nFileDaemon\n{\nName\n=\nbacula\n-\nserver\n-\nfd\nFDport\n=\n9102\nWorkingDirectory\n=\n/\nvar\n/\nlib\n/\nbacula\nPid\nDirectory\n=\n/\nrun\n/\nbacula\nMaximum\nConcurrent\nJobs\n=\n20\nPlugin\nDirectory\n=\n/\nusr\n/\nlib\n/\nbacula\n#FDAddress = 127.0.0.1\n}\nAfter making the change and saving the file, restart the File daemon service:\nsudo systemctl restart bacula-fd.service\nTip\nFor more details about the File daemon configuration, please check the upstream\nFile Daemon\ndocumentation.\nConsole\n¶\nThere is no further configuration to be done for the Console at this time. The defaults selected and adjusted by the package install are sufficient. The configuration file is\n/etc/bacula/bconsole.conf\n, and more details are available in the upstream\nConsole Configuration\ndocumentation.\nThe Console can be used to query the Director about jobs, but to use the Console with a\nnon-root\nuser, the user needs to be in the\nBacula group\n. To add a user to the Bacula group, run the following command from a terminal:\nsudo adduser <username> bacula\nReplace\n<username>\nwith the actual username. Also, if you are adding the current user to the group you should log out and back in for the new permissions to take effect.\nWarning\nBe mindful of who is added to the\nbacula\ngroup: members of this group are able to read all the data that is being backed up!\nCleaning up\n¶\nWe have added new resources to some Bacula components, and changed some existing ones. There are also resources we didn’t touch, but they will show up in the console or logs. Optionally, we can remove them to cleanup our config files.\nIn\n/etc/bacula/bacula-dir.conf\n:\nall the\nAutochanger\nresources can be removed, since they are not referred to by any other resource.\nIn\n/etc/bacula/bacula-sd.conf\n:\nThe\nAutochanger\nresources can be removed, as it’s not being used.\nThe\nFileChgr1-Dev1\n,\nFileChgr1-Dev2\n,\nFileChgr2-Dev1\n, and\nFileChgr2-Dev2\nDevices, referred to by the Autochangers above, should then also be removed.\nWe made many changes to a few configuration files, so let’s restart all the related services:\nsudo systemctl restart bacula-dir.service bacula-fd.service bacula-sd.service\nRunning a backup job\n¶\nWe now have everything in place to run our first backup job.\nOn the Bacula Director system, run the\nbconsole\ncommand as root to enter the Bacula Console:\nsudo bconsole\nThe command will connect to the the local Director, and open up an interactive prompt:\nConnecting to Director localhost:9101\n1000 OK: 10002 bacula-server-dir Version: 15.0.3 (25 March 2025)\nEnter a period to cancel a command.\n*\nYou can type\nhelp\nfor a full list of all the available commands, and\nhelp\n<command>\nfor more detailed information about the specific\n<command>\n.\nFor example, to obtain help text about the\nrun\ncommand, type\nhelp\nrun\nto obtain the following output:\nCommand       Description\n  =======       ===========\n  run           Run a job\n\nArguments:\n  job=<job-name> client=<client-name>\n  fileset=<FileSet-name> level=<level-keyword>\n  storage=<storage-name> where=<directory-prefix>\n  when=<universal-time-specification> pool=<pool-name>\n  nextpool=<next-pool-name> comment=<text> accurate=<bool> spooldata=<bool> yes\n\nWhen at a prompt, entering a period cancels the command.\nLet’s interactively run a backup job. The output below will show the\nrun\ncommand and all the replies that were typed in response to the console prompts:\n*run\nUsing Catalog \"MyCatalog\"\nA job name must be specified.\nThe defined Job resources are:\n     1: HomeBackup\n     2: BackupCatalog\n     3: RestoreFiles\nSelect Job resource (1-3): 1\nRun Backup job\nJobName:  HomeBackup\nLevel:    Incremental\nClient:   bacula-server-fd\nFileSet:  Home Set\nPool:     File (From Job resource)\nStorage:  FileBackup (From Job resource)\nWhen:     2025-10-20 20:21:03\nPriority: 10\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=7\nLet’s unpack this:\nrun\n: This is the command. Since no parameters were given, Bacula will ask for what’s missing.\nWhich job should be run:\nHomeBackup\nis the job we defined in this how-to, and we select it by its index number.\nSummary: at the end, we are given a summary of the job. Here we can still change values via the\nmod\nreply, but for now let’s just accept those values and reply\nyes\n.\nJobId: the job is accepted, and we are given an ID. In this case, it was “\n7\n”.\nTo check the result of a job, there are several methods:\nMessages: right after scheduling the job, it’s likely something will be logged. You can run the\nmessages\ncommand, and it will show the latest unread messages (and also mark them as read, so you can only benefit from this once).\nThe\nlist\njobs\ncommand, to list all jobs, or, more specifically,\nlist\njobid=7\nto list a particular job.\nInspect the full log of that particular job, via the\nlist\njoblog\njobid=<N>\ncommand.\nServer log: you can inspect the server log at\n/var/log/bacula/bacula.log\n.\nFor example, if we run\nlist\njobid=7\n, this is the output:\n+-------+------------+---------------------+------+-------+----------+----------+-----------+\n| jobid | name       | starttime           | type | level | jobfiles | jobbytes | jobstatus |\n+-------+------------+---------------------+------+-------+----------+----------+-----------+\n|     7 | HomeBackup | 2025-10-20 20:21:11 | B    | I     |        0 |        0 | T         |\n+-------+------------+---------------------+------+-------+----------+----------+-----------+\nThat tells us some details about this job, in particular that it finished correctly (the\nT\ncode).\nTip\nFor a list of status and error codes, check the upstream\nJob status and Error codes tables\n.\nTo see the full log of this specific job, we can use the\nlist\njoblog\njobid=7\ncommand. This is quite detailed, and the output below is truncated for brevity:\n+----------------------------------------------------------------------------------------------------+\n| logtext                                                                                              |\n+----------------------------------------------------------------------------------------------------+\n| bacula-server-dir JobId 7: Start Backup JobId 7, Job=HomeBackup.2025-10-20_20.21.08_03               |\n| bacula-server-dir JobId 7: Connected to Storage \"FileBackup\" at bacula-server.lxd:9103 with TLS      |\n| bacula-server-dir JobId 7: Using Device \"FileBackup\" to write.                                       |\n...\n  Build OS:               x86_64-pc-linux-gnu ubuntu 25.10\n  JobId:                  7\n  Job:                    HomeBackup.2025-10-20_20.21.08_03\n  Backup Level:           Incremental, since=2025-10-20 18:11:00\n  Client:                 \"bacula-server-fd\" 15.0.3 (25Mar25) x86_64-pc-linux-gnu,ubuntu,25.10\n  FileSet:                \"Home Set\" 2025-10-20 16:27:31\n  Pool:                   \"File\" (From Job resource)\n  Catalog:                \"MyCatalog\" (From Client resource)\n  Storage:                \"FileBackup\" (From Job resource)\n...\n  Non-fatal FD errors:    0\n  SD Errors:              0\n  FD termination status:  OK\n  SD termination status:  OK\n  Termination:            Backup OK                                                                    |\n...\nIf we inspect the backup target location on the Storage server (which in this deployment is the same as the Director), we can see that a volume file was created:\n-\nrw\n-\nr\n-----\n1\nbacula\ntape\n345\nK\nOct\n20\n20\n:\n21\n/\nstorage\n/\nbackups\n/\nVol\n-\n0001\nRestoring a backup\n¶\nSo what is it that was backed up? This job used the\nHome\nSet\n, so we expect to see files from the\n/home\ndirectory. To see what are the contents of that backup job, we can use the\nrestore\ncommand (the\nRestoreFiles\njob should never be executed directly). Below is the output of an interactive\nrestore\nsession where we selected the option “Select the most recent backup for a client”:\nFirst you select one or more JobIds that contain files\nto be restored. You will be presented several methods\nof specifying the JobIds. Then you will be allowed to\nselect which files from those JobIds are to be restored.\n\nTo select the JobIds, you have the following choices:\n     1: List last 20 Jobs run\n...\n     5: Select the most recent backup for a client\n...\nSelect item:  (1-14): 5\nAutomatically selected Client: bacula-server-fd\nAutomatically selected FileSet: Home Set\n+-------+-------+----------+----------+---------------------+------------+\n| jobid | level | jobfiles | jobbytes | starttime           | volumename |\n+-------+-------+----------+----------+---------------------+------------+\n|     4 | F     |      266 |  312,756 | 2025-10-20 17:34:43 | Vol-0001   |\n|     5 | I     |        3 |       32 | 2025-10-20 18:11:00 | Vol-0001   |\n+-------+-------+----------+----------+---------------------+------------+\nYou have selected the following JobIds: 4,5\n...\nYou are now entering file selection mode where you add (mark) and\nremove (unmark) files to be restored. No files are initially added, unless\nyou used the \"all\" keyword on the command line.\nEnter \"done\" to leave this mode.\n\ncwd is: /\n$\nHere we can navigate the filesystem and inspect which files are part of the backup:\n$ dir\ndrwxr-xr-x   1 root     root              12  2025-10-20 14:03:44  /home/\n$ cd home/ubuntu\ncwd is: /home/ubuntu/\n$ dir\n-rw-------   1 ubuntu   ubuntu            32  2025-10-20 18:10:51  /home/ubuntu/.bash_history\n-rw-r--r--   1 ubuntu   ubuntu           220  2025-10-20 14:03:50  /home/ubuntu/.bash_logout\n-rw-r--r--   1 ubuntu   ubuntu          3830  2025-10-20 14:03:50  /home/ubuntu/.bashrc\n-rw-r--r--   1 ubuntu   ubuntu           807  2025-10-20 14:03:44  /home/ubuntu/.profile\ndrwx------   1 ubuntu   ubuntu            30  2025-10-20 14:03:45  /home/ubuntu/.ssh/\n-rw-rw-r--   1 ubuntu   ubuntu             0  2025-10-20 18:10:50  /home/ubuntu/this-is-on-the-server.txt\nTo restore a file, we use the\nmark\ncommand on it. For example, let’s restore\n/home/ubuntu/.tmux.conf\n:\n$ mark .tmux.conf\n1 file marked.\n$ done\nBootstrap records written to /var/lib/bacula/bacula-server-dir.restore.1.bsr\n\nThe Job will require the following (*=>InChanger):\n   Volume(s)                 Storage(s)                SD Device(s)\n===========================================================================\n\n    Vol-0001                  FileBackup                FileBackup\n\nVolumes marked with \"*\" are in the Autochanger.\n\n\n1 file selected to be restored.\n\nRun Restore job\nJobName:         RestoreFiles\nBootstrap:       /var/lib/bacula/bacula-server-dir.restore.1.bsr\nWhere:           /storage/restore\nReplace:         Always\nFileSet:         Home Set\nBackup Client:   bacula-server-fd\nRestore Client:  bacula-server-fd\nStorage:         FileBackup\nWhen:            2025-10-20 21:02:37\nCatalog:         MyCatalog\nPriority:        10\nPlugin Options:  *None*\nOK to run? (Yes/mod/no):\nNow we have some choices. Notice how the\nRestoreFiles\njob was automatically selected. That’s the only job of the type\nRestore\nthat we defined in the Director configuration earlier. It has certain default values, and we can either accept those (by replying\nyes\n), or modify them (by replying\nmod\n).\nIf we accept these default, the marked files will be restored to the\n/storage/restore\npath on the\nbacula-server-fd\nsystem:\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=8\n*\nAnd indeed, if we inspect that location, we see the file that we marked for restoration:\n-rw-r--r-- 1 ubuntu ubuntu 2.4K Oct 20 14:03 /storage/restore/home/ubuntu/.tmux.conf\nIf we wanted to restore it to its original place, for example, if the user mistakenly deleted it and wanted it back, we would select the\nmod\noption to change where the file should be placed:\nOK to run? (Yes/mod/no): mod\nParameters to modify:\n     1: Level\n     2: Storage\n     3: Job\n     4: FileSet\n     5: Restore Client\n     6: When\n     7: Priority\n     8: Bootstrap\n     9: Where\n    10: File Relocation\n    11: Replace\n    12: JobId\n    13: Plugin Options\nSelect parameter to modify (1-13): 9\nPlease enter the full path prefix for restore (/ for none): /\nRun Restore job\nJobName:         RestoreFiles\nBootstrap:       /var/lib/bacula/bacula-server-dir.restore.2.bsr\nWhere:\nReplace:         Always\nFileSet:         Home Set\nBackup Client:   bacula-server-fd\nRestore Client:  bacula-server-fd\nStorage:         FileBackup\nWhen:            2025-10-20 21:11:55\nCatalog:         MyCatalog\nPriority:        10\nPlugin Options:  *None*\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=9\nBy giving a restoration prefix of\n/\n, we are essentially asking to restore the file at its original full path.\nAdding a client\n¶\nIf we want to start backing up a new system, we need to install the File Daemon on that system and include it in the Bacula Director. In this example, the new system we are adding is called\nworkstation1\n.\nFirst, on the system that we want to add, let’s install the client portion of Bacula, which is the File Daemon component:\nsudo apt install bacula-fd\nNext, update the Director resource in\n/etc/bacula/bacula-fd.conf\nto point at the existing Director we have already deployed:\nDirector\n{\nName\n=\nbacula\n-\nserver\n-\ndir\n# same as Director's Name on the Director server\nPassword\n=\n\"<randomly generated>\"\n}\nNotes:\nName\n: This has to be the same name set on the Director’s\n/etc/bacula/bacula-dir.conf\nfile, in the\nDirector\nresource over there. It’s not the hostname.\nPassword\n: The password was randomly generated when the\nbacula-fd\npackage was installed. This password has to match the password in the new\nClient\nresource that we will add to the Director next, so keep note of it.\nAlso in\n/etc/bacula/bacula-fd.conf\n, we have to remove or comment out the\nFDAddress\nparameter in the\nFileDaemon\nresource, so that this service will listen on all available network interfaces, and not just localhost:\nFileDaemon\n{\nName\n=\nworkstation1\n-\nfd\nFDport\n=\n9102\n# where we listen for the director\nWorkingDirectory\n=\n/\nvar\n/\nlib\n/\nbacula\nPid\nDirectory\n=\n/\nrun\n/\nbacula\nMaximum\nConcurrent\nJobs\n=\n20\nPlugin\nDirectory\n=\n/\nusr\n/\nlib\n/\nbacula\n#FDAddress = 127.0.0.1  # default is to listen on all interfaces\n}\nAnd finally, in the same file, update the\nMessages\nresource and update the Director name in there as well:\nMessages {\n    Name = Standard\n    director = bacula-server-dir = all, !skipped, !restored, !verified, !saved\n}\nWith these changes done, restart the File Daemon:\nsudo systemctl restart bacula-fd.service\nNow we switch to the Director system, where we have to let it know about this new Client that we just provisioned.\nIn\n/etc/bacula/bacula-dir.conf\n, add a new\nClient\nresource:\nClient\n{\nName\n=\nworkstation1\n-\nfd\nAddress\n=\nworkstation1\n.\nlxd\nFDPort\n=\n9102\nCatalog\n=\nMyCatalog\nPassword\n=\n\"<to be filled in>\"\n# password from bacula-fd.conf on workstation1-fd\nFile\nRetention\n=\n60\ndays\nJob\nRetention\n=\n6\nmonths\nAutoPrune\n=\nyes\n}\nNotes:\nName\n: The name has to match the name defined in the\nFileDaemon\nresource from\n/etc/bacula/bacula-fd.conf\nof the system we just added.\nPassword\n: The password has to be the same as the one defined in the\nDirector\nresource from\n/etc/bacula/bacula-fd.conf\nof that system.\nAddress\n: The hostname or IP of the system we added.\nThis makes the Director know how to reach the new client.\nNow we have to define a new job to backup files from this new client. Again on\n/etc/bacula/bacula-dir.conf\non the Director, let’s add a new\nJob\nresource:\nJob\n{\nName\n=\n\"BackupWorkstation\"\nJobDefs\n=\n\"DefaultJob\"\nClient\n=\nworkstation1\n-\nfd\n}\nThis job inherits all parameters from the\nDefaultJob\n, and just overrides the client.\nWith this done, we can restart the Director:\nsudo systemctl restart bacula-dir.service\nIf we now enter the Bacula console, we should be able to list the new client, and run its new backup job:\n*list clients\nAutomatically selected Catalog: MyCatalog\nUsing Catalog \"MyCatalog\"\n+----------+------------------+---------------+--------------+\n| clientid | name             | fileretention | jobretention |\n+----------+------------------+---------------+--------------+\n|        1 | bacula-server-fd |     5,184,000 |   15,552,000 |\n|        2 | workstation1-fd  |     5,184,000 |   15,552,000 |\n+----------+------------------+---------------+--------------+\nLet’s run the new\nBackupWorkstation\njob:\n*run\nUsing Catalog \"MyCatalog\"\nA job name must be specified.\nThe defined Job resources are:\n     1: HomeBackup\n     2: BackupWorkstation\n     3: BackupCatalog\n     4: RestoreFiles\nSelect Job resource (1-4): 2\nRun Backup job\nJobName:  BackupWorkstation\nLevel:    Incremental\nClient:   workstation1-fd\nFileSet:  Home Set\nPool:     File (From Job resource)\nStorage:  FileBackup (From Job resource)\nWhen:     2025-10-21 21:02:48\nPriority: 10\nOK to run? (Yes/mod/no): yes\nJob queued. JobId=14\nYou have messages.\nAnd for a quick check of the contents (for testing, there was a file called\nthis-is-workstation1.txt\nin\n/home/ubuntu\non that system):\n*restore\n...\n     5: Select the most recent backup for a client\n...\nSelect item:  (1-14): 5\nDefined Clients:\n     1: bacula-server-fd\n     2: workstation1-fd\nSelect the Client (1-2): 2\n...\n$ cd home/ubuntu\ncwd is: /home/ubuntu/\n$ dir this*\n-rw-rw-r--   1 ubuntu   ubuntu             0  2025-10-21 21:02:08  /home/ubuntu/this-is-workstation1.txt\nFurther reading\n¶\nFor more Bacula configuration options, refer to the\nBacula documentation\n.\nThe\nBacula home page\ncontains the latest Bacula news and developments.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 5863}}
{"id": "fcd71d4d11", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-etckeeper/", "title": "etckeeper - Ubuntu Server documentation", "text": "etckeeper - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\netckeeper\n¶\netckeeper\nallows the contents of\n/etc\nto be stored in a Version Control System (VCS) repository. It integrates with APT and automatically commits changes to\n/etc\nwhen packages are installed or upgraded.\nPlacing\n/etc\nunder version control is considered an industry best practice, and the goal of etckeeper is to make this process as painless as possible.\nInstall etckeeper\n¶\nInstall etckeeper by entering the following in a terminal:\nsudo\napt\ninstall\netckeeper\nInitialize etckeeper\n¶\nThe main configuration file,\n/etc/etckeeper/etckeeper.conf\n, is fairly simple. The main option defines which VCS to use, and by default etckeeper is configured to use git.\nThe repository is automatically initialized (and committed for the first time) during package installation. It is possible to undo this by entering the following command:\nsudo\netckeeper\nuninit\nConfigure autocommit frequency\n¶\nBy default, etckeeper will commit uncommitted changes made to\n/etc\non a daily basis. This can be disabled using the\nAVOID_DAILY_AUTOCOMMITS\nconfiguration option.\nIt will also automatically commit changes before and after package installation. For a more precise tracking of changes, it is recommended to commit your changes manually, together with a commit message, using:\nsudo\netckeeper\ncommit\n\"Reason for configuration change\"\nThe\nvcs\netckeeper command provides access to any subcommand of the VCS that etckeeper is configured to run. It will be run in\n/etc\n. For example, in the case of git:\nsudo\netckeeper\nvcs\nlog\n/etc/passwd\nTo demonstrate the integration with the package management system (APT), install\npostfix\n:\nsudo\napt\ninstall\npostfix\nWhen the installation is finished, all the\npostfix\nconfiguration files should be committed to the repository:\n[master 5a16a0d] committing changes in /etc made by \"apt install postfix\"\n Author: Your Name <xyz@example.com>\n 36 files changed, 2987 insertions(+), 4 deletions(-)\n create mode 100755 init.d/postfix\n create mode 100644 insserv.conf.d/postfix\n create mode 100755 network/if-down.d/postfix\n create mode 100755 network/if-up.d/postfix\n create mode 100644 postfix/dynamicmaps.cf\n create mode 100644 postfix/main.cf\n create mode 100644 postfix/main.cf.proto\n create mode 120000 postfix/makedefs.out\n create mode 100644 postfix/master.cf\n create mode 100644 postfix/master.cf.proto\n create mode 100755 postfix/post-install\n create mode 100644 postfix/postfix-files\n create mode 100755 postfix/postfix-script\n create mode 100755 ppp/ip-down.d/postfix\n create mode 100755 ppp/ip-up.d/postfix\n create mode 120000 rc0.d/K01postfix\n create mode 120000 rc1.d/K01postfix\n create mode 120000 rc2.d/S01postfix\n create mode 120000 rc3.d/S01postfix\n create mode 120000 rc4.d/S01postfix\n create mode 120000 rc5.d/S01postfix\n     create mode 120000 rc6.d/K01postfix\n     create mode 100755 resolvconf/update-libc.d/postfix\n     create mode 100644 rsyslog.d/postfix.conf\n     create mode 120000 systemd/system/multi-user.target.wants/postfix.service\n     create mode 100644 ufw/applications.d/postfix\nFor an example of how\netckeeper\ntracks manual changes, add new a host to\n/etc/hosts\n. Using git you can see which files have been modified:\nsudo\netckeeper\nvcs\nstatus\nand how:\nsudo\netckeeper\nvcs\ndiff\nIf you are happy with the changes you can now commit them:\nsudo\netckeeper\ncommit\n\"added new host\"\nResources\n¶\nSee the\netckeeper\nsite for more details on using etckeeper.\nFor documentation on the git VCS tool see\nthe Git website\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:13Z", "original_len_words": 503}}
{"id": "0ae63b7f60", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-gitolite/", "title": "How to set up gitolite - Ubuntu Server documentation", "text": "How to set up gitolite - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to set up gitolite\n¶\nGitolite\nGitolite allows you to setup git hosting on a central server, with fine-grained access control and many more powerful features.\nYou can use your served repositories as\ngit\nremote\nin the form of\ngit@yourserver:some/repo/path\n.\nGitolite stores\n“bare git”\nrepositories at a location of your choice, usually\n/home/git\n.\nIt has its independent user realm, each user is created by assigning their\nSSH-key\n.\nThe repos itself are owned by one system user of your choice, usually\ngit\n.\nInstall a gitolite server\n¶\nGitolite can be installed with the following command.\nThe install automation will ask for a path or content of your\nadmin\nssh key.\nFor a better understanding of your setup we recommend to leave the prompt empty for full control of your setup (so you can use\ngit\nas the username, and customize the storage path).\nsudo\napt\ninstall\ngitolite3\nConfigure gitolite\n¶\nGitolite stores its configuration in a git repository (called\ngitolite-admin\n), so there’s no configuration in\n/etc\n.\nThis configuration repository manages all other git repos, users and their permissions.\nCreate a\ngit\nuser for gitolite to use for the service (you can adjust the git repo storage path as the\n--home\ndirectory):\nsudo\nuseradd\n--system\n--home\n/home/git\ngit\nTo access the config repository, we now add the administrator’s public\nSSH-key\n.\nIf you have not yet configured an SSH key, refer to the section on\nSSH keys in our OpenSSH guide\n.\nWe copy it to\n/tmp\nso our\ngit\nuser can read the file to import it.\nPlease adjust the path to the desired admin user’s\nSSH-key\n(and algorithm, like\nid_rsa.pub\n).\ncp\n~/.ssh/id_ed25519.pub\n/tmp/admin.pub\nAs the\ngit\nuser, let’s import the administrator’s key into gitolite (it will get the\nadmin\nusername due to that key’s filename).\nsudo\n-i\n-u\ngit\ngitolite\nsetup\n-pk\n/tmp/admin.pub\nWhat this creates:\nthe management repo in\n~git/repositories/gitolite-admin.git\na global config in\n~git/.gitolite.rc\n~git/projects.list\nas repo overview\n~git/.ssh/authorized_keys\nwith\ncommand=\nto force gitolite over\nssh\nlater it will contain the\nssh\npublic key for each user you configured\nTo try if the setup worked, try\nssh\nas the user owning the admin key we just added, so see the\ngitolite repo overview\n:\nssh\ngit@yourserver\nhello\nadmin\n,\nthis\nis\ngit\n@your\n-\ngitolite\n-\nserver\nrunning\ngitolite3\nR\nW\ngitolite\n-\nadmin\nR\nW\ntesting\nManaging gitolite users and repositories\n¶\nTo configure gitolite users, repositories and permissions, clone the configuration repository.\n$yourserver\ncan be an ip-address, hostname, or just\nlocalhost\nfor your current machine.\ngit\nclone\ngit@\n$yourserver\n:gitolite-admin.git\nTo apply configuration change, commit them in the repo and\npush the changes\nback to the server with:\ngit\ncommit\n-a\ngit\npush\norigin\nmaster\nThe\ngitolite-admin\ncontains two subdirectories:\nkeydir\n(which contains the list of users’ public SSH keys) and\nconf\n(which contains configuration files).\nTo\nadd a gitolite user\n(it’s virtual - not a system username), obtain their SSH public key (from\n~user/.ssh/id_<name>.pub\n) and add it to the\nkeydir\ndirectory as\n<desired-username>.pub\n.\nTo\ndelete a gitolite user\n, you only need to delete their public key files.\nTo manage repositories and groups in\nconf/gitolite.conf\n, specify the the list of repositories followed by some access rules.\nHave an example:\n# gitolite config\n# users are created by their public key in keydir/$username.pub\n\n# group creation\n@bestproject          = name1 name2\n@projectwatchers      = name3 @bestproject\n\n# this repo itself\nrepo    gitolite-admin\n        RW+     =   admin\n        R       =   alice\n\n# a repo with access to anybody\nrepo    testing\n        RW+     = @all\n\n# a repo with special privileges, to tags and branches\nrepo    some/awesome/project\n        RW                      =   alice @bestproject\n        RW+                     =   bob\n        RW+   dev/              =   @bestproject\n        R                       =   @projectwatchers carol\n# bestproject members and alice can push code (but not force-push)\n# bestproject members can force-push branches starting with dev/\n# bob can forcepush anything\n# projectwatchers and carol have readonly access\nFor more advanced permission configuration (restricting tags, branches, …), please see the examples in the upstream documentation\npage 1\nand\npage 2\n.\nUsing your server\n¶\nNow you can use your newly set up gitolite server as a regular\ngit\nremote\n.\nOnce a user is created and has permissions, they can access the repositories.\nAs a fresh clone:\ngit\nclone\ngit@\n$server\n:some/awesome/project.git\nOr as a remote to an existing repository:\ngit\nremote\nadd\ngitolite\ngit@\n$server\n:some/awesome/project.git\nFurther reading\n¶\nGitolite’s code repository\nprovides access to source code\nGitolite’s documentation\nincludes more detailed configuration guides and a “fool-proof setup”, with how-tos for common tasks\nGitolite’s maintainer has written a book,\nGitolite Essentials\n, for more in-depth information about the software\nGeneral information about\ngit\nitself can be found at the\nGit homepage", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:14Z", "original_len_words": 822}}
{"id": "d211a5afa7", "source_url": "https://documentation.ubuntu.com/server/how-to/backups/install-rsnapshot/", "title": "How to install and configure rsnapshot - Ubuntu Server documentation", "text": "How to install and configure rsnapshot - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure rsnapshot\n¶\nrsnapshot\nis an rsync-based\nfilesystem\nsnapshot utility. It can take incremental backups of local and remote filesystems for any number of machines. rsnapshot makes extensive use of hard links, so disk space is only used when absolutely necessary. It leverages the power of rsync to create scheduled, incremental backups.\nInstall rsnapshot\n¶\nTo install\nrsnapshot\nopen a terminal shell and run:\nsudo\napt-get\ninstall\nrsnapshot\nIf you want to backup a remote filesystem, the rsnapshot server needs to be able to access the target machine over SSH without password. For more information on how to enable this please see\nOpenSSH documentation\n. If the backup target is a local filesystem there is no need to set up OpenSSH.\nConfigure rsnapshot\n¶\nThe\nrsnapshot\nconfiguration resides in\n/etc/rsnapshot.conf\n. Below you can find some of the options available there.\nThe root directory where all snapshots will be stored is found at:\nsnapshot_root\n/var/cache/rsnapshot/\nNumber of backups to keep\n¶\nSince\nrsnapshot\nuses incremental backups, we can afford to keep older backups for a while before removing them. You set these up under the\nBACKUP\nLEVELS\n/\nINTERVALS\nsection. You can tell\nrsnapshot\nto retain a specific number of backups of each kind of interval.\nretain\ndaily\n6\nretain\nweekly\n7\nretain\nmonthly\n4\nIn this example we will keep 6 snapshots of our daily strategy, 7 snapshots of our weekly strategy, and 4 snapshots of our monthly strategy. These data will guide the rotation made by\nrsnapshot\n.\nRemote machine access\n¶\nIf you are accessing a remote machine over SSH and the port to bind is not the default (port\n22\n), you need to set the following variable with the port number:\nssh_args\n-p\n22222\nWhat to backup\n¶\nNow the most important part; you need to decide what you would like to backup.\nIf you are backing up locally to the same machine, this is as easy as specifying the directories that you want to save and following it with\nlocalhost/\nwhich will be a sub-directory in the\nsnapshot_root\nthat you set up earlier.\nbackup\n/home/\nlocalhost/\nbackup\n/etc/\nlocalhost/\nbackup\n/usr/local/\nlocalhost/\nIf you are backing up a remote machine you just need to tell\nrsnapshot\nwhere the server is and which directories you would like to back up.\nbackup\nroot@example.com:/home/\nexample.com/\n+rsync_long_args\n=\n--bwlimit\n=\n16\n,exclude\n=\ncore\nbackup\nroot@example.com:/etc/\nexample.com/\nexclude\n=\nmtab,exclude\n=\ncore\nAs you can see, you can pass extra rsync parameters (the\n+\nappends the parameter to the default list – if you remove the\n+\nsign you override it) and also exclude directories.\nYou can check the comments in\n/etc/rsnapshot.conf\nand the\nrsnapshot(1)\nmanual page for more options.\nTest configuration\n¶\nAfter modifying the configuration file, it is good practice to check if the syntax is OK:\nsudo\nrsnapshot\nconfigtest\nYou can also test your backup levels with the following command:\nsudo\nrsnapshot\n-t\ndaily\nIf you are happy with the output and want to see it in action you can run:\nsudo\nrsnapshot\ndaily\nScheduling backups\n¶\nWith\nrsnapshot\nworking correctly with the current configuration, the only thing left to do is schedule it to run at certain intervals. We will use cron to make this happen since\nrsnapshot\nincludes a default cron file in\n/etc/cron.d/rsnapshot\n. If you open this file there are some entries commented out as reference.\n0 4  * * *           root    /usr/bin/rsnapshot daily\n0 3  * * 1           root    /usr/bin/rsnapshot weekly\n0 2  1 * *           root    /usr/bin/rsnapshot monthly\nThe settings above added to\n/etc/cron.d/rsnapshot\nrun:\nThe\ndaily snapshot\neveryday at 4:00 am\nThe\nweekly snapshot\nevery Monday at 3:00 am\nThe\nmonthly snapshot\non the first of every month at 2:00 am\nFor more information on how to schedule a backup using cron please take a look at the\nExecuting\nwith\ncron\nsection in\nBackups - Shell Scripts\n.\nFurther reading\n¶\nrsnapshot official web page\nrsnapshot(1)\nmanual page\nrsync(1)\nmanual page", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:14Z", "original_len_words": 703}}
{"id": "c0e20c2499", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/", "title": "Containers - Ubuntu Server documentation", "text": "Containers - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nContainers\n¶\nContainers are a lightweight, portable virtualization technology. They package software together with its dependencies so that applications can run consistently even across different environments.\nHow to use LXD\nDocker for sysadmins\nHow to run rocks on your server\nSee also\n¶\nExplanation:\nVirtualisation and containers", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:14Z", "original_len_words": 78}}
{"id": "7a72012fd4", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/docker-for-system-admins/", "title": "Docker for system admins - Ubuntu Server documentation", "text": "Docker for system admins - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nDocker for system admins\n¶\nWe are going to explore set-ups for configuring storage, networking, and logging in the subsequent sections. This will also help you get familiarized with Docker command line interface (CLI).\nInstallation\n¶\nFirst, install Docker if it’s not already installed:\n$\nsudo\napt-get\ninstall\n-y\ndocker.io\ndocker-compose-v2\nConfiguring storage\n¶\nHow to configure volumes\n¶\nCreate a volume\n$\ndocker\nvolume\ncreate\nmy-vol\n\nmy-vol\nList volumes\n$\ndocker\nvolume\nls\n\nDRIVER\nVOLUME\nNAME\nlocal\nmy-vol\nInspect volume\n$\ndocker\nvolume\ninspect\nmy-vol\n[\n{\n\"CreatedAt\"\n:\n\"2023-10-25T00:53:24Z\"\n,\n\"Driver\"\n:\n\"local\"\n,\n\"Labels\"\n:\nnull\n,\n\"Mountpoint\"\n:\n\"/var/lib/docker/volumes/my-vol/_data\"\n,\n\"Name\"\n:\n\"my-vol\"\n,\n\"Options\"\n:\nnull\n,\n\"Scope\"\n:\n\"local\"\n}\n]\nRun a container and mount a volume\n$\ndocker\nrun\n--name\nweb-server\n-d\n\\\n--mount\nsource\n=\nmy-vol,target\n=\n/app\n\\\nubuntu/apache2\n\n0709c1b632801fddd767deddda0d273289ba423e9228cc1d77b2194989e0a882\nInspect your container to make sure the volume is mounted correctly:\ndocker\ninspect\nweb-server\n--format\n'{{ json .Mounts }}'\n|\njq\n.\n[\n{\n\"Type\"\n:\n\"volume\"\n,\n\"Name\"\n:\n\"my-vol\"\n,\n\"Source\"\n:\n\"/var/lib/docker/volumes/my-vol/_data\"\n,\n\"Destination\"\n:\n\"/app\"\n,\n\"Driver\"\n:\n\"local\"\n,\n\"Mode\"\n:\n\"z\"\n,\n\"RW\"\n:\ntrue\n,\n\"Propagation\"\n:\n\"\"\n}\n]\nBy default, all your volumes will be stored in\n/var/lib/docker/volumes\n.\nStop and remove the container, then remove its volume.\ndocker\nstop\nweb-server\ndocker\nrm\nweb-server\ndocker\nvolume\nrm\nmy-vol\nHow to configure bind mounts\n¶\nCreate a Docker container and bind mount your host directory:\n$\ndocker\nrun\n-d\n\\\n--name\nweb-server\n\\\n--mount\ntype\n=\nbind,source\n=\n\"\n$(\npwd\n)\n\"\n,target\n=\n/app\n\\\nubuntu/apache2\n\n6f5378e34d6c6811702e16d047a5a80f18adbd9d8a14b11050ae3c3353bf8d2a\nInspect your container to check for the bind mount:\ndocker\ninspect\nweb-server\n--format\n'{{ json .Mounts }}'\n|\njq\n.\n[\n{\n\"Type\"\n:\n\"bind\"\n,\n\"Source\"\n:\n\"/root\"\n,\n\"Destination\"\n:\n\"/app\"\n,\n\"Mode\"\n:\n\"\"\n,\n\"RW\"\n:\ntrue\n,\n\"Propagation\"\n:\n\"rprivate\"\n}\n]\nStop and remove the container\ndocker\nstop\nweb-server\ndocker\nrm\nweb-server\nHow to configure Tmpfs\n¶\nCreate a Docker container and mount a tmpfs:\n$\ndocker\nrun\n--name\nweb-server\n-d\n\\\n--mount\ntype\n=\ntmpfs,target\n=\n/app\n\\\nubuntu/apache2\n\n03483cc28166fc5c56317e4ee71904941ec5942071e7c936524f74d732b6a24c\nInspect your container to check for the tmpfs mount:\ndocker\ninspect\nweb-server\n--format\n'{{ json .Mounts }}'\n|\njq\n.\n[\n{\n\"Type\"\n:\n\"tmpfs\"\n,\n\"Source\"\n:\n\"\"\n,\n\"Destination\"\n:\n\"/app\"\n,\n\"Mode\"\n:\n\"\"\n,\n\"RW\"\n:\ntrue\n,\n\"Propagation\"\n:\n\"\"\n}\n]\nChoosing the right storage drivers\n¶\nBefore changing the configuration and restarting the daemon, make sure that the specified filesystem (zfs, btrfs, or device mapper) is mounted at\n/var/lib/docker\n.\nOtherwise, if you configure the Docker daemon to use a storage driver different from the filesystem mounted at\n/var/lib/docker\n, a failure will happen. The Docker daemon expects that\n/var/lib/docker\nis correctly set up when it starts.\nCheck the current storage driver\n$\ndocker\ninfo\n|\ngrep\n\"Storage Driver\"\nStorage\nDriver:\noverlay2\nEnsure the required Filesystem is available. We will be using the ZFS Filesystem.\n$\napt\ninstall\nzfsutils-linux\n-y\n# Install ZFS\n$\nfallocate\n-l\n5G\n/zfs-pool.img\n# Create a 5GB file\n$\nzpool\ncreate\nmypool\n/zfs-pool.img\n# Create a ZFS pool\n$\nzfs\ncreate\n-o\nmountpoint\n=\n/var/lib/docker\nmypool/docker\n# Create a ZFS dataset and mount it to dockers directory, \"/var/lib/docker\".\n$\nzfs\nlist\n# Verify that it mounted successfully\nNAME\nUSED\nAVAIL\nREFER\nMOUNTPOINT\nmypool\n162K\n4\n.36G\n24K\n/mypool\nmypool/docker\n39K\n4\n.36G\n39K\n/var/lib/docker\nChange the storage driver\nStop the docker daemon\nsystemctl\nstop\ndocker\nEdit\n/etc/docker/daemon.json\nusing your favorite editor, then update the storage driver value to\nzfs\n.\nvim\n/etc/docker/daemon.json\n{\n\"storage-driver\"\n:\n\"zfs\"\n}\nRestart the docker daemon\nsystemctl\nrestart\ndocker\nVerify the change\n$\ndocker\ninfo\n|\ngrep\n\"Storage Driver\"\nStorage\nDriver:\nzfs\nConfiguring networking\n¶\nThis is how you can create a user-defined network using the Docker CLI:\nCreate a network\n$\ndocker\nnetwork\ncreate\n--driver\nbridge\nmy-net\n\nD84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\nList networks\n$\ndocker\nnetwork\nls\n\nNETWORK\nID\nNAME\nDRIVER\nSCOPE\n1f55a8891c4a\nbridge\nbridge\nlocal\n9ca94be2c1a0\nhost\nhost\nlocal\nd84efaca11d6\nmy-net\nbridge\nlocal\n5d300e6a07b1\nnone\nnull\nlocal\nInspect the network we created\ndocker\nnetwork\ninspect\nmy-net\n[\n{\n\"Name\"\n:\n\"my-net\"\n,\n\"Id\"\n:\n\"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\"\n,\n\"Created\"\n:\n\"2023-10-25T22:18:52.972569338Z\"\n,\n\"Scope\"\n:\n\"local\"\n,\n\"Driver\"\n:\n\"bridge\"\n,\n\"EnableIPv6\"\n:\nfalse\n,\n\"IPAM\"\n:\n{\n\"Driver\"\n:\n\"default\"\n,\n\"Options\"\n:\n{},\n\"Config\"\n:\n[\n{\n\"Subnet\"\n:\n\"172.18.0.0/16\"\n,\n\"Gateway\"\n:\n\"172.18.0.1\"\n}\n]\n},\n\"Internal\"\n:\nfalse\n,\n\"Attachable\"\n:\nfalse\n,\n\"Ingress\"\n:\nfalse\n,\n\"ConfigFrom\"\n:\n{\n\"Network\"\n:\n\"\"\n},\n\"ConfigOnly\"\n:\nfalse\n,\n\"Containers\"\n:\n{},\n\"Options\"\n:\n{},\n\"Labels\"\n:\n{}\n}\n]\nContainers can connect to a defined network when they are created (via\ndocker\nrun\n) or at any time of its lifecycle.\nConnecting a new container to an existing network\n¶\n$\ndocker\nrun\n-d\n--name\nc1\n--network\nmy-net\nubuntu/apache2\n\nC7aa78f45ce3474a276ca3e64023177d5984b3df921aadf97e221da8a29a891e\nView the network connected to the container\ndocker\ninspect\nc1\n--format\n'{{ json .NetworkSettings }}'\n|\njq\n.\n{\n\"Bridge\"\n:\n\"\"\n,\n\"SandboxID\"\n:\n\"ee1cc10093fdfdf5d4a30c056cef47abbfa564e770272e1e5f681525fdd85555\"\n,\n\"HairpinMode\"\n:\nfalse\n,\n\"LinkLocalIPv6Address\"\n:\n\"\"\n,\n\"LinkLocalIPv6PrefixLen\"\n:\n0\n,\n\"Ports\"\n:\n{\n\"80/tcp\"\n:\nnull\n},\n\"SandboxKey\"\n:\n\"/var/run/docker/netns/ee1cc10093fd\"\n,\n\"SecondaryIPAddresses\"\n:\nnull\n,\n\"SecondaryIPv6Addresses\"\n:\nnull\n,\n\"EndpointID\"\n:\n\"\"\n,\n\"Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"IPAddress\"\n:\n\"\"\n,\n\"IPPrefixLen\"\n:\n0\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"MacAddress\"\n:\n\"\"\n,\n\"Networks\"\n:\n{\n\"my-net\"\n:\n{\n\"IPAMConfig\"\n:\nnull\n,\n\"Links\"\n:\nnull\n,\n\"Aliases\"\n:\n[\n\"c7aa78f45ce3\"\n],\n\"NetworkID\"\n:\n\"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\"\n,\n\"EndpointID\"\n:\n\"1cb76d44a484d302137bb4b042c8142db8e931e0c63f44175a1aa75ae8af9cb5\"\n,\n\"Gateway\"\n:\n\"172.18.0.1\"\n,\n\"IPAddress\"\n:\n\"172.18.0.2\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"MacAddress\"\n:\n\"02:42:ac:12:00:02\"\n,\n\"DriverOpts\"\n:\nnull\n}\n}\n}\nConnecting a running container to an existing network\n¶\nMake a running container connect to the existing network\nCreate the container\n$\ndocker\nrun\n-d\n--name\nc2\nubuntu/nginx\n\nFea22fbb6e3685eae28815f3ad8c8a655340ebcd6a0c13f3aad0b45d71a20935\nConnect the running container to the network and verify that it’s connected.\ndocker\nnetwork\nconnect\nmy-net\nc2\ndocker\ninspect\nc2\n--format\n'{{ json .NetworkSettings }}'\n|\njq\n.\n{\n\"Bridge\"\n:\n\"\"\n,\n\"SandboxID\"\n:\n\"82a7ea6efd679dffcc3e4392e0e5da61a8ccef33dd78eb5381c9792a4c01f366\"\n,\n\"HairpinMode\"\n:\nfalse\n,\n\"LinkLocalIPv6Address\"\n:\n\"\"\n,\n\"LinkLocalIPv6PrefixLen\"\n:\n0\n,\n\"Ports\"\n:\n{\n\"80/tcp\"\n:\nnull\n},\n\"SandboxKey\"\n:\n\"/var/run/docker/netns/82a7ea6efd67\"\n,\n\"SecondaryIPAddresses\"\n:\nnull\n,\n\"SecondaryIPv6Addresses\"\n:\nnull\n,\n\"EndpointID\"\n:\n\"490c15cf3bcb149dd8649e3ac96f71addd13f660b4ec826dc39e266184b3f65b\"\n,\n\"Gateway\"\n:\n\"172.17.0.1\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"IPAddress\"\n:\n\"172.17.0.3\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"MacAddress\"\n:\n\"02:42:ac:11:00:03\"\n,\n\"Networks\"\n:\n{\n\"bridge\"\n:\n{\n\"IPAMConfig\"\n:\nnull\n,\n\"Links\"\n:\nnull\n,\n\"Aliases\"\n:\nnull\n,\n\"NetworkID\"\n:\n\"1f55a8891c4a523a288aca8881dae0061f9586d5d91c69b3a74e1ef3ad1bfcf4\"\n,\n\"EndpointID\"\n:\n\"490c15cf3bcb149dd8649e3ac96f71addd13f660b4ec826dc39e266184b3f65b\"\n,\n\"Gateway\"\n:\n\"172.17.0.1\"\n,\n\"IPAddress\"\n:\n\"172.17.0.3\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"MacAddress\"\n:\n\"02:42:ac:11:00:03\"\n,\n\"DriverOpts\"\n:\nnull\n},\n\"my-net\"\n:\n{\n\"IPAMConfig\"\n:\n{},\n\"Links\"\n:\nnull\n,\n\"Aliases\"\n:\n[\n\"fea22fbb6e36\"\n],\n\"NetworkID\"\n:\n\"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\"\n,\n\"EndpointID\"\n:\n\"17856b7f6902db39ff6ab418f127d75d8da597fdb8af0a6798f35a94be0cb805\"\n,\n\"Gateway\"\n:\n\"172.18.0.1\"\n,\n\"IPAddress\"\n:\n\"172.18.0.3\"\n,\n\"IPPrefixLen\"\n:\n16\n,\n\"IPv6Gateway\"\n:\n\"\"\n,\n\"GlobalIPv6Address\"\n:\n\"\"\n,\n\"GlobalIPv6PrefixLen\"\n:\n0\n,\n\"MacAddress\"\n:\n\"02:42:ac:12:00:03\"\n,\n\"DriverOpts\"\n:\n{}\n}\n}\n}\nThe container c2 is connected to two networks\nbridge\nand\nmy-net\n.\nThe default network created by the Docker daemon is called\nbridge\nusing the\nbridge network driver\n.\nModifying the default network “bridge”\n¶\nA system administrator can modify this default networks IP address by editing\n/etc/docker/daemon.json\nand including the below into the JSON object\nvim\n/etc/docker/daemon.json\n{\n\"bip\"\n:\n\"192.168.1.1/24\"\n,\n\"fixed-cidr\"\n:\n\"192.168.1.0/25\"\n,\n\"fixed-cidr-v6\"\n:\n\"2001:db8::/64\"\n,\n\"mtu\"\n:\n1500\n,\n\"default-gateway\"\n:\n\"192.168.1.254\"\n,\n\"default-gateway-v6\"\n:\n\"2001:db8:abcd::89\"\n,\n\"dns\"\n:\n[\n\"10.20.1.2\"\n,\n\"10.20.1.3\"\n]\n}\nRestart the Docker daemon\nsystemctl\nrestart\ndocker\nVerify your changes\ndocker\nnetwork\ninspect\nbridge\nExposing a container port to the host\n¶\nAfter deciding how you are going to manage the network and selecting the most appropriate driver, there are some specific deployment details that a system administrator has to bear in mind when running containers.\nExposing ports of any system is always a concern, since it increases the surface for malicious attacks. For containers, we also need to be careful, analysing whether we really need to publish ports to the host. For instance, if the goal is to allow containers to access a specific port from another container, there is no need to publish any port to the host. This can be solved by connecting all the containers to the same network. You should publish ports of a container to the host only if you want to make it available to non-Docker workloads. When a container is created no port is published to the host, the option\n--publish\n(or\n-p\n) should be passed to\ndocker\nrun\nor\ndocker\ncreate\nlisting which port will be exposed and how.\nThe\n--publish\noption of Docker CLI accepts the following options:\nFirst, the host port that will be used to publish the container’s port. It can also contain the IP address of the host. For example,\n0.0.0.0:8080\n.\nSecond, the container’s port to be published. For example,\n80\n.\nThird (optional), the type of port that will be published which can be TCP or UDP. For example,\n80/tcp\nor\n80/udp\n.\nAn example of how to publish port\n80\nof a container to port\n8080\nof the host:\nCreate a container and expose it’s port to the host\n$\ndocker\nrun\n-d\n--name\nweb-server\n--publish\n8080\n:80\nubuntu/nginx\n\nf451aa1990db7d2c9b065c6158e2315997a56a764b36a846a19b1b96ce1f3910\nView the containers network settings\ndocker\ninspect\nweb-server\n--format\n'{{ json .NetworkSettings.Ports }}'\n|\njq\n.\n{\n\"80/tcp\"\n:\n[\n{\n\"HostIp\"\n:\n\"0.0.0.0\"\n,\n\"HostPort\"\n:\n\"8080\"\n},\n{\n\"HostIp\"\n:\n\"::\"\n,\n\"HostPort\"\n:\n\"8080\"\n}\n]\n}\nThe\nHostIp\nvalues are\n0.0.0.0\n(IPv4) and\n::\n(IPv6), and the service running in the container is accessible to everyone in the network (reaching the host), if you want to publish the port from the container and let the service be available just to the host you can use\n--publish\n127.0.0.1:8080:80\ninstead. The published port can be TCP or UDP and one can specify that passing\n--publish\n8080:80/tcp\nor\n--publish\n8080:80/udp\n.\nThe system administrator might also want to manually set the IP address or the\nhostname\nof the container. To achieve this, one can use the\n--ip\n(IPv4),\n--ip6\n(IPv6), and\n--hostname\noptions of the\ndocker\nnetwork\nconnect\ncommand to specify the desired values.\nAnother important aspect of networking with containers is the\nDNS\nservice. By default containers will use the DNS setting of the host, defined in\n/etc/resolv.conf\n. Therefore, if a container is created and connected to the default\nbridge\nnetwork it will get a copy of host’s\n/etc/resolv.conf\n. If the container is connected to a user-defined network, then it will use Docker’s embedded DNS server. The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host. In case the system administrator wants to configure the DNS service, the\ndocker\nrun\nand\ndocker\ncreate\ncommands have options to allow that, such as\n--dns\n(IP address of a DNS server) and\n--dns-opt\n(key-value pair representing a DNS option and its value). For more information, check the manpages of those commands.\nManaging logs\n¶\nThe default logging driver is called\njson\nfile\n, and the system administrator can change it to suite their needs.\nModifying the logging driver via the docker daemon file\n¶\nEdit the docker daemon file and update the logging driver\nvim\n/etc/docker/daemon.json\n{\n\"log-driver\"\n:\n\"journald\"\n}\nModifying the logging driver when creating a container\n¶\nAnother option is specifying the logging driver during container creation time:\nSpecify a log driver when executing a\ndocker\nrun\n$\ndocker\nrun\n-d\n--name\nweb-server\n--log-driver\n=\njournald\nubuntu/nginx\n\n1c08b667f32d8b834f0d9d6320721e07de5f22168cfc8a024d6e388daf486dfa\nVerify your configuration\ndocker\ninspect\nweb-server\n--format\n'{{ json .HostConfig.LogConfig }}'\n|\njq\n.\n{\n\"Type\"\n:\n\"journald\"\n,\n\"Config\"\n:\n{}\n}\nView logs\n$\ndocker\nlogs\nweb-server\n\n/docker-entrypoint.sh:\n/docker-entrypoint.d/\nis\nnot\nempty,\nwill\nattempt\nto\nperform\nconfiguration\n/docker-entrypoint.sh:\nLooking\nfor\nshell\nscripts\nin\n/docker-entrypoint.d/\n/docker-entrypoint.sh:\nLaunching\n/docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh:\nConfiguration\ncomplete\n;\nready\nfor\nstart\nup\nDepending on the driver you might also want to pass some options. You can do that via the CLI, passing\n--log-opt\nor in the daemon config file adding the key\nlog-opts\n. For more information check the\nlogging driver documentation\n.\nDocker CLI also provides the\ndocker\nlogs\nand\ndocker\nservice\nlogs\ncommands which allows one to check for the logs produced by a given container or service (set of containers) in the host. However, those two commands are functional only if the logging driver for the containers is\njson-file\n,\nlocal\nor\njournald\n. They are useful for debugging in general, but there is the downside of increasing the storage needed in the host.\nThe remote logging drivers are useful to store data in an external service/host, and they also avoid spending more disk space in the host to store log files. Nonetheless, sometimes, for debugging purposes, it is important to have log files locally. Considering that, Docker has a feature called “dual logging”, which is enabled by default, and even if the system administrator configures a logging driver different from\njson-file\n,\nlocal\nand\njournald\n, the logs will be available locally to be accessed via the Docker CLI. If this is not the desired behavior, the feature can be disabled in the\n/etc/docker/daemon.json\nfile:\n{\n\"log-driver\"\n:\n\"syslog\"\n,\n\"log-opts\"\n:\n{\n\"cache-disabled\"\n:\n\"true\"\n,\n\"syslog-address\"\n:\n\"udp://1.2.3.4:1111\"\n}\n}\nThe option\ncache-disabled\nis used to disable the “dual logging” feature. If you try to run\ndocker\nlogs\nwith that configuration you will get the following error:\n$\ndocker\nlogs\nweb-server\n\nError\nresponse\nfrom\ndaemon:\nconfigured\nlogging\ndriver\ndoes\nnot\nsupport\nreading\nResources\n¶\nTo read an explanatory guide to Docker storage, networking, and logging see:\nDocker storage, networking, and logging", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 2358}}
{"id": "6a7cce1a14", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/lxd-containers/", "title": "LXD containers - Ubuntu Server documentation", "text": "LXD containers - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nLXD containers\n¶\nLXD\n(pronounced lex-dee) is a modern, secure, and powerful system container and virtual machine manager.\nIt provides a unified experience for running and managing full Linux systems inside containers or virtual machines. You can access it via the command line, its\nbuilt-in graphical user interface\n, or a set of powerful\nREST APIs\n.\nLXD scales from one instance on a single machine\nto a cluster\nin a full data center rack, making it suitable for both development and production workloads. You can even use LXD to set up a small, scalable private cloud,\nsuch as a MicroCloud\n.\nThis document will focus on how to configure and administer LXD on Ubuntu systems using the command line. On Ubuntu Server Cloud images, LXD comes pre-installed.\nOnline resources\n¶\nYou can visit\nthe official LXD documentation\n, or get in touch with the LXD team in their\nUbuntu Discourse forum\n. The team also maintains a\nYouTube channel\nwith helpful videos.\nInstallation\n¶\nLXD is pre-installed on Ubuntu Server cloud images. On other systems, the\nlxd\npackage can be installed using:\nsudo\nsnap\ninstall\nlxd\nThis will install the self-contained LXD snap package.\nKernel preparation\n¶\nIn general, Ubuntu should have all the desired features enabled by default. One exception to this is that in order to enable swap accounting, the boot argument\nswapaccount=1\nmust be set. This can be done by appending it to the\nGRUB_CMDLINE_LINUX_DEFAULT=\nvariable in /etc/default/grub, then running ‘update-grub’ as root and rebooting.\nConfiguration\n¶\nIn order to use LXD, some basic settings need to be configured first. This is done by running\nlxd\ninit\n, which will allow you to choose:\nDirectory or\nZFS\ncontainer backend. If you choose ZFS, you can choose which block devices to use, or the size of a file to use as backing store.\nAvailability over the network.\nA ‘trust password’ used by remote clients to vouch for their client certificate.\nYou must run ‘lxd init’ as root. ‘lxc’ commands can be run as any user who is a member of group lxd. If user joe is not a member of group ‘lxd’, you may run:\nadduser\njoe\nlxd\nas root to change it. The new membership will take effect on the next login, or after running\nnewgrp\nlxd\nfrom an existing login.\nSee\nHow to initialize LXD\nin the LXD documentation for more information on the configuration settings. Also, refer to the definitive configuration provided with the source code for the server, container, profile, and device configuration.\nCreating your first container\n¶\nThis section will describe the simplest container tasks.\nCreating a container\n¶\nEvery new container is created based on either an image, an existing container, or a container snapshot. At install time, LXD is configured with the following image servers:\nubuntu\n: this serves official Ubuntu cloud image releases.\nubuntu-daily\n: this serves official Ubuntu cloud images of the daily development releases.\nubuntu-minimal\n: this serves official Ubuntu Minimal cloud image releases.\nimages\n: this server provides unofficial images for a variety of Linux distributions. This is not the recommended server for Ubuntu images.\nThe command to create and start a container is\nlxc\nlaunch\nremote\n:\nimage\ncontainername\nImages are identified by their hash, but are also aliased. The\nubuntu\nremote knows many aliases such as\n18.04\nand\nbionic\n. A list of all images available from the Ubuntu Server can be seen using:\nlxc\nimage\nlist\nubuntu\n:\nTo see more information about a particular image, including all the aliases it is known by, you can use:\nlxc\nimage\ninfo\nubuntu\n:\nbionic\nYou can generally refer to an Ubuntu image using the release name (\nbionic\n) or the release number (\n18.04\n). In addition,\nlts\nis an alias for the latest supported LTS release. To choose a different architecture, you can specify the desired architecture:\nlxc\nimage\ninfo\nubuntu\n:\nlts\n/\narm64\nNow, let’s start our first container:\nlxc\nlaunch\nubuntu\n:\nbionic\nb1\nThis will download the official current Bionic cloud image for your current architecture, then create a container named\nb1\nusing that image, and finally start it. Once the command returns, you can see it using:\nlxc\nlist\nlxc\ninfo\nb1\nand open a shell in it using:\nlxc\nexec\nb1\n--\nbash\nA convenient alias for the command above is:\nlxc\nshell\nb1\nThe try-it page mentioned above gives a full synopsis of the commands you can use to administer containers.\nNow that the\nbionic\nimage has been downloaded, it will be kept in sync until no new containers have been created based on it for (by default) 10 days. After that, it will be deleted.\nLXD server configuration\n¶\nBy default, LXD is socket activated and configured to listen only on a local UNIX socket. While LXD may not be running when you first look at the process listing, any LXC command will start it up. For instance:\nlxc\nlist\nThis will create your client certificate and contact the LXD server for a list of containers. To make the server accessible over the network you can set the http port using:\nlxc\nconfig\nset\ncore\n.\nhttps_address\n:\n8443\nThis will tell LXD to listen to port 8443 on all addresses.\nAuthentication\n¶\nBy default, LXD will allow all members of group\nlxd\nto talk to it over the UNIX socket. Communication over the network is authorized using server and client certificates.\nBefore client\nc1\nwishes to use remote\nr1\n,\nr1\nmust be registered using:\nlxc\nremote\nadd\nr1\nr1\n.\nexample\n.\ncom\n:\n8443\nThe fingerprint of r1’s certificate will be shown, to allow the user at c1 to reject a false certificate. The server in turn will verify that c1 may be trusted in one of two ways. The first is to register it in advance from any already-registered client, using:\nlxc\nconfig\ntrust\nadd\nr1\ncertfile\n.\ncrt\nNow when the client adds r1 as a known remote, it will not need to provide a password as it is already trusted by the server.\nThe other step is to configure a ‘trust password’ with\nr1\n, either at initial configuration using\nlxd\ninit\n, or after the fact using:\nlxc\nconfig\nset\ncore\n.\ntrust_password\nPASSWORD\nThe password can then be provided when the client registers\nr1\nas a known remote.\nBacking store\n¶\nLXD supports several backing stores. The recommended and the default backing store is\nzfs\n. If you already have a ZFS pool configured, you can tell LXD to use it during the\nlxd\ninit\nprocedure, otherwise a file-backed zpool will be created automatically. With ZFS, launching a new container is fast because the\nfilesystem\nstarts as a copy on write clone of the images’ filesystem. Note that unless the container is privileged (see below) LXD will need to change ownership of all files before the container can start, however this is fast and change very little of the actual filesystem data.\nThe other supported backing stores are described in detail in the\nStorage configuration\nsection of the LXD documentation.\nContainer configuration\n¶\nContainers are configured according to a set of profiles, described in the next section, and a set of container-specific configuration. Profiles are applied first, so that container specific configuration can override profile configuration.\nContainer configuration includes properties like the architecture, limits on resources such as CPU and RAM, security details including apparmor restriction overrides, and devices to apply to the container.\nDevices can be of several types, including UNIX character, UNIX block, network interface, or disk. In order to insert a host mount into a container, a ‘disk’ device type would be used. For instance, to mount\n/opt\nin container\nc1\nat\n/opt\n, you could use:\nlxc\nconfig\ndevice\nadd\nc1\nopt\ndisk\nsource\n=/\nopt\npath\n=\nopt\nSee:\nlxc\nhelp\nconfig\nfor more information about editing container configurations. You may also use:\nlxc\nconfig\nedit\nc1\nto edit the whole of\nc1\n’s configuration. Comments at the top of the configuration will show examples of correct syntax to help administrators hit the ground running. If the edited configuration is not valid when the editor is exited, then the editor will be restarted.\nProfiles\n¶\nProfiles are named collections of configurations which may be applied to more than one container. For instance, all containers created with\nlxc\nlaunch\n, by default, include the\ndefault\nprofile, which provides a network interface\neth0\n.\nTo mask a device which would be inherited from a profile but which should not be in the final container, define a device by the same name but of type ‘none’:\nlxc\nconfig\ndevice\nadd\nc1\neth1\nnone\nNesting\n¶\nContainers all share the same host kernel. This means that there is always an inherent trade-off between features exposed to the container and host security from malicious containers. Containers by default are therefore restricted from features needed to nest child containers. In order to run lxc or lxd containers under a lxd container, the\nsecurity.nesting\nfeature must be set to true:\nlxc\nconfig\nset\ncontainer1\nsecurity\n.\nnesting\ntrue\nOnce this is done,\ncontainer1\nwill be able to start sub-containers.\nIn order to run unprivileged (the default in LXD) containers nested under an unprivileged container, you will need to ensure a wide enough UID mapping. Please see the ‘UID mapping’ section below.\nLimits\n¶\nLXD supports flexible constraints on the resources which containers can consume. The limits come in the following categories:\nCPU: limit cpu available to the container in several ways.\nDisk: configure the priority of I/O requests under load\nRAM: configure memory and swap availability\nNetwork: configure the network priority under load\nProcesses: limit the number of concurrent processes in the container.\nFor a full list of limits known to LXD, see\nthe configuration documentation\n.\nUID mappings and privileged containers\n¶\nBy default, LXD creates unprivileged containers. This means that root in the container is a non-root UID on the host. It is privileged against the resources owned by the container, but unprivileged with respect to the host, making root in a container roughly equivalent to an unprivileged user on the host. (The main exception is the increased attack surface exposed through the system call interface)\nBriefly, in an unprivileged container, 65536 UIDs are ‘shifted’ into the container. For instance, UID 0 in the container may be 100000 on the host, UID 1 in the container is 100001, etc, up to 165535. The starting value for UIDs and\nGIDs\n, respectively, is determined by the ‘root’ entry the\n/etc/subuid\nand\n/etc/subgid\nfiles. (See the\nsubuid(5)\n) manual page.)\nIt is possible to request a container to run without a UID mapping by setting the\nsecurity.privileged\nflag to true:\nlxc\nconfig\nset\nc1\nsecurity\n.\nprivileged\ntrue\nNote however that in this case the root user in the container is the root user on the host.\nApparmor\n¶\nLXD confines containers by default with an apparmor profile which protects containers from each other and the host from containers. For instance this will prevent root in one container from signaling root in another container, even though they have the same uid mapping. It also prevents writing to dangerous, un-namespaced files such as many sysctls and\n/proc/sysrq-trigger\n.\nIf the apparmor policy for a container needs to be modified for a container\nc1\n, specific apparmor policy lines can be added in the\nraw.apparmor\nconfiguration key.\nSeccomp\n¶\nAll containers are confined by a default seccomp policy. This policy prevents some dangerous actions such as forced umounts, kernel module loading and unloading, kexec, and the\nopen_by_handle_at\nsystem call. The seccomp configuration cannot be modified, however a completely different seccomp policy – or none – can be requested using\nraw.lxc\n(see below).\nRaw LXC configuration\n¶\nLXD configures containers for the best balance of host safety and container usability. Whenever possible it is highly recommended to use the defaults, and use the LXD configuration keys to request LXD to modify as needed. Sometimes, however, it may be necessary to talk to the underlying lxc driver itself. This can be done by specifying LXC configuration items in the ‘raw.lxc’ LXD configuration key. These must be valid items as documented in the\nlxc.container.conf(5)\nmanual page.\nSnapshots\n¶\nContainers can be renamed and live-migrated using the\nlxc\nmove\ncommand:\nlxc\nmove\nc1\nfinal\n-\nbeta\nThey can also be snapshotted:\nlxc\nsnapshot\nc1\nYYYY\n-\nMM\n-\nDD\nLater changes to c1 can then be reverted by restoring the snapshot:\nlxc\nrestore\nu1\nYYYY\n-\nMM\n-\nDD\nNew containers can also be created by copying a container or snapshot:\nlxc\ncopy\nu1\n/\nYYYY\n-\nMM\n-\nDD\ntestcontainer\nPublishing images\n¶\nWhen a container or container snapshot is ready for consumption by others, it can be published as a new image using;\nlxc\npublish\nu1\n/\nYYYY\n-\nMM\n-\nDD\n--\nalias\nfoo\n-\n2.0\nThe published image will be private by default, meaning that LXD will not allow clients without a trusted certificate to see them. If the image is safe for public viewing (i.e. contains no private information), then the ‘public’ flag can be set, either at publish time using\nlxc\npublish\nu1\n/\nYYYY\n-\nMM\n-\nDD\n--\nalias\nfoo\n-\n2.0\npublic\n=\ntrue\nor after the fact using\nlxc\nimage\nedit\nfoo\n-\n2.0\nand changing the value of the public field.\nImage export and import\n¶\nImage can be exported as, and imported from, tarballs:\nlxc\nimage\nexport\nfoo\n-\n2.0\nfoo\n-\n2.0\n.\ntar\n.\ngz\nlxc\nimage\nimport\nfoo\n-\n2.0\n.\ntar\n.\ngz\n--\nalias\nfoo\n-\n2.0\n--\npublic\nTroubleshooting\n¶\nTo view debug information about LXD itself, on a systemd based host use\njournalctl\n-\nu\nlxd\nContainer logfiles for container c1 may be seen using:\nlxc\ninfo\nc1\n--\nshow\n-\nlog\nThe configuration file which was used may be found under\n/var/log/lxd/c1/lxc.conf\nwhile apparmor profiles can be found in\n/var/lib/lxd/security/apparmor/profiles/c1\nand seccomp profiles in\n/var/lib/lxd/security/seccomp/c1\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 2374}}
{"id": "a30bba0977", "source_url": "https://documentation.ubuntu.com/server/how-to/containers/run-rocks-on-your-server/", "title": "How to run rocks on your server - Ubuntu Server documentation", "text": "How to run rocks on your server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to run rocks on your server\n¶\nDeploying rocks with Docker\n¶\nAs with any other OCI-compliant container image,\nrocks\ncan be deployed with your favorite container management tool. This section depicts a typical deployment workflow for a generic Grafana rock, using\nDocker\n.\nFirst, install Docker if it’s not already installed:\n$\nsudo\napt-get\ninstall\n-y\ndocker.io\ndocker-compose-v2\nWe can deploy a container with the\ndocker\nrun\ncommand. This command has a number of\npossible parameters\n. The\n“Usage” section\nof the Grafana rock’s documentation has a table with an overview of parameters specific to the image.\n$ sudo docker run -d --name grafana-container -e TZ=UTC -p 3000:3000 ubuntu/grafana:10.3.3-22.04_stable\nUnable to find image 'ubuntu/grafana:10.3.3-22.04_stable' locally\n10.3.3-22.04_stable: Pulling from ubuntu/grafana\nbccd10f490ab: Already exists \n549078d9d057: Pull complete \n6ef870aa8500: Pull complete \n2b475da7ccbd: Pull complete \nDigest: sha256:df566ef90ecb14267a459081949ee7b6693fa573b97a7134a9a6722207275caa\nStatus: Downloaded newer image for ubuntu/grafana:10.3.3-22.04_stable\n356e623ef2c16bc7d810bddccad8d7980f9c633aefc3a88bc8761eac4e1b1c50\nIn this particular case, we’re using:\n-d\nto run the container in the background.\nWe are also specifying a well-defined name for the container, with the\n--name\nparameter.\nWith\n-e\nwe are setting the container’s timezone (\nTZ\n) environment variable to\nUTC\n.\nWe also use\n-p\nto map port 3000 of the container to 3000 on localhost.\nThe last parameter indicates the name of the rock, as listed in Docker Hub. Notice that the image tag we requested has the\n_stable\nsuffix to indicate the image’s risk. This is called a\nChannel Tag\nand it follows a similar convention to\nsnap “channels”\n.\nThis container, named\ngrafana-container\n, serves Grafana 10.3.3 in an Ubuntu 22.04 LTS environment and can be accessed via local port 3000. Load the website up in your local web browser:\nIf you don’t have Firefox handy,\ncurl\ncan be used instead:\n$\ncurl\n-s\nhttp://localhost:3000/login\n|\ngrep\n\"<title>\"\n<title>Grafana</title>\nNow that we’ve tested the deployment of the Grafana rock as a single container, let’s clean it up:\n$\nsudo\ndocker\nps\nCONTAINER\nID\nIMAGE\nCOMMAND\nCREATED\nSTATUS\nPORTS\nNAMES\n356e623ef2c1\nubuntu/grafana:10.3.3-22.04_stable\n\"/bin/pebble enter -…\"\n17\nminutes\nago\nUp\n17\nminutes\n0\n.0.0.0:3000->3000/tcp,\n:::3000->3000/tcp\ngrafana-container\nWe can stop and remove the container as follows:\n$\nsudo\ndocker\nstop\ngrafana-container\n$\nsudo\ndocker\nrm\ngrafana-container\nThe\nGrafana rock’s documentation\nwill also show you how to use Docker’s\n-v\nbind mounts to configure Grafana’s provisioning directory and data persistence.\nMulti-container deployment\n¶\nThe section above explained the use of a single container for running a single software instance, but one of the benefits of using rocks is the ability to easily create and architecturally organize (or “orchestrate”) them to operate together in a modular fashion.\nThis section will demonstrate use of\ndocker-compose\nto set up two container services that inter-operate to implement a trivial observability stack with the\nPrometheus\nand\nGrafana\nrocks.\nStart by creating a Prometheus configuration file called\nprometheus.yml\nwith the following contents:\nglobal\n:\nscrape_interval\n:\n1m\nscrape_configs\n:\n-\njob_name\n:\n'prometheus'\nscrape_interval\n:\n1m\nstatic_configs\n:\n-\ntargets\n:\n[\n'localhost:9090'\n]\nNote that this is a very simplistic example, where Prometheus only collects metrics about itself. You could expand the above configuration to tell Prometheus to scrape metrics from other sources.\nThen, create the Compose file\ndocker-compose.yml\nand define both services:\nservices\n:\ngrafana\n:\nimage\n:\nubuntu/grafana:10.3.3-22.04_stable\ncontainer_name\n:\ngrafana-container\nenvironment\n:\nTZ\n:\nUTC\nports\n:\n-\n\"3000:3000\"\nprometheus\n:\nimage\n:\nubuntu/prometheus:2.49.1-22.04_stable\ncontainer_name\n:\nprometheus-container\nenvironment\n:\nTZ\n:\nUTC\nports\n:\n-\n\"9090:9090\"\nvolumes\n:\n-\n./prometheus.yml:/etc/prometheus/prometheus.yml\nNote that the Prometheus configuration file is being given to the container via a Docker volume (of type “bind mount”). The above sample could also be improved to also use another volume for persisting data, and even a Grafana default configuration for the Prometheus datasource.\nSince we already installed Docker in the section above, all that is needed is to create and start the containers defined in this Compose file. This can be achieved with:\n$\nsudo\ndocker\ncompose\nup\n-d\n[\n+\n]\nRunning\n10\n/10\n✔\ngrafana\nPulled\n✔\nbccd10f490ab\nAlready\nexists\n✔\n549078d9d057\nPull\ncomplete\n✔\n6ef870aa8500\nPull\ncomplete\n✔\n2b475da7ccbd\nPull\ncomplete\n✔\nprometheus\nPulled\n✔\na8b1c5f80c2d\nAlready\nexists\n✔\nf021062473aa\nPull\ncomplete\n✔\n9c6122d12d1d\nPull\ncomplete\n✔\n274b56f68abe\nPull\ncomplete\n[\n+\n]\nRunning\n3\n/3\n✔\nNetwork\ncompose_default\nCreated\n✔\nContainer\nprometheus-container\nStarted\n✔\nContainer\ngrafana-container\nStarted\nAs before, the\n-d\nindicates that all containers in this stack should be started in the background. You can confirm they are live and running with:\n$\nsudo\ndocker\ncompose\nps\nNAME\nIMAGE\nCOMMAND\nSERVICE\nCREATED\nSTATUS\nPORTS\ngrafana-container\nubuntu/grafana:10.3.3-22.04_stable\n\"/bin/pebble enter -…\"\ngrafana\n3\nseconds\nago\nUp\n3\nseconds\n0\n.0.0.0:3000->3000/tcp,\n:::3000->3000/tcp\nprometheus-container\nubuntu/prometheus:2.49.1-22.04_stable\n\"/bin/pebble enter -…\"\nprometheus\n3\nseconds\nago\nUp\n3\nseconds\n0\n.0.0.0:9090->9090/tcp,\n:::9090->9090/tcp\nOpening\nhttp://localhost:3000\nwill give you the same Grafana login page as before:\nUse the default username\nadmin\nand password\nadmin\nto login:\nBy clicking on “Data Sources” you can then add Prometheus and provide the server URL\nhttp://prometheus:9090\n:\nThis URL works because Docker Compose ensures both containers are on the same Docker network and that they can be discovered via their service name.\nFinally, click on “Explore” from the Grafana menu, and select the\nprometheus\ndatasource. You can now query and visualize the Prometheus metrics. For example:\nNext Steps\n¶\nAs you can see,\ndocker-compose\nmakes it convenient to set up multi-container applications without needing to perform runtime changes to the containers. As you can imagine, this can permit building a more sophisticated management system to handle fail-over, load-balancing, scaling, upgrading old nodes, and monitoring status. But rather than needing to implement all of this directly on top of\ndocker-container\n, you may want to investigate Kubernetes-style cluster management software such as\nmicrok8s\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 985}}
{"id": "f00b9c1888", "source_url": "https://documentation.ubuntu.com/server/how-to/data-and-storage/", "title": "Data and storage - Ubuntu Server documentation", "text": "Data and storage - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nData and storage\n¶\nThe following sections provide details on various topics related to storing, managing and accessing data.\nData management\n¶\nOpenLDAP\nshows how to set up and configure OpenLDAP\nDatabases\nprovides details on two of the most common databases found in Ubuntu: MySQL and PostgreSQL\nStorage and backups\n¶\nStorage\nshows how to set up and manage Logical Volumes\nBackups and version control\npresents common options for backing up your data and your system\nSee also\n¶\nExplanation:\nData and storage", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:15Z", "original_len_words": 116}}
{"id": "87e0d07c71", "source_url": "https://documentation.ubuntu.com/server/how-to/databases/", "title": "Databases - Ubuntu Server documentation", "text": "Databases - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nDatabases\n¶\nUbuntu provides two popular database servers: MySQL and PostgreSQL. Both are popular choices with similar feature sets, and both are equally supported in Ubuntu.\nThese guides show you how to install and configure them.\nMySQL\nPostgreSQL\nSee also\n¶\nExplanation:\nIntroduction to databases", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 76}}
{"id": "95620926f1", "source_url": "https://documentation.ubuntu.com/server/how-to/databases/install-mysql/", "title": "Install and configure a MySQL server - Ubuntu Server documentation", "text": "Install and configure a MySQL server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure a MySQL server\n¶\nMySQL\nis a fast, multi-threaded, multi-user, and robust SQL database server. It is intended for mission-critical, heavy-load production systems and mass-deployed software.\nInstall MySQL\n¶\nTo install MySQL, run the following command from a terminal prompt:\nsudo\napt\ninstall\nmysql-server\nOnce the installation is complete, the MySQL server should be started automatically. You can quickly check its current status via systemd:\nsudo\nservice\nmysql\nstatus\nWhich should provide an output like the following:\n● mysql.service - MySQL Community Server\n   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\n   Active: active (running) since Tue 2019-10-08 14:37:38 PDT; 2 weeks 5 days ago\n Main PID: 2028 (mysqld)\n    Tasks: 28 (limit: 4915)\n   CGroup: /system.slice/mysql.service\n           └─2028 /usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid\n Oct 08 14:37:36 db.example.org systemd[1]: Starting MySQL Community Server...\nOct 08 14:37:38 db.example.org systemd[1]: Started MySQL Community Server.\nThe network status of the MySQL service can also be checked by running the\nss\ncommand at the terminal prompt:\nsudo\nss\n-tap\n|\ngrep\nmysql\nWhen you run this command, you should see something similar to the following:\nLISTEN    0         151              127.0.0.1:mysql             0.0.0.0:*       users:((\"mysqld\",pid=149190,fd=29))\nLISTEN    0         70                       *:33060                   *:*       users:((\"mysqld\",pid=149190,fd=32))\nIf the server is not running correctly, you can type the following command to start it:\nsudo\nservice\nmysql\nrestart\nA good starting point for troubleshooting problems is the systemd journal, which can be accessed from the terminal prompt with this command:\nsudo\njournalctl\n-u\nmysql\nConfigure MySQL\n¶\nYou can edit the files in\n/etc/mysql/\nto configure the basic settings – log file, port number, etc. For example, to configure MySQL to listen for connections from network hosts, in the file\n/etc/mysql/mysql.conf.d/mysqld.cnf\n, change the\nbind-address\ndirective to the server’s IP address:\nbind\n-\naddress\n=\n192.168.0.5\nNote\nReplace\n192.168.0.5\nwith the appropriate address, which can be determined via the\nip\naddress\nshow\ncommand.\nAfter making a configuration change, the MySQL daemon will need to be restarted with the following command:\nsudo\nsystemctl\nrestart\nmysql.service\nUser setup\n¶\nBy default,\nmysql-server\ninitially provides a\n'root'@'localhost'\nuser for managing the server locally. You can enter the MySQL command-line as this user by running:\nsudo mysql -u root\nNo password is required by MySQL as it authenticates with\nauth_socket\n.\nCreate a new user\n¶\nFrom the command-line, you can create additional MySQL users with different privileges using the\nCREATE\nUSER\ncommand. For authentication, the two main options are to use a password or use a socket like the root user.\nTo create a user authenticated with a password, you can use MySQL’s provided\ncaching_sha2_password\nplugin. It can be invoked in the following way, providing the password in plaintext:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH caching_sha2_password BY 'password';\nA random password can also be generated here with:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH caching_sha2_password BY RANDOM PASSWORD;\nMySQL’s upstream documentation\nprovides an overview of additional options when creating accounts with passwords.\nSocket-based authentication is used to allow a local system user to access an account without entering a password. Invoke this with:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH auth_socket;\nBy default, only the system user with the matching username can access this account. If you want the MySQL account username to differ from the system user username, then use the\nAS\noption:\nCREATE USER 'username'@'localhost' IDENTIFIED WITH auth_socket AS 'system-user-username';\nAdding user permissions\n¶\nA newly created user will require privilege updates to interact with databases in any way. These are provided by the\nGRANT\ncommand alongside specified roles or operations. For example, to give your user the ability to view table entries using the\nSELECT\noperation on all databases, run the following:\nGRANT SELECT on *.* TO 'username'@'localhost';\nDatabase engines\n¶\nWhilst the default configuration of MySQL provided by the Ubuntu packages is perfectly functional and performs well there are things you may wish to consider before you proceed.\nMySQL is designed to allow data to be stored in different ways. These methods are referred to as either database or storage engines. There are two main storage engines that you’ll be interested in:\nInnoDB\nand\nMyISAM\n. Storage engines are transparent to the end user. MySQL will handle things differently under the surface, but regardless of which storage engine is in use, you will interact with the database in the same way.\nEach engine has its own advantages and disadvantages.\nWhile it is possible (and may be advantageous) to mix and match database engines on a table level, doing so reduces the effectiveness of the performance tuning you can do as you’ll be splitting the resources between two engines instead of dedicating them to one.\nInnoDB\n¶\nAs of MySQL 5.5, InnoDB is the default engine, and is highly recommended over MyISAM unless you have specific needs for features unique to that engine.\nInnoDB is a more modern database engine, designed to be\nACID compliant\nwhich guarantees database transactions are processed reliably. To meet ACID compliance all transactions are journaled independently of the main tables. This allows for much more reliable data recovery as data consistency can be checked.\nWrite locking can occur on a row-level basis within a table. That means multiple updates can occur on a single table simultaneously. Data caching is also handled in memory within the database engine, allowing caching on a more efficient row-level basis rather than file block.\nMyISAM\n¶\nMyISAM is the older of the two. It can be faster than InnoDB under certain circumstances and favors a read-only workload. Some web applications have been tuned around MyISAM (though that’s not to imply that they will be slower under InnoDB).\nMyISAM also supports the\nFULLTEXT\nindex type, which allows very fast searches of large quantities of text data. However MyISAM is only capable of locking an entire table for writing. This means only one process can update a table at a time. As any application that uses the table scales this may prove to be a hindrance.\nIt also lacks journaling, which makes it harder for data to be recovered after a crash. The following link provides some points for consideration about using\nMyISAM on a production database\n.\nBackups\n¶\nMySQL databases should be backed up regularly. Backups can be accomplished through several methods, of which we’ll discuss three here.\nmysqldump\nis included with\nmysql-server\n. It is useful for backing up smaller databases, allows backups to be edited prior to a restore, and can be used for exporting to CSV and XML.\nMySQL Shell’s Dump Utility\nallows for backups of specific schema and tables, both to local files and remote secure servers. It is recommended for creating partial backups, and for integration with Python programs.\nPercona Xtrabackup\ncreates full backups with far greater performance than the former options. However, it lacks the ability to customize schema and tables. It is the recommended option for backing up large databases in a production environment.\nmysqldump\n¶\nmysqldump\nis a built-in tool that performs\nlogical backups\nfor MySQL.\nTo dump the data of a publicly available database on the local MySQL server into a file, run the following:\nmysqldump\n[\ndatabase\nname\n]\n>\ndump.sql\nFor restricted databases, specify a user with the proper permissions using\n-u\n:\nmysqldump\n-u\nroot\n[\ndatabase\nname\n]\n>\ndump.sql\nTo restore a database from the backup file, run the\nmysql\ncommand and pipe the file through stdin:\nmysql\n-u\nroot\n[\ndatabase\nname\n]\n<\ndump.sql\nSee the\nupstream documentation\nfor more information.\nMySQL Shell Dump Utility\n¶\nMySQL Shell, supported in Ubuntu 24.04 LTS and later, contains a set of utilities for dumping, backing up, and restoring MySQL data. It provides a programmatic option for logical backups with filtering options.\nTo install MySQL Shell, run the following:\nsudo\napt\ninstall\nmysql-shell\nRun the following to connect to the local MySQL server on Ubuntu with MySQL Shell in Python mode:\nmysqlsh\n--socket\n=\n/var/run/mysqld/mysqld.sock\n--no-password\n--python\nInitiate a local backup of all data in Python mode with:\nutil\n.\ndump_instance\n(\n\"/tmp/worlddump\"\n)\nDump a specific set of tables with\ndump_tables\n:\nutil\n.\ndump_tables\n(\n\"database name\"\n,\n[\n\"table 1\"\n,\n\"table 2\"\n],\n\"/tmp/tabledump\"\n)\nTo restore dumped data, use the\ndump loading utility\n.\nutil\n.\nload_dump\n(\n\"/tmp/worlddump\"\n)\nNote\nTo restore data from a local file,\nlocal_infile\nneeds to be enabled on the MySQL server. Activate this by accessing the server with the\nmysql\ncommand and entering\nSET\nGLOBAL\nlocal_infile=1;\n.\nSee the\nMySQL Shell dump documentation\nfor more information.\nPercona Xtrabackup\n¶\nAlso supported in Ubuntu 24.04 LTS and later, Percona Xtrabackup is a tool for creating\nphysical backups\n. It is similar to the commercial offering of\nMySQL Enterprise Backup\n.\nTo install Xtrabackup, run the following command from a terminal prompt:\nsudo\napt\ninstall\npercona-xtrabackup\nCreate a new backup with the\nxtrabackup\ncommand. This can be done while the server is running.\nxtrabackup\n--backup\n--target-dir\n=\n/tmp/worlddump\nTo restore from a backup, service will need to be interrupted. This can be achieved with the following:\nsudo\nsystemctl\nstop\nmysql\nxtrabackup\n--prepare\n--target-dir\n=\n/tmp/worlddump\nsudo\nrm\n-rf\n/var/lib/mysql\nsudo\nxtrabackup\n--copy-back\n--target-dir\n=\n/tmp/worlddump\n--datadir\n=\n/var/lib/mysql\nsudo\nchown\n-R\nmysql:mysql\n/var/lib/mysql\nsudo\nsystemctl\nstart\nmysql\nFor more information, see\nPercona’s upstream documentation\n.\nAdvanced configuration\n¶\nCreating a tuned configuration\n¶\nThere are a number of parameters that can be adjusted within MySQL’s configuration files. This will allow you to improve the server’s performance over time.\nMany parameters can be adjusted with the existing database, however some may affect the data layout and thus need more care to apply.\nFirst, if you have existing data, you will first need to carry out a\nmysqldump\nand reload:\nmysqldump\n--all-databases\n--routines\n-u\nroot\n-p\n>\n~/fulldump.sql\nThis will then prompt you for the root password before creating a copy of the data. It is advisable to make sure there are no other users or processes using the database while this takes place. Depending on how much data you’ve got in your database, this may take a while. You won’t see anything on the screen during the process.\nOnce the dump has been completed, shut down MySQL:\nsudo\nservice\nmysql\nstop\nIt’s also a good idea to backup the original configuration:\nsudo\nrsync\n-avz\n/etc/mysql\n/root/mysql-backup\nNext, make any desired configuration changes. Then, delete and re-initialize the database space and make sure ownership is correct before restarting MySQL:\nsudo\nrm\n-rf\n/var/lib/mysql/*\nsudo\nmysqld\n--initialize\nsudo\nchown\n-R\nmysql:\n/var/lib/mysql\nsudo\nservice\nmysql\nstart\nThe final step is re-importation of your data by piping your SQL commands to the database.\ncat\n~/fulldump.sql\n|\nmysql\nFor large data imports, the ‘Pipe Viewer’ utility can be useful to track import progress. Ignore any ETA times produced by\npv\n; they’re based on the average time taken to handle each row of the file, but the speed of inserting can vary wildly from row to row with\nmysqldumps\n:\nsudo\napt\ninstall\npv\npv\n~/fulldump.sql\n|\nmysql\nOnce this step is complete, you are good to go!\nNote\nThis is not necessary for all\nmy.cnf\nchanges. Most of the variables you can change to improve performance are adjustable even whilst the server is running. As with anything, make sure to have a good backup copy of your config files and data before making changes.\nMySQL Tuner\n¶\nMySQL Tuner\nis a Perl script that connects to a running MySQL instance and offers configuration suggestions for optimising the database for your workload. The longer the server has been running, the better the advice\nmysqltuner\ncan provide. In a production environment, consider waiting for at least 24 hours before running the tool. You can install\nmysqltuner\nwith the following command:\nsudo\napt\ninstall\nmysqltuner\nThen once it has been installed, simply run:\nmysqltuner\n– and wait for its final report.\nThe top section provides general information about the database server, and the bottom section provides tuning suggestions to alter in your\nmy.cnf\n. Most of these can be altered live on the server without restarting; look through the\nofficial MySQL documentation\nfor the relevant variables to change in production.\nThe following example is part of a report from a production database showing potential benefits from increasing the query cache:\n-------- Recommendations -----------------------------------------------------\nGeneral recommendations:\n    Run OPTIMIZE TABLE to defragment tables for better performance\n    Increase table_cache gradually to avoid file descriptor limits\nVariables to adjust:\n    key_buffer_size (> 1.4G)\n    query_cache_size (> 32M)\n    table_cache (> 64)\n    innodb_buffer_pool_size (>= 22G)\nObviously, performance optimisation strategies vary from application to application; what works best for WordPress might not be the best for Drupal or Joomla. Performance can depend on the types of queries, use of indexes, how efficient the database design is and so on.\nYou may find it useful to spend some time searching for database tuning tips based on the applications you’re using. Once you’ve reached the point of diminishing returns from database configuration adjustments, look to the application itself for improvements, or invest in more powerful hardware and/or scale up the database environment.\nFurther reading\n¶\nFull documentation is available in both online and offline formats from the\nMySQL Developers portal\nFor general SQL information see the O’Reilly books\nGetting Started with SQL: A Hands-On Approach for Beginners\nby Thomas Nield as an entry point and\nSQL in a Nutshell\nas a quick reference.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 2249}}
{"id": "60b68e8d2c", "source_url": "https://documentation.ubuntu.com/server/how-to/databases/install-postgresql/", "title": "Install and configure PostgreSQL - Ubuntu Server documentation", "text": "Install and configure PostgreSQL - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure PostgreSQL\n¶\nPostgreSQL\n(commonly referred to as “Postgres”) is an object-relational database system that has all the features of traditional commercial database systems, but with enhancements to be found in next-generation database management systems (DBMS).\nInstall PostgreSQL\n¶\nTo install PostgreSQL, run the following command in the command prompt:\nsudo\napt\ninstall\npostgresql\nThe database service is automatically configured with viable defaults, but can be customized based on your specific needs.\nConfigure PostgreSQL\n¶\nPostgreSQL supports multiple client authentication methods. In Ubuntu,\npeer\nis the default authentication method used for\nlocal\nconnections, while\nscram-sha-256\nis the default for\nhost\nconnections (this used to be\nmd5\nuntil Ubuntu 21.10). Please refer to the\nPostgreSQL Administrator’s Guide\nif you would like to configure alternatives like Kerberos.\nThe following discussion assumes that you wish to enable TCP/IP connections and use the\nscram-sha-256\nmethod for client authentication. PostgreSQL configuration files are stored in the\n/etc/postgresql/<version>/main\ndirectory. For example, if you install PostgreSQL 14, the configuration files are stored in the\n/etc/postgresql/14/main\ndirectory.\nTip\nTo configure\nIDENT\nauthentication, add entries to the\n/etc/postgresql/*/main/pg_ident.conf\nfile. There are detailed comments in the file to guide you.\nBy default, only connections from the local system are allowed. To enable all other computers to connect to your PostgreSQL server, edit the file\n/etc/postgresql/*/main/postgresql.conf\n. Locate the line:\n#listen_addresses = ‘localhost’\nand change it to\n*\n:\nlisten_addresses = '*'\nNote\nTo listen on all IPv4 interfaces, set\nlisten_addresses\nto ‘\n0.0.0.0\n’, while ‘\n::\n’ will listen on all IPv6 interfaces. ‘\n*\n’ will cause PostgreSQL to listen on all available network interfaces, both IPv4 and IPv6.\nFor details on other parameters, refer to the configuration file or to the\nPostgreSQL documentation\nfor information on how they can be edited.\nNow that we can connect to our PostgreSQL server, the next step is to set a password for the\npostgres\nuser. Run the following command at a terminal prompt to connect to the default PostgreSQL template database:\nsudo\n-u\npostgres\npsql\ntemplate1\nThe above command connects to PostgreSQL database\ntemplate1\nas user\npostgres\n. Once you connect to the PostgreSQL server, you will be at an SQL prompt. You can run the following SQL command at the\npsql\nprompt to configure the password for the user\npostgres\n:\nALTER\nUSER\npostgres\nwith\nencrypted\npassword\n'your_password'\n;\nAfter configuring the password, edit the file\n/etc/postgresql/*/main/pg_hba.conf\nto use\nscram-sha-256\nauthentication with the\npostgres\nuser, allowed for the\ntemplate1\ndatabase, from any system in the local network (which in the example is\n192.168.1.1/24\n) :\nhostssl template1       postgres        192.168.1.1/24        scram-sha-256\nNote\nThe config statement\nhostssl\nused here will reject TCP connections that would not use SSL. PostgreSQL in Ubuntu has the SSL feature built in and configured by default, so it works right away. On your PostgreSQL server this uses the certificate created by\nssl-cert\npackage which is great, but for production use you should consider updating that with a proper certificate from a recognized Certificate Authority (CA).\nFinally, you should restart the PostgreSQL service to initialize the new configuration. From a terminal prompt enter the following to restart PostgreSQL:\nsudo\nsystemctl\nrestart\npostgresql.service\nWarning\nThe above configuration is not complete by any means. Please refer to the\nPostgreSQL Administrator’s Guide\nto configure more parameters.\nYou can test server connections from other machines by using the PostgreSQL client as follows, replacing the domain name with your actual server domain name or IP address:\nsudo\napt\ninstall\npostgresql-client\npsql\n--host\nyour-servers-dns-or-ip\n--username\npostgres\n--password\n--dbname\ntemplate1\nStreaming replication\n¶\nPostgreSQL has a nice feature called\nstreaming replication\nwhich provides the ability to continuously ship and apply the Write-Ahead Log\n(WAL) XLOG\nrecords to some number of standby servers to keep them current. Here is a simple way to replicate a PostgreSQL server (main) to a standby server.\nFirst, create a replication user in the main server, to be used from the standby server:\nsudo\n-u\npostgres\ncreateuser\n--replication\n-P\n-e\nreplicator\nLet’s configure the main server to turn on the streaming replication. Open the file\n/etc/postgresql/*/main/postgresql.conf\nand make sure you have the following lines:\nlisten_addresses = '*'\nwal_level = replica\nAlso edit the file\n/etc/postgresql/*/main/pg_hba.conf\nto add an extra line to allow the standby server connection for replication (that is a special keyword) using the\nreplicator\nuser:\nhost  replication   replicator   <IP address of the standby>      scram-sha-256\nRestart the service to apply changes:\nsudo\nsystemctl\nrestart\npostgresql\nNow, in the standby server, let’s stop the PostgreSQL service:\nsudo\nsystemctl\nstop\npostgresql\nEdit the\n/etc/postgresql/*/main/postgresql.conf\nto set up hot standby:\nhot_standby = on\nBack up the current state of the main server (those commands are still issued on the standby system):\nsudo\nsu\n-\npostgres\n# backup the current content of the standby server (update the version of your postgres accordingly)\ncp\n-R\n/var/lib/postgresql/14/main\n/var/lib/postgresql/14/main_bak\n# remove all the files in the data directory\nrm\n-rf\n/var/lib/postgresql/14/main/*\npg_basebackup\n-h\n<IP\naddress\nof\nthe\nmain\nserver>\n-D\n/var/lib/postgresql/14/main\n-U\nreplicator\n-P\n-v\n-R\nAfter this, a full single pass will have been completed, copying the content of the main database onto the local system being the standby. In the\npg_basebackup\ncommand the flags represent the following:\n-h\n: The\nhostname\nor IP address of the main server\n-D\n: The data directory\n-U\n: The user to be used in the operation\n-P\n: Turns on progress reporting\n-v\n: Enables verbose mode\n-R\n: Creates a\nstandby.signal\nfile and appends connection settings to\npostgresql.auto.conf\nFinally, let’s start the PostgreSQL service on standby server:\nsudo\nsystemctl\nstart\npostgresql\nTo make sure it is working, go to the main server and run the following command:\nsudo\n-u\npostgres\npsql\n-c\n\"select * from pg_stat_replication;\"\nAs mentioned, this is a very simple introduction, there are way more great details in the upstream documentation about the configuration of\nreplication\nas well as further\nHigh Availability, Load Balancing, and Replication\n.\nTo test the replication you can now create a test database in the main server and check if it is replicated in the standby server:\nsudo\n-u\npostgres\ncreatedb\ntest\n# on the main server\nsudo\n-u\npostgres\npsql\n-c\n\"\\l\"\n# on the standby server\nYou need to be able to see the\ntest\ndatabase, that was created on the main server, in the standby server.\nBackups\n¶\nPostgreSQL databases should be backed up regularly. Refer to the\nPostgreSQL Administrator’s Guide\nfor different approaches.\nFurther reading\n¶\nAs mentioned above, the\nPostgreSQL Administrator’s Guide\nis an excellent resource. The guide is also available in the\npostgresql-doc\npackage. Execute the following in a terminal to install the package:\nsudo\napt\ninstall\npostgresql-doc\nThis package provides further manpages on PostgreSQL\ndblink\nand “server programming interface” as well as the upstream HTML guide. To view the guide enter\nxdg-open\n/usr/share/doc/postgresql-doc-*/html/index.html\nor point your browser at it.\nFor general SQL information see the O’Reilly books\nGetting Started with SQL: A Hands-On Approach for Beginners\nby Thomas Nield as an entry point and\nSQL in a Nutshell\nas a quick reference.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 1207}}
{"id": "f251cdb5f0", "source_url": "https://documentation.ubuntu.com/server/how-to/graphics/", "title": "Graphics - Ubuntu Server documentation", "text": "Graphics - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nGraphics\n¶\nOn-system GPU\n¶\nInstall NVIDIA drivers\nVirtual GPU\n¶\nA virtual GPU (vGPU) partitions a physical GPU to enable GPU-accelerated workloads in virtualized environments.\nVirtual GPU (vGPU) with QEMU/KVM", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:16Z", "original_len_words": 62}}
{"id": "2e3f27ab49", "source_url": "https://documentation.ubuntu.com/server/how-to/graphics/gpu-virtualization-with-qemu-kvm/", "title": "GPU virtualisation with QEMU/KVM - Ubuntu Server documentation", "text": "GPU virtualisation with QEMU/KVM - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nGPU virtualisation with QEMU/KVM\n¶\nGraphics\n¶\nGraphics for QEMU/KVM always comes in two pieces: a\nfrontend\nand a backend.\nfrontend\n: Controlled via the\n-vga\nargument, which is provided to the guest. Usually one of\ncirrus\n,\nstd\n,\nqxl\n, or\nvirtio\n. The default these days is\nqxl\nwhich strikes a good balance between guest compatibility and performance. The guest needs a driver for whichever option is selected – this is the most common reason to not use the default (e.g., on very old Windows versions).\nbackend\n: Controlled via the\n-display\nargument. This is what the host uses to actually display the graphical content, which can be an application window via\ngtk\nor a\nvnc\n.\nIn addition, one can enable the\n-spice\nback end (which can be done in addition to\nvnc\n). This can be faster and provides more authentication methods than\nvnc\n.\nIf you want no graphical output at all, you can save some memory and CPU cycles by setting\n-nographic\n.\nIf you run with\nspice\nor\nvnc\nyou can use native\nvnc\ntools or virtualization-focused tools like\nvirt-viewer\n. You can read more about these in the\nlibvirt section\n.\nAll these options  are considered basic usage of graphics, but there are also advanced options for more specific use-cases. Those cases usually differ in their\nease-of-use and capability\n, such as:\nNeed 3D acceleration\n: Use\n-vga\nvirtio\nwith a local display having a\nGL\ncontext\n-display\ngtk,gl=on\n. This will use\nvirgil3d\non the host, and guest drivers are needed (which are common in Linux since\nKernels >= 4.4\nbut can be hard to come by for other cases). While not as fast as the next two options, the major benefit is that it can be used without additional hardware and without a proper input-output memory management unit (IOMMU)\nset up for device passthrough\n.\nNeed native performance\n: Use PCI passthrough of additional\nGPUs\nin the system. You’ll need an\nIOMMU\nset up, and you’ll need to unbind the cards from the host before you can pass it through, like so:\n-device\nvfio-pci,host\n=\n05\n:00.0,bus\n=\n1\n,addr\n=\n00\n.0,multifunction\n=\non,x-vga\n=\non\n-device\nvfio-pci,host\n=\n05\n:00.1,bus\n=\n1\n,addr\n=\n00\n.1\nNeed native performance, but multiple guests per card\n: Like with PCI passthrough, but using mediated devices to shard a card on the host into multiple devices, then passing those:\n-display\ngtk,gl\n=\non\n-device\nvfio-pci,sysfsdev\n=\n/sys/bus/pci/devices/0000:00:02.0/4dd511f6-ec08-11e8-b839-2f163ddee3b3,display\n=\non,rombar\n=\n0\nYou can read more\nabout vGPU at kraxel\nand\nUbuntu GPU mdev evaluation\n. The sharding of the cards is driver-specific and therefore will differ per manufacturer –\nIntel\n,\nNvidia\n, or\nAMD\n.\nThe advanced cases in particular can get pretty complex – it is recommended to use QEMU through\nlibvirt section\nfor those cases. libvirt will take care of all but the host kernel/BIOS tasks of such configurations. Below are the common basic actions needed for faster options (i.e., passthrough and mediated devices passthrough).\nThe initial step for both options is the same; you want to ensure your system has its IOMMU enabled and the device to pass should be in a group of its own. Enabling the VT-d and IOMMU is usually a BIOS action and thereby manufacturer dependent.\nPreparing the input-output memory management unit (IOMMU)\n¶\nOn the kernel side, there are various\noptions you can enable/configure\nfor the\nIOMMU feature\n. In recent Ubuntu kernels (>=5.4 => Focal or Bionic-HWE kernels) everything usually works by default, unless your hardware setup makes you need any of those tuning options.\nNote\nThe card used in all examples below e.g. when filtering for or assigning PCI IDs, is an NVIDIA V100 on PCI ID 41.00.0\n$\nlspci\n|\ngrep\n3D\n41\n:00.0\n3D\ncontroller:\nNVIDIA\nCorporation\nGV100GL\n[\nTesla\nV100\nPCIe\n16GB\n]\n(\nrev\na1\n)\nYou can check your boot-up kernel messages for IOMMU/\nDMAR\nmessages or even filter it for a particular PCI ID.\nTo list all:\ndmesg\n|\ngrep\n-i\n-e\nDMAR\n-e\nIOMMU\nWhich produces an output like this:\n[    3.509232] iommu: Default domain type: Translated\n...\n[    4.516995] pci 0000:00:01.0: Adding to iommu group 0\n...\n[    4.702729] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank).\nTo filter for the installed 3D card:\ndmesg\n|\ngrep\n-i\n-e\nDMAR\n-e\nIOMMU\n|\ngrep\n$(\nlspci\n|\nawk\n'/ 3D / {print $1}'\n)\nWhich shows the following output:\n[    4.598150] pci 0000:41:00.0: Adding to iommu group 66\nIf you have a particular device and want to check for its group you can do that via\nsysfs\n. If you have multiple cards or want the full list you can traverse the same\nsysfs\npaths for that.\nFor example, to find the group for our example card:\nfind\n/sys/kernel/iommu_groups/\n-name\n\"*\n$(\nlspci\n|\nawk\n'/ 3D / {print $1}'\n)\n*\"\nWhich it tells us is found here:\n/sys/kernel/iommu_groups/66/devices/0000:41:00.0\nWe can also check if there are other devices in this group:\nll\n/\nsys\n/\nkernel\n/\niommu_groups\n/\n66\n/\ndevices\n/\nlrwxrwxrwx\n1\nroot\nroot\n0\nJan\n3\n06\n:\n57\n0000\n:\n41\n:\n00.0\n->\n../../../../\ndevices\n/\npci0000\n:\n40\n/\n0000\n:\n40\n:\n03.1\n/\n0000\n:\n41\n:\n00.0\n/\nAnother useful tool for this stage (although the details are beyond the scope of this article) can be\nvirsh\nnode*\n, especially\nvirsh\nnodedev-list\n--tree\nand\nvirsh\nnodedev-dumpxml\n<pcidev>\n.\nNote\nSome older or non-server boards tend to group devices in one IOMMU group, which isn’t very useful as it means you’ll need to pass “all or none of them” to the same guest.\nPreparations for PCI and mediated devices pass-through – block host drivers\n¶\nFor both, you’ll want to ensure the normal driver isn’t loaded. In some cases you can do that at runtime via\nvirsh\nnodedev-detach\n<pcidevice>\n.\nlibvirt\nwill even do that automatically if, on the passthrough configuration, you have set\n<hostdev\nmode='subsystem'\ntype='pci'\nmanaged='yes'>\n.\nThis usually works fine for e.g. network cards, but some other devices like GPUs do not like to be unassigned, so there the required step usually is block loading the drivers you do not want to be loaded. In our GPU example the\nnouveau\ndriver would load and that has to be blocked. To do so you can create a\nmodprobe\nblacklist.\necho\n\"blacklist nouveau\"\n|\nsudo\ntee\n/etc/modprobe.d/blacklist-nouveau.conf\necho\n\"options nouveau modeset=0\"\n|\nsudo\ntee\n-a\n/etc/modprobe.d/blacklist-nouveau.conf\nsudo\nupdate-initramfs\n-u\nsudo\nreboot\nYou can check which kernel modules are loaded and available via\nlspci\n-v\n:\nlspci\n-v\n|\ngrep\n-A\n10\n\" 3D \"\nWhich in our example shows:\n41:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] (rev a1)\n...\nKernel modules: nvidiafb, nouveau\nIf the configuration did not work instead it would show:\nKernel driver in use: nouveau\nPreparations for mediated devices pass-through - driver\n¶\nFor PCI passthrough, the above steps would be all the preparation needed, but for mediated devices one also needs to install and set up the host driver. The example here continues with our NVIDIA V100 which is\nsupported and available from Nvidia\n.\nThere is also an Nvidia document about the same steps available on\ninstallation and configuration of vGPU on Ubuntu\n.\nOnce you have the drivers from Nvidia, like\nnvidia-vgpu-ubuntu-470_470.68_amd64.deb\n, then install them and check (as above) that that driver is loaded. The one you need to see is\nnvidia_vgpu_vfio\n:\nlsmod\n|\ngrep\nnvidia\nWhich we can see in the output:\nnvidia_vgpu_vfio       53248  38\nnvidia              35282944  586 nvidia_vgpu_vfio\nmdev                   24576  2 vfio_mdev,nvidia_vgpu_vfio\ndrm                   491520  6 drm_kms_helper,drm_vram_helper,nvidia\nNote\nWhile it works without a vGPU manager, to get the full capabilities you’ll need to configure the\nvGPU manager (that came with above package)\nand a license server so that each guest can get a license for the vGPU provided to it. Please see\nNvidia’s documentation for the license server\n. While not officially supported on Linux (as of Q1 2022), it’s worthwhile to note that it runs fine on Ubuntu with\nsudo\napt\ninstall\nunzip\ndefault-jre\ntomcat9\nliblog4j2-java\nlibslf4j-java\nusing\n/var/lib/tomcat9\nas the server path in the license server installer.\nIt’s also worth mentioning that the Nvidia license server went\nEOL\non 31 July 2023\n. At that time, it was replaced by the\nNVIDIA License System\n.\nHere is an example of those when running fine:\n# general status\n$ systemctl status nvidia-vgpu-mgr\n     Loaded: loaded (/lib/systemd/system/nvidia-vgpu-mgr.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2021-09-14 07:30:19 UTC; 3min 58s ago\n    Process: 1559 ExecStart=/usr/bin/nvidia-vgpu-mgr (code=exited, status=0/SUCCESS)\n   Main PID: 1564 (nvidia-vgpu-mgr)\n      Tasks: 1 (limit: 309020)\n     Memory: 1.1M\n     CGroup: /system.slice/nvidia-vgpu-mgr.service\n             └─1564 /usr/bin/nvidia-vgpu-mgr\n\nSep 14 07:30:19 node-watt systemd[1]: Starting NVIDIA vGPU Manager Daemon...\nSep 14 07:30:19 node-watt systemd[1]: Started NVIDIA vGPU Manager Daemon.\nSep 14 07:30:20 node-watt nvidia-vgpu-mgr[1564]: notice: vmiop_env_log: nvidia-vgpu-mgr daemon started\n\n# Entries when a guest gets a vGPU passed\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): gpu-pci-id : 0x4100\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): vgpu_type : Quadro\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): Framebuffer: 0x1dc000000\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): Virtual Device Id: 0x1db4:0x1252\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): FRL Value: 60 FPS\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: ######## vGPU Manager Information: ########\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: Driver Version: 470.68\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): vGPU supported range: (0x70001, 0xb0001)\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): Init frame copy engine: syncing...\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): vGPU migration enabled\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: display_init inst: 0 successful\n\n# Entries when a guest grabs a license\nSep 15 06:55:50 node-watt nvidia-vgpu-mgr[4260]: notice: vmiop_log: (0x0): vGPU license state: Unlicensed (Unrestricted)\nSep 15 06:55:52 node-watt nvidia-vgpu-mgr[4260]: notice: vmiop_log: (0x0): vGPU license state: Licensed\n\n# In the guest the card is then fully recognized and enabled\n$ nvidia-smi -a | grep -A 2 \"Licensed Product\"\n    vGPU Software Licensed Product\n        Product Name                      : NVIDIA RTX Virtual Workstation\n        License Status                    : Licensed\nA\nmediated device\nis essentially the partitioning of a hardware device using\nfirmware\nand host driver features. This brings a lot of flexibility and options; in our example we can split our 16G GPU into 2x8G, 4x4G, 8x2G or 16x1G just as we need it. The following gives an example of how to split it into two 8G cards for a compute profile and pass those to guests.\nPlease refer to the\nNVIDIA documentation\nfor advanced tunings and different card profiles.\nThe tool for listing and configuring these mediated devices is\nmdevctl\n:\nsudo\nmdevctl\ntypes\nWhich will list the available types:\n...\n  nvidia-300\n    Available instances: 0\n    Device API: vfio-pci\n    Name: GRID V100-8C\n    Description: num_heads=1, frl_config=60, framebuffer=8192M, max_resolution=4096x2160, max_instance=2\nKnowing the PCI ID (\n0000:41:00.0\n) and the mediated device type we want (\nnvidia-300\n) we can now create those mediated devices:\n$ sudo mdevctl define --parent 0000:41:00.0 --type nvidia-300\nbc127e23-aaaa-4d06-a7aa-88db2dd538e0\n$ sudo mdevctl define --parent 0000:41:00.0 --type nvidia-300\n1360ce4b-2ed2-4f63-abb6-8cdb92100085\n$ sudo mdevctl start --parent 0000:41:00.0 --uuid bc127e23-aaaa-4d06-a7aa-88db2dd538e0\n$ sudo mdevctl start --parent 0000:41:00.0 --uuid 1360ce4b-2ed2-4f63-abb6-8cdb92100085\nAfter that, you can check the UUID of your ready mediated devices:\n$ sudo mdevctl list -d\nbc127e23-aaaa-4d06-a7aa-88db2dd538e0 0000:41:00.0 nvidia-108 manual (active)\n1360ce4b-2ed2-4f63-abb6-8cdb92100085 0000:41:00.0 nvidia-108 manual (active)\nThose UUIDs can then be used to pass the mediated devices to the guest - which from here is rather similar to the pass through of a full PCI device.\nPassing through PCI or mediated devices\n¶\nAfter the above setup is ready one can pass through those devices, in\nlibvirt\nfor a PCI passthrough that looks like:\n<\nhostdev\nmode\n=\n'subsystem'\ntype\n=\n'pci'\nmanaged\n=\n'yes'\n>\n<\nsource\n>\n<\naddress\ndomain\n=\n'0x0000'\nbus\n=\n'0x41'\nslot\n=\n'0x00'\nfunction\n=\n'0x0'\n/>\n</\nsource\n>\n</\nhostdev\n>\nAnd for mediated devices it is quite similar, but using the UUID.\n<\nhostdev\nmode\n=\n'subsystem'\ntype\n=\n'mdev'\nmanaged\n=\n'no'\nmodel\n=\n'vfio-pci'\ndisplay\n=\n'on'\n>\n<\nsource\n>\n<\naddress\nuuid\n=\n'634fc146-50a3-4960-ac30-f09e5cedc674'\n/>\n</\nsource\n>\n</\nhostdev\n>\nThose sections can be\npart of the guest definition\nitself, to be added on guest startup and freed on guest shutdown. Or they can be in a file and used by for hot-add remove if the hardware device and its drivers support it\nvirsh\nattach-device\n.\nNote\nThis works great on Focal, but\ntype='none'\nas well as\ndisplay='off'\nweren’t available on Bionic. If this level of control is required one would need to consider using the\nUbuntu Cloud Archive\nor\nServer-Backports\nfor a newer stack of the virtualisation components.\nAnd finally, it might be worth noting that while mediated devices are becoming more common and known for vGPU handling, they are a general infrastructure also used (for example) for\ns390x vfio-ccw\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:17Z", "original_len_words": 2186}}
{"id": "d62c14a545", "source_url": "https://documentation.ubuntu.com/server/how-to/graphics/install-nvidia-drivers/", "title": "NVIDIA drivers installation - Ubuntu Server documentation", "text": "NVIDIA drivers installation - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nNVIDIA drivers installation\n¶\nThis page shows how to install the NVIDIA drivers from the command line, using either the\nubuntu-drivers\ntool (recommended), or APT.\nNVIDIA drivers releases\n¶\nWe package two types of NVIDIA drivers:\nUnified Driver Architecture (UDA)\ndrivers - which are recommended for the generic desktop use, and which you can also find\non the NVIDIA website\n.\nEnterprise Ready Drivers\n(\nERD\n) - which are recommended on servers and for computing tasks. Their packages can be recognized by the\n-server\nsuffix. You can read more about these drivers\nin the NVIDIA documentation\n.\nAdditionally, we package the\nNVIDIA Fabric Manager\nand the\nNVIDIA Switch Configuration and Query (NSCQ) Library\n, which you will only need if you have NVswitch hardware. The Fabric Manager and NSCQ library are only available with the ERDs or\n-server\ndriver versions.\nCheck driver versions\n¶\nTo check the version of your currently running driver:\ncat\n/proc/driver/nvidia/version\nThe recommended way (ubuntu-drivers tool)\n¶\nThe\nubuntu-drivers\ntool relies on the same logic as the “Additional Drivers” graphical tool, and allows more flexibility on desktops and on servers.\nThe\nubuntu-drivers\ntool is recommended if your computer uses Secure Boot, since it always tries to install signed drivers which are known to work with Secure Boot.\nCheck the available drivers for your hardware\n¶\nFor desktop:\nsudo\nubuntu-drivers\nlist\nor, for servers:\nsudo\nubuntu-drivers\nlist\n--gpgpu\nYou should see a list such as the following:\nnvidia-driver-470\nnvidia-driver-470-server\nnvidia-driver-535\nnvidia-driver-535-open\nnvidia-driver-535-server\nnvidia-driver-535-server-open\nnvidia-driver-550\nnvidia-driver-550-open\nnvidia-driver-550-server\nnvidia-driver-550-server-open\nInstalling the drivers for generic use (e.g. desktop and gaming)\n¶\nYou can either rely on automatic detection, which will install the driver that is considered the best match for your hardware:\nsudo\nubuntu-drivers\ninstall\nOr you can tell the\nubuntu-drivers\ntool which driver you would like installed. If this is the case, you will have to use the driver version (such as\n535\n) that you saw when you used the\nubuntu-drivers\nlist\ncommand.\nLet’s assume we want to install the\n535\ndriver:\nsudo\nubuntu-drivers\ninstall\nnvidia:535\nInstalling the drivers on servers and/or for computing purposes\n¶\nYou can either rely on automatic detection, which will install the driver that is considered the best match for your hardware:\nsudo\nubuntu-drivers\ninstall\n--gpgpu\nOr you can tell the\nubuntu-drivers\ntool which driver you would like installed. If this is the case, you will have to use the driver version (such as\n535\n) and the\n-server\nsuffix that you saw when you used the\nubuntu-drivers\nlist\n--gpgpu\ncommand.\nLet’s assume we want to install the\n535-server\ndriver (listed as\nnvidia-driver-535-server\n):\nsudo\nubuntu-drivers\ninstall\n--gpgpu\nnvidia:535-server\nYou will also want to install the following additional components:\nsudo\napt\ninstall\nnvidia-utils-535-server\nOptional step\n¶\nIf your system comes with NVswitch hardware, then you will want to install Fabric Manager and the NVSwitch Configuration and Query library. You can do so by running the following:\nsudo\napt\ninstall\nnvidia-fabricmanager-535\nlibnvidia-nscq-535\nNote\nWhile\nnvidia-fabricmanager\nand\nlibnvidia-nscq\ndo not have the same\n-server\nlabel in their name, they are really meant to match the\n-server\ndrivers in the Ubuntu archive. For example,\nnvidia-fabricmanager-535\nwill match the\nnvidia-driver-535-server\npackage version (not the\nnvidia-driver-535\npackage\n).\nManual driver installation (using APT)\n¶\nInstalling the NVIDIA driver manually means installing the correct kernel modules first, then installing the metapackage for the driver series.\nInstalling the kernel modules\n¶\nIf your system uses Secure Boot (as most x86 modern systems do), your kernel will require the kernel modules to be signed. There are two (mutually exclusive) ways to achieve this.\nInstalling the pre-compiled NVIDIA modules for your kernel\n¶\nInstall the metapackage for your kernel flavour (e.g.\ngeneric\n,\nlowlatency\n, etc) which is specific to the driver branch (e.g.\n535\n) that you want to install, and whether you want the compute vs. general display driver (e.g.\n-server\nor not):\nsudo\napt\ninstall\nlinux-modules-nvidia-\n${\nDRIVER_BRANCH\n}${\nSERVER\n}\n-\n${\nLINUX_FLAVOUR\n}\n(e.g.\nlinux-modules-nvidia-535-generic\n)\nCheck that the modules for your specific kernel/\nABI\nwere installed by the metapackage:\nsudo\napt-cache\npolicy\nlinux-modules-nvidia-\n${\nDRIVER_BRANCH\n}${\nSERVER\n}\n-\n$(\nuname\n-r\n)\n(e.g.\nsudo\napt-cache\npolicy\nlinux-modules-nvidia-535-$(uname\n-r)\n)\nIf the modules were not installed for your current running kernel, upgrade to the latest kernel or install them by specifying the running kernel version:\nsudo\napt\ninstall\nlinux-modules-nvidia-\n${\nDRIVER_BRANCH\n}${\nSERVER\n}\n-\n$(\nuname\n-r\n)\n(e.g.\nsudo\napt\ninstall\nlinux-modules-nvidia-535-$(uname\n-r)\n)\nBuilding your own kernel modules using the NVIDIA DKMS package\n¶\nInstall the relevant NVIDIA\nDKMS\npackage and\nlinux-headers\nto build the kernel modules, and enroll your own key to sign the modules.\nInstall the\nlinux-headers\nmetapackage for your kernel flavour (e.g.\ngeneric\n,\nlowlatency\n, etc):\nsudo\napt\ninstall\nlinux-headers-\n${\nLINUX_FLAVOUR\n}\nCheck that the headers for your specific kernel were installed by the metapackage:\nsudo\napt-cache\npolicy\nlinux-headers-\n$(\nuname\n-r\n)\nIf the headers for your current running kernel were not installed, install them by specifying the running kernel version:\nsudo\napt\ninstall\nlinux-headers-\n$(\nuname\n-r\n)\nFinally, install the NVIDIA DKMS package for your desired driver series (this may automatically guide you through creating and enrolling a new key for Secure Boot):\nsudo\napt\ninstall\nnvidia-dkms-\n${\nDRIVER_BRANCH\n}${\nSERVER\n}\nInstalling the user-space drivers and the driver libraries\n¶\nAfter installing the correct kernel modules (see the relevant section of this document), install the correct driver metapackage:\nsudo\napt\ninstall\nnvidia-driver-\n${\nDRIVER_BRANCH\n}${\nSERVER\n}\n(Optional) Installing Fabric Manager and the NSCQ library\n¶\nIf your system comes with NVswitch hardware, then you will want to install Fabric Manager and the NVSwitch Configuration and Query library. You can do so by running the following:\nsudo\napt\ninstall\nnvidia-fabricmanager-\n${\nDRIVER_BRANCH\n}\nlibnvidia-nscq-\n${\nDRIVER_BRANCH\n}\nNote\nWhile\nnvidia-fabricmanager\nand\nlibnvidia-nscq\ndo not have the same\n-server\nlabel in their name, they are really meant to match the\n-server\ndrivers in the Ubuntu archive. For example,\nnvidia-fabricmanager-535\nwill match the\nnvidia-driver-535-server\npackage version (not the\nnvidia-driver-535\npackage).\nSwitching between pre-compiled and DKMS modules\n¶\nUninstalling the NVIDIA drivers (below)\nManual driver installation using APT\nUninstalling the NVIDIA drivers\n¶\nRemove any NVIDIA packages from your system:\nsudo\napt\n--purge\nremove\n'*nvidia*${DRIVER_BRANCH}*'\nIf you are unsure which\n${DRIVER_BRANCH}\nto pick for removal you might look at the installed nvidia packages and see the different\n${DRIVER_BRANCH}\nnumbers that are present on your system.\nSince\nautoremove\nwill take care of all indirect dependencies it is sufficient to list those that have been directly installed by using\napt-mark\n.\napt-mark\nshowmanual\n|\ngrep\nnvidia\n`\n.\nRemove any additional packages that may have been installed as a dependency (e.g. the\ni386\nlibraries on amd64 systems) and which were not caught by the previous command:\nsudo\napt\nautoremove\nTransitional packages to new driver branches\n¶\nWhen NVIDIA stops support on a driver branch, then Canonical will transition you to the next supported driver branch automatically if you try to install that driver branch.\nSee NVIDIA’s\ncurrent support matrix\nin their documentation.\nTroubleshooting\n¶\nDriver/library version mismatch error\n¶\nIf you encounter the following error when running the\nnvidia-smi\ncommand:\nFailed to initialize NVML: Driver/library version mismatch\nThis typically indicates that the userspace driver packages were upgraded while the kernel module is still on the older version (for example, the client reports one driver version while the kernel module reports another). This situation often occurs after a system upgrade. To verify this, check the kernel logs:\nsudo dmesg\nLook for error messages similar to:\nNVRM: API mismatch: the client has the version 570.172.08, but\nNVRM: this kernel module has the version 570.158.01.  Please\nNVRM: make sure that this kernel module and all NVIDIA driver\nNVRM: components have the same version.\nSolution\n: Rebooting the system will load the updated kernel module and bring the versions back in sync.\nNo devices were found error\n¶\nIf you encounter the following error when running the\nnvidia-smi\ncommand:\nNo devices were found\nThis may occur if the open-source NVIDIA kernel driver\nnouveau\nis pre-installed and loaded, which conflicts with the proprietary NVIDIA driver. To check whether nouveau is loaded:\nlsmod | grep nouveau\nSolution\n: If nouveau kernel module is loaded, blacklist it and rebuild the initramfs:\necho \"blacklist nouveau\" | sudo tee /etc/modprobe.d/disable-nouveau.conf\necho \"options nouveau modeset=0\" | sudo tee -a /etc/modprobe.d/disable-nouveau.conf\nsudo rmmod nouveau || true\nsudo update-initramfs -u\nThen reboot the system for the changes to take effect.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:17Z", "original_len_words": 1443}}
{"id": "63a70fc702", "source_url": "https://documentation.ubuntu.com/server/how-to/high-availability/", "title": "High availability - Ubuntu Server documentation", "text": "High availability - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHigh availability\n¶\nDistributed Replicated Block Device (DRBD) mirrors block devices between multiple hosts. This guide shows how to install and configure a DRBD.\nInstall a Distributed Replicated Block Device\nSee also\n¶\nExplanation:\nHigh Availability\nReference:\nHigh availability", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:17Z", "original_len_words": 71}}
{"id": "58b4aad1bf", "source_url": "https://documentation.ubuntu.com/server/how-to/high-availability/install-drbd/", "title": "Distributed Replicated Block Device (DRBD) - Ubuntu Server documentation", "text": "Distributed Replicated Block Device (DRBD) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nDistributed Replicated Block Device (DRBD)\n¶\nDistributed Replicated Block Device (DRBD) mirrors block devices between multiple hosts. The replication is transparent to other applications on the host systems. Any block device hard disks, partitions, RAID devices, logical volumes, etc can be mirrored.\nInstall DRBD\n¶\nTo get started using DRBD, first install the necessary packages. In a terminal window, run the following command:\nsudo\napt\ninstall\ndrbd-utils\nNote\nIf you are using the\nvirtual kernel\nas part of a virtual machine you will need to manually compile the\ndrbd\nmodule. It may be easier to install the\nlinux-modules-extra-$(uname\n-r)\npackage inside the virtual machine.\nConfigure DRBD\n¶\nThis section covers setting up a DRBD to replicate a separate\n/srv\npartition, with an\next3\nfilesystem\nbetween two hosts. The partition size is not particularly relevant, but both partitions need to be the same size.\nThe two hosts in this example will be called\ndrbd01\nand\ndrbd02\n. They will need to have name resolution configured either through\nDNS\nor the\n/etc/hosts\nfile. See our\nguide to DNS\nfor details.\nOn the first host, edit\n/etc/drbd.conf\nas follows:\nglobal\n{\nusage\n-\ncount\nno\n;\n}\ncommon\n{\nsyncer\n{\nrate\n100\nM\n;\n}\n}\nresource\nr0\n{\nprotocol\nC\n;\nstartup\n{\nwfc\n-\ntimeout\n15\n;\ndegr\n-\nwfc\n-\ntimeout\n60\n;\n}\nnet\n{\ncram\n-\nhmac\n-\nalg\nsha1\n;\nshared\n-\nsecret\n\"secret\"\n;\n}\non\ndrbd01\n{\ndevice\n/\ndev\n/\ndrbd0\n;\ndisk\n/\ndev\n/\nsdb1\n;\naddress\n192.168.0.1\n:\n7788\n;\nmeta\n-\ndisk\ninternal\n;\n}\non\ndrbd02\n{\ndevice\n/\ndev\n/\ndrbd0\n;\ndisk\n/\ndev\n/\nsdb1\n;\naddress\n192.168.0.2\n:\n7788\n;\nmeta\n-\ndisk\ninternal\n;\n}\n}\nNote\nThere are many other options in\n/etc/drbd.conf\n, but for this example the default values are enough.\nNow copy\n/etc/drbd.conf\nto the second host:\nscp\n/etc/drbd.conf\ndrbd02:~\nAnd, on\ndrbd02\n, move the file to\n/etc\n:\nsudo\nmv\ndrbd.conf\n/etc/\nNow using the\ndrbdadm\nutility, initialize the meta data storage. On both servers, run:\nsudo\ndrbdadm\ncreate-md\nr0\nNext, on both hosts, start the\ndrbd\ndaemon:\nsudo\nsystemctl\nstart\ndrbd.service\nOn\ndrbd01\n(or whichever host you wish to be the primary), enter the following:\nsudo\ndrbdadm\n--\n--overwrite-data-of-peer\nprimary\nall\nAfter running the above command, the data will start syncing with the secondary host. To watch the progress, on\ndrbd02\nenter the following:\nwatch\n-n1\ncat\n/proc/drbd\nTo stop watching the output press\nCtrl\n+\nC\n.\nFinally, add a filesystem to\n/dev/drbd0\nand mount it:\nsudo\nmkfs.ext3\n/dev/drbd0\nsudo\nmount\n/dev/drbd0\n/srv\nTesting\n¶\nTo test that the data is actually syncing between the hosts copy some files on\ndrbd01\n, the primary, to\n/srv\n:\nsudo\ncp\n-r\n/etc/default\n/srv\nNext, unmount\n/srv\n:\nsudo\numount\n/srv\nNow demote the\nprimary\nserver to the\nsecondary\nrole:\nsudo\ndrbdadm\nsecondary\nr0\nNow on the\nsecondary\nserver, promote it to the\nprimary\nrole:\nsudo\ndrbdadm\nprimary\nr0\nLastly, mount the partition:\nsudo\nmount\n/dev/drbd0\n/srv\nUsing\nls\nyou should see\n/srv/default\ncopied from the former primary host\ndrbd01\n.\nFurther reading\n¶\nFor more information on DRBD see the\nDRBD web site\n.\nThe\ndrbd.conf(5)\nmanual page contains details on the options not covered in this guide.\nAlso, see the\ndrbdadm(8)\nmanpage.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:17Z", "original_len_words": 592}}
{"id": "a24f77d276", "source_url": "https://documentation.ubuntu.com/server/how-to/installation/", "title": "Server installation - Ubuntu Server documentation", "text": "Server installation - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nServer installation\n¶\nIf you are new to Ubuntu, we recommend our\nbasic installation\ntutorial to get you started.\nAutomatic install\n¶\nThe Ubuntu Installer has its own documentation for automatic (or “hands off”) installations. These guides from the Ubuntu Installer documentation are available for automatic installations.\nIntroduction to Automated Server installer\nAutoinstall quickstart\nAutoinstall quickstart on s390x\nAdvanced install\n¶\nThis list of guides contains installation instructions for architecture-specific and more advanced setups. Select your preferred architecture to see which guides are available.\namd64\nNetboot install\narm64\nNetboot install\nChoose between the arm64 and arm64+largemem installer options\nppc64el\nNetboot install\nVirtual CD-ROM and Petitboot install\ns390x\nInstall via z/VM\nNon-interactive IBM z/VM autoinstall\nInstall via LPAR\nNon-interactive IBM Z LPAR autoinstall\nSee also\n¶\nReference:\nSystem requirements", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:18Z", "original_len_words": 160}}
{"id": "1e3fce9955", "source_url": "https://documentation.ubuntu.com/server/how-to/mail-services/", "title": "Mail services - Ubuntu Server documentation", "text": "Mail services - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nMail services\n¶\nSending email from one person to another over a network or the Internet requires:\nThe sender’s email client (\nMail User Agent\n) sends the message.\nOne or more\nMail Transfer Agents\n(MTA) to transfer the message.\nThe final MTA sends the message to a\nMail Delivery Agent\n(MDA) for delivery to the recipient’s inbox.\nFinally, the recipient’s email client retrieves the message, usually via a\nPOP3\nor\nIMAP\nserver.\nThese systems must all be configured correctly to successfully deliver a message.\nMail User Agent\n¶\nThunderbird\nis the default Mail User Agent (email client) used by Ubuntu. It comes pre-installed on all Ubuntu machines from Ubuntu 16.04 LTS (Xenial) onwards.\nIf you need to install Thunderbird manually,\nthis short guide\nwill walk you through the steps.\nMail Transfer Agent\n¶\nOn Ubuntu,\nPostfix\nis the default supported MTA. It is compatible with the\nsendmail\nMTA.\nInstall Postfix\nexplains how to install and configure Postfix, including how to configure SMTP for secure communications.\nExim4\ncan be installed in place of sendmail, although its configuration is quite different.\nInstall Exim4\nexplains how to install and configure Exim4 on Ubuntu.\nMail Delivery Agent\n¶\nDovecot\nis an MDA written with security primarily in mind. It supports the\nmbox\nand\nMaildir\nmailbox formats.\nInstall Dovecot\nexplains how to set up Dovecot as an IMAP or POP3 server", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:18Z", "original_len_words": 259}}
{"id": "2372cd0da9", "source_url": "https://documentation.ubuntu.com/server/how-to/mail-services/install-dovecot/", "title": "Install and configure Dovecot - Ubuntu Server documentation", "text": "Install and configure Dovecot - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure Dovecot\n¶\nInstall Dovecot\n¶\nTo install a basic Dovecot server with common POP3 and IMAP functions, run the following command:\nsudo\napt\ninstall\ndovecot-imapd\ndovecot-pop3d\nThere are various other Dovecot modules including\ndovecot-sieve\n(mail filtering),\ndovecot-solr\n(full text search),\ndovecot-antispam\n(spam filter training),\ndovecot-ldap\n(user directory).\nConfigure Dovecot\n¶\nTo configure Dovecot, edit the file\n/etc/dovecot/dovecot.conf\nand its included config files in\n/etc/dovecot/conf.d/\n. By default, all installed protocols will be enabled via an\ninclude\ndirective in\n/etc/dovecot/dovecot.conf\n.\n!include_try /usr/share/dovecot/protocols.d/*.protocol\nIMAPS and POP3S are more secure because they use SSL encryption to connect. A basic self-signed SSL certificate is automatically set up by package\nssl-cert\nand used by Dovecot in\n/etc/dovecot/conf.d/10-ssl.conf\n.\nMbox\nformat is configured by default, but you can also use\nMaildir\nif required. More details can be found in the comments in\n/etc/dovecot/conf.d/10-mail.conf\n. Also see\nthe Dovecot web site\nto learn about further benefits and details.\nMake sure to also configure your chosen Mail Transport Agent (MTA) to transfer the incoming mail to the selected type of mailbox.\nRestart the Dovecot daemon\n¶\nOnce you have configured Dovecot, restart its daemon in order to test your setup using the following command:\nsudo\nservice\ndovecot\nrestart\nTry to log in with the commands\ntelnet\nlocalhost\npop3\n(for POP3) or\ntelnet\nlocalhost\nimap2\n(for IMAP).  You should see something like the following:\nbhuvan@rainbow:~$ telnet localhost pop3\nTrying 127.0.0.1...\nConnected to localhost.localdomain.\nEscape character is '^]'.\n+OK Dovecot ready.\nDovecot SSL configuration\n¶\nBy default, Dovecot is configured to use SSL automatically using the package\nssl-cert\nwhich provides a self signed certificate.\nYou can instead generate your own custom certificate for Dovecot using\nopenssh\n, for example:\nsudo\nopenssl\nreq\n-new\n-x509\n-days\n1000\n-nodes\n-out\n\"/etc/dovecot/dovecot.pem\"\n\\\n-keyout\n\"/etc/dovecot/private/dovecot.pem\"\nNext, edit\n/etc/dovecot/conf.d/10-ssl.conf\nand amend following lines to specify that Dovecot should use these custom certificates :\nssl_cert = </etc/dovecot/private/dovecot.pem\nssl_key = </etc/dovecot/private/dovecot.key\nYou can get the SSL certificate from a Certificate Issuing Authority or you can create self-signed one. Once you create the certificate, you will have a key file and a certificate file that you want to make known in the config shown above.\nSee also\nFor more details on creating custom certificates, see our guide on\nsecurity certificates\n.\nConfigure a firewall for an email server\n¶\nTo access your mail server from another computer, you must configure your firewall to allow connections to the server on the necessary ports.\nIMAP - 143\nIMAPS - 993\nPOP3 - 110\nPOP3S - 995\nFurther reading\n¶\nThe\nDovecot website\nhas more general information about Dovecot.\nThe\nDovecot manual\nprovides full documentation for Dovecot use.\nThe\nDovecot Ubuntu Wiki\npage has more details on configuration.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:18Z", "original_len_words": 486}}
{"id": "d6c36af75a", "source_url": "https://documentation.ubuntu.com/server/how-to/mail-services/install-exim4/", "title": "Install and configure Exim4 - Ubuntu Server documentation", "text": "Install and configure Exim4 - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure Exim4\n¶\nInstall Exim4\n¶\nTo install\nExim4\n, run the following command:\nsudo\napt\ninstall\nexim4\nConfigure Exim4\n¶\nTo configure Exim4, run the following command:\nsudo\ndpkg-reconfigure\nexim4-config\nThis displays a “wizard” user interface for configuring the software. One important question in this configuration is whether Exim4 should split the configuration over multiple files, or use a single configuration file.\nNote\nThe default configuration layout for Exim4 is the single configuration file one.\nIf using multiple configuration files, then the configuration will be split in a directory structure under\n/etc/exim4/conf.d\n, like so:\n/etc/exim4/\n└── conf.d\n    ├── acl\n    ├── auth\n    ├── main\n    ├── retry\n    ├── rewrite\n    ├── router\n    └── transport\nEach subdirectory contains one or more individual configuration files.\nIf, however, Exim4 was set up to use a single configuration file, then that file will be\n/etc/exim4/exim4.conf.template\n. It will essentially be as if all individual configuration files from the previous layout were concatenated into one file.\nIn any of these scenarios, after making a change to the configuration, the following command must be executed to update the actual configuration file that Exim4 will use:\nsudo update-exim4.conf\nThe\nupdate-exim4.conf\ncommand will update the autogenerated configuration file stored in\n/var/lib/exim4/config.autogenerated\n. This is the actual configuration file that Exim4 uses.\nWarning\nYou should never manually edit the configuration file\n/var/lib/exim4/config.autogenerated\n, because it is updated automatically every time you run\nupdate-exim4.conf\n. Any changes you make to it will eventually be lost.\nIf configuration changes were made, the service should also be restarted:\nsudo systemctl restart exim4\nAll the choices made via\ndpkg-reconfigure\nexim4-config\nare stored in the\n/etc/exim4/update-exim4.conf.conf\nfile. To re-configure the software you can either re-run\ndpkg-reconfigure\nas before, or manually edit this file using your preferred editor.\nStart the Exim4 daemon\n¶\nThe following command will start the Exim4 daemon:\nsudo\nservice\nexim4\nstart\nSMTP authentication\n¶\nThere are multiple authentication options available for Exim4. Here we will document two methods:\nAuthenticate Linux users present in the local shadow file (\n/etc/shadow\n), via\nsaslauthd\nand PAM.\nAuthenticate arbitrary users against a custom Exim4 password database (\n/etc/exim4/passwd\n).\nBoth of these methods use clear text passwords transmitted over the network, so they need to be protected by Transport Layer Security (TLS).\nWarning\nAll configuration steps shown from now on will assume a split-configuration mode for Exim4. If you have selected the non-split mode, then all commands that edit a configuration file under\n/etc/exim4/conf.d\nin the sections below should be replaced with editing the single file\n/etc/exim4/exim4.conf.template\n.\nEnabling TLS\n¶\nFirst, enter the following into a terminal prompt to create a certificate for use with TLS:\nsudo\n/usr/share/doc/exim4-base/examples/exim-gencert\nThis command will ask some questions about the certificate, like country, city, and others. The most important one, and that must be correct otherwise TLS won’t work for this server, is the “Server name” one. It\nMUST\nmatch the fully qualified hostname (FQDN) of the system where Exim4 is deployed.\nWarning\nThis will install a self-signed certificate. If deploying this system in production, you must get a proper certificate signed by a recognized Certificate Authority (CA), or, if using an internal, you will have to distribute the CA to all clients expected to connect to this server.\nConfigure Exim4 for TLS by editing the\n/etc/exim4/conf.d/main/03_exim4-config_tlsoptions\nfile and adding the following:\nMAIN_TLS_ENABLE = yes\nAuthenticating existing Linux users\n¶\nTo authenticate existing Linux users, that is, users who already have accounts on this system, we will use the\nsaslauthd\nservice.\nNote\nTo manage local Linux users, please refer to\nUser management\n.\nConfigure Exim4 to use the\nsaslauthd\ndaemon for authentication by editing\n/etc/exim4/conf.d/auth/30_exim4-config_examples\n– uncomment the\nplain_saslauthd_server\nand\nlogin_saslauthd_server\nsections:\nplain_saslauthd_server:\n  driver = plaintext\n  public_name = PLAIN\n  server_condition = ${if saslauthd{{$auth2}{$auth3}}{1}{0}}\n  server_set_id = $auth2\n  server_prompts = :\n  .ifndef AUTH_SERVER_ALLOW_NOTLS_PASSWORDS\n  server_advertise_condition = ${if eq{$tls_cipher}{}{}{*}}\n  .endif\n\nlogin_saslauthd_server:\n  driver = plaintext\n  public_name = LOGIN\n  server_prompts = \"Username:: : Password::\"\n  # don't send system passwords over unencrypted connections\n  server_condition = ${if saslauthd{{$auth1}{$auth2}}{1}{0}}\n  server_set_id = $auth1\n  .ifndef AUTH_SERVER_ALLOW_NOTLS_PASSWORDS\n  server_advertise_condition = ${if eq{$tls_cipher}{}{}{*}}\n  .endif\nThis enables the\nPLAIN\nand\nLOGIN\nauthentication mechanisms via\nsaslauthd\n.\nFor Ubuntu 22.04 and earlier, of it you plan to use authentication mechanisms that will need read access to\n/etc/sasldb2\n(not covered in this guide), you need to add the\nDebian-exim\nuser to the\nsasl\ngroup:\nsudo gpasswd -a Debian-exim sasl\nTo make all these changes effective, the main configuration file needs to be updated, and Exim4 restarted:\nsudo update-exim4.conf\nsudo systemctl restart exim4\nThis concludes the Exim4 side of the configuration. Next, the\nsasl2-bin\npackage needs to be installed:\nsudo apt install sasl2-bin\nThe main configuration for\nsaslauthd\nis in the\n/etc/default/saslauthd\nfile. What needs to be verified is the\nMECHANISMS\nsetting, which we want to be\nPAM\n:\nMECHANISMS\n=\n\"pam\"\nNote\nIn Ubuntu 22.04 Jammy and earlier, we also need to add\nSTART=\"yes\"\nto\n/etc/default/saslauthd\n.\nFinally, enable and start the\nsaslauthd\nservice:\nsudo systemctl enable saslauthd\nsudo systemctl start saslauthd\nExim4 is now configured with SMTP-AUTH using TLS authenticating local Linux users via PAM.\nAuthenticating arbitrary users\n¶\nExim4 can also be configured to authenticate arbitrary users, that is, users that do note exist on the local system. These mechanisms are called\nplain_server\nand\nlogin_server\n. Edit\n/etc/exim4/conf.d/auth/30_exim4-config_examples\nand uncomment these sections:\nplain_server:\n  driver = plaintext\n  public_name = PLAIN\n  server_condition = \"${if crypteq{$auth3}{${extract{1}{:}{${lookup{$auth2}lsearch{CONFDIR/passwd}{$value}{*:*}}}}}{1}{0}}\"\n  server_set_id = $auth2\n  server_prompts = :\n  .ifndef AUTH_SERVER_ALLOW_NOTLS_PASSWORDS\n  server_advertise_condition = ${if eq{$tls_in_cipher}{}{}{*}}\n  .endif\n\nlogin_server:\n  driver = plaintext\n  public_name = LOGIN\n  server_prompts = \"Username:: : Password::\"\n  server_condition = \"${if crypteq{$auth2}{${extract{1}{:}{${lookup{$auth1}lsearch{CONFDIR/passwd}{$value}{*:*}}}}}{1}{0}}\"\n  server_set_id = $auth1\n  .ifndef AUTH_SERVER_ALLOW_NOTLS_PASSWORDS\n  server_advertise_condition = ${if eq{$tls_in_cipher}{}{}{*}}\n  .endif\nWarning\nDO NOT enable both these and the\n_saslauthd_server\nvariants (from “Authenticating existing Linux users” above) at the same time!\nThese mechanisms will lookup usernames and passwords in the\n/etc/exim4/passwd\nfile, which has to be created and populated. The format of this file is:\nusername:crypted-password:cleartext-password\nThe Exim4 installation ships a helper script that can populate this file. It is a simple interactive script that can be run like this:\nsudo /usr/share/doc/exim4-base/examples/exim-adduser\nIt will prompt for a username and password. In this example we are creating an\nubuntu\nentry with the password\nubuntusecret\n:\nUser: ubuntu\nPassword: ubuntusecret\nAfter that, we will have a\n/etc/exim4/passwd\nfile, owned by\nroot:root\nand mode\n0644\n, with contents similar to this:\nubuntu:$1$ZvPA$HTddFobmJD1vURtJHBmbw/:ubuntusecret\nSince this file contains secrets, it should be protected, and Exim4 has to be allowed to read it:\nsudo chown root:Debian-exim /etc/exim4/passwd\nsudo chmod 0640 /etc/exim4/passwd\nThe same script can also be used to manage users in this\npasswd\nfile:\nTo change the password of an existing user, edit the\npasswd\nfile, delete the line corresponding to the user, save the file, and run the script again to provide the new password.\nTo add another user, run the script and provide the new user name, and their password.\nTo remove a user, edit the file with a text editor and delete the line corresponding to the user that should be removed.\nWarning\nThe\n/usr/share/doc/exim4-base/examples/exim-adduser\nserves mostly as an example and is not able to handle many scenarios. For example, it won’t check if the username you are providing already exists in the\npasswd\nfile, which can lead to multiple entries for the same user, with unpredictable results.\nFinally, update the Exim4 configuration and restart the service:\nsudo\nupdate-exim4.conf\nsudo\nsystemctl\nrestart\nexim4\nNote\nThere is no need to restart Exim4 after making changes to the\n/etc/exim4/passwd\nfile.\nTroubleshooting\n¶\nExim4 has logs in its own directory in\n/var/log/exim4/mainlog\n. Whenever troubleshooting the service, always look at that log file.\nA quick test to verify if\nsaslauthd\nis working can be performed with the\ntestsaslauthd\ncommand. Assuming you have a local user called\nubuntu\nwith a password of\nubuntusecret\n, this command can be used to test the authentication on the Exim4 server:\ntestsaslauthd -u ubuntu -p ubuntusecret\nThe result should be OK:\n0: OK \"Success.\"\nNote that this tests only the\nsaslauthd\nservice, not the Exim4 integration with it. For that we need to actually connect to the SMTP service and try out the authentication. A good helper tool for this is shipped in the\ncyrus-clients\npackage. Since this is part of another email system, it’s best to install it on another machine, and not on the same machine as Exim4.\nsudo apt install cyrus-clients --no-install-recommends\nHere we are using the extra\n--no-install-recommends\noption because we don’t need all the other components of the Cyrus email system.\nThe tool we are interested in is called\nsmtptest\n, and its documentation can be inspected in its manual page at\ncyrus-smtptest(1)\n.\nFor our purposes, we will run it like this, assuming an\nubuntu\nuser with the\nubuntusecret\npassword, and that the Exim4 server is running on the\nn-exim.lxd\nsystem:\n/usr/lib/cyrus/bin/smtptest -t \"\" -a ubuntu -w ubuntusecret n-exim.lxd\nThe command-line parameters are:\n-t\n\"\"\n: Enable TLS.\n-a\nubuntu\n: Use\nubuntu\nas the authenticating user.\n-w\nubuntusecret\n: Authenticate using the\nubuntusecret\npassword.\nn-exim.lxd\n: The hostname of the Exim4 server to connect to.\nIf all works well, the output will be similar to this, showing that the connection was switched to TLS, and the authentication worked:\nS: 220 n-exim ESMTP Exim 4.97 Ubuntu Mon, 23 Jun 2025 21:11:59 +0000\nC: EHLO smtptest\nS: 250-n-exim Hello n-exim.lxd [10.10.17.9]\nS: 250-SIZE 52428800\nS: 250-8BITMIME\nS: 250-PIPELINING\nS: 250-PIPECONNECT\nS: 250-CHUNKING\nS: 250-STARTTLS\nS: 250-PRDR\nS: 250 HELP\nC: STARTTLS\nS: 220 TLS go ahead\nverify error:num=18:self-signed certificate\nTLS connection established: TLSv1.3 with cipher TLS_AES_256_GCM_SHA384 (256/256 bits)\nC: EHLO smtptest\nS: 250-n-exim Hello n-exim.lxd [10.10.17.9]\nS: 250-SIZE 52428800\nS: 250-8BITMIME\nS: 250-PIPELINING\nS: 250-PIPECONNECT\nS: 250-AUTH PLAIN LOGIN\nS: 250-CHUNKING\nS: 250-PRDR\nS: 250 HELP\nC: AUTH LOGIN\nS: 334 VXNlcm5hbWU6\nC: dWJ1bnR1\nS: 334 UGFzc3dvcmQ6\nC: dWJ1bnR1c2VjcmV0\nS: 235 Authentication succeeded\nAuthenticated.\nSecurity strength factor: 256\nIt will appear to hang at this point, but it’s just waiting for the SMTP commands, i.e., receive an email. You can exit by typing\nQUIT\nfollowed by pressing enter.\nInteresting points to note in the output above:\nNo authentication was offered before the connection was switched to TLS. That’s because the only mechanisms which are configured are plain-text ones. Without TLS, the password would be exposed on the network.\nSince this documentation used a self-signed certificate, that was highlighted right before the TLS session was established. A real email client would probably abort the connnection at this point.\nAfter TLS was established, the\nLOGIN\nmechanism was chosen.\nThe username and password are sent base64 encoded. Do not mistake that for encryption: this is just an encoding mechanism!\nTip\nWant to obtain the original username and password back from the base64 encoded values? Feed those values to the\nbase64\n-d\ntool. Example, using the value from the session above:\n$ echo -n dWJ1bnR1c2VjcmV0 | base64 -d; echo\nubuntusecret\nTo test the\nPLAIN\nmechanism, add the\n-m\nplain\ncommand-line option:\n/usr/lib/cyrus/bin/smtptest -t \"\" -a ubuntu -w ubuntusecret -m plain n-exim.lxd\nIn the new output,\nPLAIN\nwas selected:\nS: 220 n-exim ESMTP Exim 4.97 Ubuntu Mon, 23 Jun 2025 21:15:39 +0000\nC: EHLO smtptest\nS: 250-n-exim Hello n-exim.lxd [10.10.17.9]\nS: 250-SIZE 52428800\nS: 250-8BITMIME\nS: 250-PIPELINING\nS: 250-PIPECONNECT\nS: 250-CHUNKING\nS: 250-STARTTLS\nS: 250-PRDR\nS: 250 HELP\nC: STARTTLS\nS: 220 TLS go ahead\nverify error:num=18:self-signed certificate\nTLS connection established: TLSv1.3 with cipher TLS_AES_256_GCM_SHA384 (256/256 bits)\nC: EHLO smtptest\nS: 250-n-exim Hello n-exim.lxd [10.10.17.9]\nS: 250-SIZE 52428800\nS: 250-8BITMIME\nS: 250-PIPELINING\nS: 250-PIPECONNECT\nS: 250-AUTH PLAIN LOGIN\nS: 250-CHUNKING\nS: 250-PRDR\nS: 250 HELP\nC: AUTH PLAIN AHVidW50dQB1YnVudHVzZWNyZXQ=\nS: 235 Authentication succeeded\nAuthenticated.\nSecurity strength factor: 256\nTroubleshooting tips\n¶\nHere are some troubleshooting tips.\nPermissions\n¶\nIf using\nsaslauthd\n: Can the\nDebian-exim\nuser read and write to the\nsaslauthd\nsocket in\n/run/saslauthd/mux\nsocket?\nIf using\n/etc/exim4/passwd\n: Can the\nDebian-exim\nuser read this file?\nConfig\n¶\nIf changing a configuration file under\n/etc/exim4/conf.d/\n, make sure to be using the split-config mode! Check the\n/etc/exim4/update-exim4.conf.conf\nfile to see which mode is in use.\nSimilarly, if changing the configuration file\n/etc/exim4/exim4.conf.template\n, make sure to be using the non-split mode.\nAfter any configuration file change, be it split mode or not, be sure to run\nsudo\nupdate-exim4.conf\nand restart the\nexim4\nservice.\nFurther reading\n¶\nSee\nexim.org\nfor more information.\nAnother resource is the\nExim4 Ubuntu Wiki\npage.\nFurther resources to\nset up mailman3 with Exim4\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:18Z", "original_len_words": 2093}}
{"id": "b4b5dfb147", "source_url": "https://documentation.ubuntu.com/server/how-to/mail-services/install-postfix/", "title": "Install and configure Postfix - Ubuntu Server documentation", "text": "Install and configure Postfix - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure Postfix\n¶\nNote\nThis guide does not cover setting up Postfix\nVirtual Domains\n. For information on Virtual Domains and other advanced configurations see the references list at the end of this page.\nInstall Postfix\n¶\nTo install\nPostfix\nrun the following command:\nsudo\napt\ninstall\npostfix\nIt is OK to accept defaults initially by pressing return for each question. Some of the configuration options will be investigated in greater detail in the configuration stage.\nWarning\nThe\nmail-stack-delivery\nmetapackage has been deprecated in Focal. The package still exists for compatibility reasons, but won’t setup a working email system.\nConfigure Postfix\n¶\nThere are four things you should decide before configuring:\nThe <Domain> for which you’ll accept email (we’ll use\nmail.example.com\nin our example)\nThe network and class range of your mail server (we’ll use\n192.168.0.0/24\n)\nThe username (we’re using\nsteve\n)\nType of mailbox format (\nmbox\nis the default, but we’ll use the alternative,\nMaildir\n)\nTo configure postfix, run the following command:\nsudo\ndpkg-reconfigure\npostfix\nThe user interface will be displayed. On each screen, select the following values:\nInternet Site\nmail.example.com\nsteve\nmail.example.com\n,\nlocalhost.localdomain\n,\nlocalhost\nNo\n127.0.0.0/8\n\\[::ffff:127.0.0.0\\]/104\n\\[::1\\]/128\n192.168.0.0/24\n0\n+\nall\nTo set the mailbox format, you can either edit the configuration file directly, or use the\npostconf\ncommand.  In either case, the configuration parameters will be stored in\n/etc/postfix/main.cf\nfile. Later if you wish to re-configure a particular parameter, you can either run the command or change it manually in the file.\nConfigure mailbox format\n¶\nTo configure the mailbox format for\nMaildir\n:\nsudo\npostconf\n-e\n'home_mailbox = Maildir/'\nThis will place new mail in\n/home/<username>/Maildir\nso you will need to configure your Mail Delivery Agent (MDA) to use the same path.\nSMTP authentication\n¶\nSMTP-AUTH allows a client to identify itself through the Simple Authentication and Security Layer (SASL) authentication mechanism, using Transport Layer Security (TLS) to encrypt the authentication process. Once it has been authenticated, the SMTP server will allow the client to relay mail.\nConfigure SMTP authentication\n¶\nTo configure Postfix for SMTP-AUTH using SASL (Dovecot SASL), run these commands at a terminal prompt:\nsudo\npostconf\n-e\n'smtpd_sasl_type = dovecot'\nsudo\npostconf\n-e\n'smtpd_sasl_path = private/auth'\nsudo\npostconf\n-e\n'smtpd_sasl_local_domain ='\nsudo\npostconf\n-e\n'smtpd_sasl_security_options = noanonymous'\nsudo\npostconf\n-e\n'broken_sasl_auth_clients = yes'\nsudo\npostconf\n-e\n'smtpd_sasl_auth_enable = yes'\nsudo\npostconf\n-e\n'smtpd_recipient_restrictions = \\\npermit_sasl_authenticated,permit_mynetworks,reject_unauth_destination'\nNote\nThe\nsmtpd_sasl_path\nconfig parameter is a path relative to the Postfix queue directory.\nThere are several SASL mechanism properties worth evaluating to improve the security of your deployment. The option “noanonymous” prevents the use of mechanisms that permit anonymous authentication.\nConfigure TLS\n¶\nNext, generate or obtain a digital certificate for TLS. MUAs connecting to your mail server via TLS will need to recognize the certificate used for TLS. This can either be done using a certificate from Let’s Encrypt, from a commercial CA or with a self-signed certificate that users manually install/accept.\nFor MTA-to-MTA, TLS certificates are never validated without prior agreement from the affected organizations. For MTA-to-MTA TLS, there is no reason not to use a self-signed certificate unless local policy requires it. See our\nguide on security certificates\nfor details about generating digital certificates and setting up your own Certificate Authority (CA).\nOnce you have a certificate, configure Postfix to provide TLS encryption for both incoming and outgoing mail:\nsudo\npostconf\n-e\n'smtp_tls_security_level = may'\nsudo\npostconf\n-e\n'smtpd_tls_security_level = may'\nsudo\npostconf\n-e\n'smtp_tls_note_starttls_offer = yes'\nsudo\npostconf\n-e\n'smtpd_tls_chain_files = /etc/ssl/private/server.key,/etc/ssl/certs/server.crt'\nsudo\npostconf\n-e\n'smtpd_tls_loglevel = 1'\nsudo\npostconf\n-e\n'smtpd_tls_received_header = yes'\nsudo\npostconf\n-e\n'myhostname = mail.example.com'\nIf you are using your own Certificate Authority to sign the certificate, enter:\nsudo\npostconf\n-e\n'smtpd_tls_CAfile = /etc/ssl/certs/cacert.pem'\nAgain, for more details about certificates see our\nsecurity certificates guide\n.\nOutcome of initial configuration\n¶\nAfter running all the above commands, Postfix will be configured for SMTP-AUTH with a self-signed certificate for TLS encryption.\nNow, the file\n/etc/postfix/main.cf\nshould look like this:\n# See /usr/share/postfix/main.cf.dist for a commented, more complete\n# version\n    \nsmtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)\nbiff = no\n    \n# appending .domain is the MUA's job.\nappend_dot_mydomain = no\n    \n# Uncomment the next line to generate \"delayed mail\" warnings\n#delay_warning_time = 4h\n    \nmyhostname = server1.example.com\nalias_maps = hash:/etc/aliases\nalias_database = hash:/etc/aliases\nmyorigin = /etc/mailname\nmydestination = server1.example.com, localhost.example.com, localhost\nrelayhost =\nmynetworks = 127.0.0.0/8\nmailbox_command = procmail -a \"$EXTENSION\"\nmailbox_size_limit = 0\nrecipient_delimiter = +\ninet_interfaces = all\nsmtpd_sasl_local_domain =\nsmtpd_sasl_auth_enable = yes\nsmtpd_sasl_security_options = noanonymous\nbroken_sasl_auth_clients = yes\nsmtpd_recipient_restrictions =\npermit_sasl_authenticated,permit_mynetworks,reject _unauth_destination\nsmtpd_tls_auth_only = no\nsmtp_tls_security_level = may\nsmtpd_tls_security_level = may\nsmtp_tls_note_starttls_offer = yes\nsmtpd_tls_key_file = /etc/ssl/private/smtpd.key\nsmtpd_tls_cert_file = /etc/ssl/certs/smtpd.crt\nsmtpd_tls_CAfile = /etc/ssl/certs/cacert.pem\nsmtpd_tls_loglevel = 1\nsmtpd_tls_received_header = yes\nsmtpd_tls_session_cache_timeout = 3600s\ntls_random_source = dev:/dev/urandom\nThe Postfix initial configuration is now complete. Run the following command to restart the Postfix daemon:\nsudo\nsystemctl\nrestart\npostfix.service\nSASL\n¶\nPostfix supports SMTP-AUTH as defined in\nRFC2554\n. It is based on\nSASL\n. However it is still necessary to set up SASL authentication before you can use SMTP-AUTH.\nWhen using IPv6, the\nmynetworks\nparameter may need to be modified to allow IPv6 addresses, for example:\nmynetworks = 127.0.0.0/8, [::1]/128\nConfigure SASL\n¶\nPostfix supports two SASL implementations:\nCyrus SASL\nand\nDovecot SASL\n.\nTo enable Dovecot SASL the\ndovecot-core\npackage will need to be installed:\nsudo\napt\ninstall\ndovecot-core\nNext, edit\n/etc/dovecot/conf.d/10-master.conf\nand change the following:\nservice auth {\n  # auth_socket_path points to this userdb socket by default. It's typically\n  # used by dovecot-lda, doveadm, possibly imap process, etc. Its default\n  # permissions make it readable only by root, but you may need to relax these\n  # permissions. Users that have access to this socket are able to get a list\n  # of all usernames and get results of everyone's userdb lookups.\n  unix_listener auth-userdb {\n    #mode = 0600\n    #user = \n    #group = \n  }\n    \n  # Postfix smtp-auth\n  unix_listener /var/spool/postfix/private/auth {\n    mode = 0660\n    user = postfix\n    group = postfix\n  }\n }\nTo permit use of SMTP-AUTH by Outlook clients, change the following line in the\nauthentication mechanisms\nsection of\n/etc/dovecot/conf.d/10-auth.conf\nfrom:\nauth_mechanisms = plain\nto this:\nauth_mechanisms = plain login\nOnce you have configured Dovecot, restart it with:\nsudo\nsystemctl\nrestart\ndovecot.service\nTest your setup\n¶\nSMTP-AUTH configuration is complete – now it is time to test the setup. To see if SMTP-AUTH and TLS work properly, run the following command:\ntelnet\nmail.example.com\n25\nAfter you have established the connection to the Postfix mail server, type:\nehlo\nmail.example.com\nIf you see the following in the output, then everything is working perfectly. Type\nquit\nto exit.\n250-STARTTLS\n250-AUTH LOGIN PLAIN\n250-AUTH=LOGIN PLAIN\n250 8BITMIME\nTroubleshooting\n¶\nWhen problems arise, there are a few common ways to diagnose the cause.\nEscaping\nchroot\n¶\nThe Ubuntu Postfix package will, by default, install into a\nchroot\nenvironment for security reasons. This can add greater complexity when troubleshooting problems.\nTo turn off the\nchroot\nusage, locate the following line in the\n/etc/postfix/master.cf\nconfiguration file:\nsmtp      inet  n       -       -       -       -       smtpd\nModify it as follows:\nsmtp      inet  n       -       n       -       -       smtpd\nYou will then need to restart Postfix to use the new configuration. From a terminal prompt enter:\nsudo\nservice\npostfix\nrestart\nSMTPS\n¶\nIf you need secure SMTP, edit\n/etc/postfix/master.cf\nand uncomment the following line:\nsmtps     inet  n       -       -       -       -       smtpd\n  -o smtpd_tls_wrappermode=yes\n  -o smtpd_sasl_auth_enable=yes\n  -o smtpd_client_restrictions=permit_sasl_authenticated,reject\n  -o milter_macro_daemon_name=ORIGINATING\nLog viewing\n¶\nPostfix sends all log messages to\n/var/log/mail.log\n. However, error and warning messages can sometimes get lost in the normal log output so they are also logged to\n/var/log/mail.err\nand\n/var/log/mail.warn\nrespectively.\nTo see messages entered into the logs in real time you can use the\ntail\n-f\ncommand:\ntail\n-f\n/var/log/mail.err\nIncrease logging detail\n¶\nThe amount of detail recorded in the logs can be increased via the configuration options. For example, to increase TLS activity logging set the\nsmtpd_tls_loglevel\noption to a value from 1 to 4.\nsudo\npostconf\n-e\n'smtpd_tls_loglevel = 4'\nReload the service after any configuration change, to activate the new config:\nsudo\nsystemctl\nreload\npostfix.service\nLogging mail delivery\n¶\nIf you are having trouble sending or receiving mail from a specific domain you can add the domain to the\ndebug_peer_list\nparameter.\nsudo\npostconf\n-e\n'debug_peer_list = problem.domain'\nsudo\nsystemctl\nreload\npostfix.service\nIncrease daemon verbosity\n¶\nYou can increase the verbosity of any Postfix daemon process by editing the\n/etc/postfix/master.cf\nand adding a\n-v\nafter the entry. For example, edit the\nsmtp\nentry:\nsmtp\nunix\n-\n-\n-\n-\n-\nsmtp\n-v\nThen, reload the service as usual:\nsudo\nsystemctl\nreload\npostfix.service\nLog SASL debug info\n¶\nTo increase the amount of information logged when troubleshooting SASL issues you can set the following options in\n/etc/dovecot/conf.d/10-logging.conf\nauth_debug\n=\nyes\nauth_debug_passwords\n=\nyes\nAs with Postfix, if you change a Dovecot configuration the process will need to be reloaded:\nsudo\nsystemctl\nreload\ndovecot.service\nNote\nSome of the options above can drastically increase the amount of information sent to the log files. Remember to return the log level back to normal after you have corrected the problem – then reload the appropriate daemon for the new configuration to take effect.\nFurther reading\n¶\nAdministering a Postfix server can be a very complicated task. At some point you may need to turn to the Ubuntu community for more experienced help.\nThe\nPostfix website\ndocuments all available configuration options.\nO’Reilly’s\nPostfix: The Definitive Guide\nis rather dated but provides deep background information about configuration options.\nThe\nUbuntu Wiki Postfix\npage has more information from an Ubuntu context.\nThere is also a\nDebian Wiki Postfix\npage that’s a bit more up to date; they also have a set of\nPostfix Tutorials\nfor different Debian versions.\nInfo on how to\nset up mailman3 with postfix\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:19Z", "original_len_words": 1669}}
{"id": "1b283722cf", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/", "title": "Networking - Ubuntu Server documentation", "text": "Networking - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nNetworking\n¶\nThis section contains how-to guides on most aspects of networking in Ubuntu. If you would like a broader overview of these topics before getting started, refer to our\nintroduction to networking\n.\nConfiguration\n¶\nNetwork configuration in Ubuntu is handled through Netplan. See our general walkthrough on\nConfiguring networks\n, or refer to\nthe Netplan documentation\nfor more specific instructions.\nNetwork tools\n¶\nThe File Transfer Protocol (FTP) can be set up to provide files for download.\nFile transfers with FTP\nThe Domain Name Service (DNS) maps IP addresses to fully qualified domain names (FQDN). The DNS Security Extensions (DNSSEC) allow DNS data to be verified.\nSet up a name server (DNS)\nSet up DNS Security Extensions (DNSSEC)\nDNSSEC Troubleshooting\nOpen vSwitch (OVS) with the Data Plane Development Kit (DPDK) provides virtual switching for network automation in virtualized environments.\nUse Open vSwitch with DPDK\nDHCP\n¶\nSet up Dynamic Host Configuration Protocol (DHCP) for automatic IP address assignment for devices on your network. There are two DHCP servers available in Ubuntu:\nisc-kea\nis the most modern, and is available from 23.04 onwards.\nInstall DHCP isc-kea\nInstall DHCP isc-dhcp-server\nTime synchronization\n¶\nThe Network Time Protocol (NTP) synchronizes time over a network. Ubuntu uses\nchrony\nby default to handle this. However, users can install and use\ntimedatectl\n/\ntimesyncd\ninstead if preferred.\nTime sync with chrony\nTime sync with timedatectl and timesyncd\nServing time with chrony\nNetwork shares\n¶\nSharing files and resources across a network is a common requirement - this is where the Network File System (NFS) comes in.\nNetwork File System (NFS) sharing\nIf you need to share network resources between Linux and Windows systems, see our sections on Samba and Active Directory.\nSamba\nActive Directory integration\nPrinting\n¶\nThe Common UNIX Printing System (CUPS) is the most common way to manage print services in Ubuntu.\nSet up a CUPS print server\nSee also\n¶\nExplanation:\nNetworking section", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:19Z", "original_len_words": 353}}
{"id": "569adb6a90", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/chrony-client/", "title": "Synchronize time using Chrony - Ubuntu Server documentation", "text": "Synchronize time using Chrony - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSynchronize time using Chrony\n¶\nUbuntu uses\nchrony\nfor synchronizing time, which is installed by default as of Ubuntu 25.10. You can optionally use\ntimedatectl\n/\ntimesyncd\nto\nsynchronize time using systemd\n.\nCheck status of\nchrony\nclient\n¶\nThe current status of time synchronization can be checked with the\ntimedatectl\nstatus\ncommand, which is available via\nsystemd\nand will produce output like this:\nLocal time: Mo 2025-06-16 15:21:46 CEST\n           Universal time: Mo 2025-06-16 13:21:46 UTC\n                 RTC time: Mo 2025-06-16 13:21:46\n                Time zone: Europe/Berlin (CEST, +0200)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\nFor more details on time accuracy, Chrony can be queried directly, using the\nchronyc\n-N\ntracking\ncommand, producing output like this:\nReference ID    : B97DBE7B (2.ntp.ubuntu.com)\nStratum         : 3\nRef time (UTC)  : Mon Jun 16 13:06:04 2025\nSystem time     : 0.000000004 seconds slow of NTP time\nLast offset     : +0.001758954 seconds\nRMS offset      : 0.017604901 seconds\nFrequency       : 3.889 ppm slow\nResidual freq   : +0.202 ppm\nSkew            : 1.458 ppm\nRoot delay      : 0.022837413 seconds\nRoot dispersion : 0.003050051 seconds\nUpdate interval : 1032.5 seconds\nLeap status     : Normal\nNetwork Time Security (NTS)\n¶\nChrony supports “Network Time Security” (NTS) and enables it by default, using the Ubuntu NTS pools. This is done by specifying a\nserver\nor\npool\nas usual. Afterwards, options can be listed and it is there that\nnts\ncan be added. For example:\nserver <server-fqdn-or-IP> iburst nts\n# or as concrete example\npool 1.ntp.ubuntu.com iburst maxsources 1 nts prefer\nFor\nvalidation of NTS enablement\n, one can list the time sources in use by executing the\nchronyc\n-N\nsources\ncommand, to find the timeserver in use, as indicated by the\n^*\nsymbol in the first column. Then check the\nauthdata\nof that connection using\nsudo\nchronyc\n-N\nauthdata\n. If the client was able to successfully establish a NTS connection, it will show the\nMode:\nNTS\nfield and non-zero values for\nKeyID\n,\nType\nand\nKLen\n:\n$ sudo chronyc -N authdata\nName/IP address             Mode KeyID Type KLen Last Atmp  NAK Cook CLen\n=========================================================================\n1.ntp.ubuntu.com             NTS     6   30  128  14d    0    0    8   64\n2.ntp.ubuntu.com             NTS     6   30  128  14d    0    0    8   64\n3.ntp.ubuntu.com             NTS     1   30  128  27d    0    0    8   64\n4.ntp.ubuntu.com             NTS     2   30  128  20d    0    0    5   64\nntp-bootstrap.ubuntu.com     NTS     3   30  128   7d    0    0    8   64\nNTS related constraints\n¶\nKey Exchange port:\nNTS/KE uses a separate port (4460/tcp)** to negotiate\nsecurity parameters, which are then used via the normal NTP port (123/udp).\nThis is a new deployment, running on different IP addresses than the\ntraditional Ubuntu NTP pool.\nWarning\nIf the network does not allow access to the Ubuntu NTS servers and required\nports, with the default configuration is in place,\nchrony\nwill not be able\nto adjust the system’s clock. To revert to NTP, edit the configuration file\nin\n/etc/chrony/sources.d/ubuntu-ntp-pools.sources\nand revert to using\nthe listed NTP servers in favor of the NTS ones.\nBad Clocks and secure time syncing:\nNTS is based on TLS, and TLS needs a\nreasonably correct clock. Due to that, an NTS-based sync might fail if the\nclock is too far off. On hardware affected by this problem, one can consider\nusing the\nnocerttimecheck\noption, which allows to set the number of times\nthat the time can be synced without checking validation and expiration.\nNote\nA new CA is installed in\n/etc/chrony/nts-bootstrap-ubuntu.crt\nthat is\nused specifically for the Ubuntu NTS bootstrap server, needed for when the\nclock is too far off. This is added to certificate set ID “1”, and defined\nvia\n/etc/chrony/conf.d/ubuntu-nts.conf\n.\nConfigure Chrony\n¶\nAn admin can control the timezone and how the system clock should relate to the\nhwclock\nusing the common\ntimedatectl\n[set-timezone/set-local-rtc]\ncommands, provided by\nsystemd\n. For more specific actions, like adding of time-sources, the\nchronyc\ncommand can be used. See\nman\nchronyc\nfor more details.\nOne can edit configuration in\n/etc/chrony/sources.d/\nto add/remove server lines. By default these servers are configured:\n# Use NTS by default\n# NTS uses an additional port to negotiate security: 4460/tcp\n# The normal NTP port remains in use: 123/udp\npool 1.ntp.ubuntu.com iburst maxsources 1 nts prefer\npool 2.ntp.ubuntu.com iburst maxsources 1 nts prefer\npool 3.ntp.ubuntu.com iburst maxsources 1 nts prefer\npool 4.ntp.ubuntu.com iburst maxsources 1 nts prefer\n# The bootstrap server is needed by systems without a hardware clock, or a very\n# large initial clock offset. The specified certificate set is defined in\n# /etc/chrony/conf.d/ubuntu-nts.conf.\npool ntp-bootstrap.ubuntu.com iburst maxsources 1 nts certset 1\nAfter adding or removing sources, they can be reloaded using\nsudo\nchrony\nreload\nsources\n.\nOf the pool,\n2.ubuntu.pool.ntp.org\nand\nntp.ubuntu.com\nalso support IPv6, if needed. If you need to force IPv6, there is also\nipv6.ntp.ubuntu.com\nwhich is not configured by default.\nTime sources provided by DHCP (option 42)\n¶\nChrony consumes time sources provided by DHCP (option 42). Those could be traditional, non-authenticated NTP sources. Should one want to avoid this behavior, overruling the choices made by a local DHCP administrator, it can be disabled in\n/etc/chrony/chrony.conf\n. To do that one would comment out the corresponding setting:\n# Use time sources from DHCP.\n# sourcedir /run/chrony-dhcp\nChrony time-daemon\n¶\nchronyd\nitself is a normal service, so you can check its status in more detail using:\nsystemctl\nstatus\nchrony\n.\nservice\nThe output produced will look something like this:\n● chrony.service - chrony, an NTP client/server\n     Loaded: loaded (/usr/lib/systemd/system/chrony.service; enabled; preset: enabled)\n     Active: active (running) since Mon 2025-06-02 11:27:09 CEST; 2 weeks 0 days ago\n       Docs: man:chronyd(8)\n             man:chronyc(1)\n             man:chrony.conf(5)\n   Main PID: 36027 (chronyd)\n      Tasks: 2 (limit: 28388)\n     Memory: 5.8M (peak: 6.8M swap: 604.0K swap peak: 4.5M)\n        CPU: 5.038s\n     CGroup: /system.slice/chrony.service\n             ├─36027 /usr/sbin/chronyd -F 1\n             └─36028 /usr/sbin/chronyd -F 1\n\n    Jun 02 11:27:09 questing chronyd[36027]: Using right/UTC timezone to obtain leap second data\n    Jun 02 11:27:09 questing chronyd[36027]: Loaded seccomp filter (level 1)\n    Jun 02 11:27:09 questing chronyd[36027]: Added pool 1.ntp.ubuntu.com\n    Jun 02 11:27:09 questing chronyd[36027]: Added pool 2.ntp.ubuntu.com\n    Jun 02 11:27:09 questing chronyd[36027]: Added pool 3.ntp.ubuntu.com\n    Jun 02 11:27:09 questing chronyd[36027]: Added pool 4.ntp.ubuntu.com\n    Jun 02 11:27:09 questing chronyd[36027]: Added pool ntp-bootstrap.ubuntu.com\n    Jun 02 11:27:09 questing systemd[1]: Started chrony.service - chrony, an NTP client/server.\nDefault configuration such as\nsourcedir\n,\nntsdumpdir\nor\nrtcsync\nis provided in\n/etc/chrony/chrony.conf\nand additional config files can be stored in\n/etc/chrony/conf.d/\n. The NTS servers from which to fetch time for\nchrony\nare in defined in\n/etc/chrony/sources.d/ubuntu-ntp-pools.sources\n. There are more advanced options documented in the\nchrony.conf(5)\nmanpage. Common use cases are specifying an explicit trusted certificate. After changing any part of the config file you need to restart\nchrony\n, as follows:\nsudo\nsystemctl\nrestart\nchrony.service\nNext steps\n¶\nIf you would now like to also serve the Network Time Protocol via\nchrony\n, this guide will walk you through\nhow to configure your Chrony setup\n.\nReferences\n¶\nchronyc(1)\nmanual page\nchronyd(8)\nmanual page\nchrony.conf(5)\nmanual page", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:19Z", "original_len_words": 1184}}
{"id": "44ad815dac", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/cups-print-server/", "title": "Install and configure a CUPS print server - Ubuntu Server documentation", "text": "Install and configure a CUPS print server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure a CUPS print server\n¶\nThe\nCommon UNIX Printing System, or CUPS\n, is the most widely-used way to manage printing and print services in Ubuntu. This freely-available printing system has become the standard for printing in most Linux distributions, and uses the standard Internet Printing Protocol (IPP) to handle network printing.\nCUPS manages print jobs and queues, and provides support for a wide range of printers, from dot-matrix to laser, and many in between. CUPS also supports PostScript Printer Description (PPD) and auto-detection of network printers, and features a simple web-based configuration and administration tool.\nInstall CUPS\n¶\nA complete CUPS install has many package dependencies, but they can all be specified on the same command line. To perform a basic installation of CUPS, enter the following command in your terminal:\nsudo\napt\ninstall\ncups\nOnce the download and installation have finished, the CUPS server will be started automatically.\nConfigure the CUPS server\n¶\nThe CUPS server’s behavior is configured through directives found in the\n/etc/cups/cupsd.conf\nconfiguration file. This CUPS configuration file follows the same syntax as the main configuration file for the Apache HTTP server. Some examples of commonly-configured settings will be presented here.\nMake a copy of the configuration file\n¶\nWe recommend that you make a copy of the original CUPS configuration file and protect it from writing, before you start configuring CUPS. You will then have the original settings as a reference, which you can reuse or restore as necessary.\nsudo\ncp\n/etc/cups/cupsd.conf\n/etc/cups/cupsd.conf.original\nsudo\nchmod\na-w\n/etc/cups/cupsd.conf.original\nConfigure Server administrator\n¶\nTo configure the email address of the designated CUPS server administrator, edit the\n/etc/cups/cupsd.conf\nconfiguration file with your preferred text editor, and add or modify the\nServerAdmin\nline accordingly. For example, if you are the administrator for the CUPS server, and your e-mail address is\nbjoy@somebigco.com\n, then you would modify the ServerAdmin line to appear as follows:\nServerAdmin bjoy@somebigco.com\nConfigure Listen\n¶\nBy default on Ubuntu, CUPS listens only on the loopback interface at IP address\n127.0.0.1\n.\nTo instruct CUPS to listen on an actual network adapter’s IP address, you must specify either a\nhostname\n, the IP address, or (optionally) an IP address/port pairing via the addition of a\nListen\ndirective.\nFor example, if your CUPS server resides on a local network at the IP address\n192.168.10.250\nand you’d like to make it accessible to the other systems on this subnetwork, you would edit the\n/etc/cups/cupsd.conf\nand add a Listen directive, as follows:\nListen 127.0.0.1:631           # existing loopback Listen\nListen /var/run/cups/cups.sock # existing socket Listen\nListen 192.168.10.250:631      # Listen on the LAN interface, Port 631 (IPP)\nIn the example above, you can comment out or remove the reference to the Loopback address (\n127.0.0.1\n) if you do not want the CUPS daemon (\ncupsd\n) to listen on that interface, but would rather have it only listen on the Ethernet interfaces of the Local Area Network (LAN). To enable listening for all network interfaces for which a certain hostname is bound, including the Loopback, you could create a Listen entry for the hostname\nsocrates\nlike this:\nListen socrates:631  # Listen on all interfaces for the hostname 'socrates'\nor by omitting the Listen directive and using\nPort\ninstead, as in:\nPort 631  # Listen on port 631 on all interfaces\nFor more examples of configuration directives in the CUPS server configuration file, view the associated system manual page by entering the following command:\nman\ncupsd.conf\nPost-configuration restart\n¶\nWhenever you make changes to the\n/etc/cups/cupsd.conf\nconfiguration file, you’ll need to restart the CUPS server by typing the following command at a terminal prompt:\nsudo\nsystemctl\nrestart\ncups.service\nWeb Interface\n¶\nCUPS can be configured and monitored using a web interface, which by default is available at\nhttp://localhost:631/admin\n. The web interface can be used to perform all printer management tasks.\nTo perform administrative tasks via the web interface, you must either have the root account enabled on your server, or authenticate as a user in the\nlpadmin\ngroup. For security reasons, CUPS won’t authenticate a user that doesn’t have a password.\nTo add a user to the\nlpadmin\ngroup, run at the terminal prompt:\nsudo\nusermod\n-aG\nlpadmin\nusername\nFurther documentation is available in the “Documentation/Help” tab of the web interface.\nError logs\n¶\nFor troubleshooting purposes, you can access CUPS server errors via the error log file at:\n/var/log/cups/error_log\n. If the error log does not show enough information to troubleshoot any problems you encounter, the verbosity of the CUPS log can be increased by changing the\nLogLevel\ndirective in the configuration file (discussed above) from the default of “info” to “debug” or even “debug2”, which logs everything.\nIf you make this change, remember to change it back once you’ve solved your problem, to prevent the log file from becoming overly large.\nReferences\n¶\nCUPS Website\nDebian Open-iSCSI page", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:19Z", "original_len_words": 850}}
{"id": "04fa6b6aba", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/dnssec-troubleshooting/", "title": "Basic DNSSEC troubleshooting - Ubuntu Server documentation", "text": "Basic DNSSEC troubleshooting - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nBasic DNSSEC troubleshooting\n¶\nSome of the troubleshooting tips that will be shown here are focused on the BIND9\nDNS\nserver and its tools, but the general principle applies to\nDNSSEC\nin all implementations.\nHandy “bad” and “good” DNSSEC domains\n¶\nIt helps to have some good known domains with broken and working DNSSEC available for testing, so we can be sure our tooling is catching those, and not just failing everywhere. There is no guarantee that these domains will be up forever, and certainly there are more out there, but this list is a good first choice:\nThese should fail DNSSEC validation:\ndnssec-failed.org\nsigfail.ippacket.stream\nThese should pass DNSSEC validation:\nisc.org\nsigok.ippacket.stream\nLogs\n¶\nBy default, the BIND9 server will log certain DNSSEC failures, and the journal log should be the first place to check.\nFor example, if we ask a BIND9 Validating Resolver for the IP address of the\nwww.dnssec-failed.org\nname, we get a failure:\n$ dig @127.0.0.1 -t A www.dnssec-failed.org\n; <<>> DiG 9.18.28-0ubuntu0.24.04.1-Ubuntu <<>> @127.0.0.1 -t A www.dnssec-failed.org\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 26260\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n; COOKIE: 6339d7228b8587f401000000671bc2eb2fe25bdf099ef1af (good)\n;; QUESTION SECTION:\n;www.dnssec-failed.org.         IN      A\n\n;; Query time: 460 msec\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\n;; WHEN: Fri Oct 25 16:10:19 UTC 2024\n;; MSG SIZE  rcvd: 78\nThat’s a very generic failure: it just says\nSERVFAIL\n, and gives us no IP:\nIN\nA\nis empty. The BIND9 logs, however, tell a more detailed story:\n$ journalctl -u named.service -f\n(...)\nnamed[286]: validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\nnamed[286]: no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 68.87.85.132#53\nnamed[286]: validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\nnamed[286]: no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 68.87.68.244#53\nnamed[286]: validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\nnamed[286]: no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 68.87.76.228#53\nnamed[286]: validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\nnamed[286]: no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 68.87.72.244#53\nnamed[286]: validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\nnamed[286]: no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 69.252.250.103#53\nnamed[286]: broken trust chain resolving 'www.dnssec-failed.org/A/IN': 68.87.72.244#53\nClient-side tooling: dig\n¶\nOne of the more versatile DNS troubleshooting tools is\ndig\n, generally used for interrogating DNS name servers to lookup and display domain information, but its broad functionality makes it a flexible aid for DNS troubleshooting. It provides direct control over setting most of the DNS flags in queries, and displays detailed responses for inspection.\nFor DNSSEC troubleshooting purposes, we are interested in the following features:\n+dnssec\n: Set the “DNSSEC OK” bit in the queries, which tells the resolver to include in its responses the DNSSEC RRSIG records. This is also shown as a\ndo\nflag in queries.\n+cd\n: This means\ncheck disabled\nand tells the resolver we can accept unauthenticated data in the DNS responses.\nad\n: When included in a response, this flag means\nauthenticated data\n, and tells us that the resolver who provided this answer has performed DNSSEC validation.\n@<IP>\n: This parameter lets us direct the query to a specific DNS server running at the provided IP address.\nFor example, let’s query the local systemd-resolved DNS stub resolver (running at\n127.0.0.53\n) for the\nisc.org\ntype\nA\nrecord, and request DNSSEC data:\n$ dig @127.0.0.53 -t A +dnssec +multiline isc.org\n\n; <<>> DiG 9.18.30-0ubuntu0.24.04.2-Ubuntu <<>> @127.0.0.53 -t A +dnssec +multiline isc.org\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 13164\n;; flags: qr rd ra ad; QUERY: 1, ANSWER: 5, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags: do; udp: 65494\n;; QUESTION SECTION:\n;isc.org.\t\tIN A\n\n;; ANSWER SECTION:\nisc.org.\t\t300 IN A 151.101.130.217\nisc.org.\t\t300 IN A 151.101.66.217\nisc.org.\t\t300 IN A 151.101.2.217\nisc.org.\t\t300 IN A 151.101.194.217\nisc.org.\t\t300 IN RRSIG A 13 2 300 (\n\t\t\t\t20250830114017 20250816111944 27566 isc.org.\n\t\t\t\tu6hKKZGX3DUD6JJAjHsGog+nfR9bz5qp2g1h3qibZI+A\n\t\t\t\tqWH05fWJJjoSMzcnOgzIO1299zPIZd0xdMAh1wbkjw== )\n\n;; Query time: 83 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\n;; WHEN: Mon Aug 25 13:28:33 CEST 2025\n;; MSG SIZE  rcvd: 203\nLet’s unpack this answer for the important troubleshooting parts:\nThe answer has the\nad\nflag set, meaning this data was authenticated. In other words, DNSSEC validation was successful.\nThe status of the query is\nNOERROR\n, and we have 5 records in the answer section.\nAn RRSIG record for the “A” Resource Record was returned as requested by the\n+dnssec\ncommand-line parameter. This is also confirmed by the presence of the “do” flag in the “OPT PSEUDOSECTION”.\nIf we repeat this query with a domain that we know fails DNSSEC validation, we get the following reply:\n$ dig @127.0.0.53 -t A +dnssec +multiline dnssec-failed.org\n\n; <<>> DiG 9.18.30-0ubuntu0.24.04.2-Ubuntu <<>> @127.0.0.53 -t A +dnssec +multiline dnssec-failed.org\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 8314\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 2\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags: do; udp: 65494\n;; QUESTION SECTION:\n;dnssec-failed.org.\tIN A\n\n;; ANSWER SECTION:\ndnssec-failed.org.\t300 IN A 96.99.227.255\n\n;; ADDITIONAL SECTION:\ndnssec-failed.org.\t300 IN RRSIG A 5 2 300 (\n\t\t\t\t20250904145125 20250818144625 44973 dnssec-failed.org.\n\t\t\t\tUF75l9JkH/AZ9ApNF86stA81B+L36j0F/L8ENvMknsfK\n\t\t\t\tFwll6cLEJWBalKeyhK7p3U/Lqet9L3Oti8H7RudmgZ4v\n\t\t\t\tkdYMDrb9mqnXscY1R/kmSrtu3gOO1ob+khKinyAVwKLb\n\t\t\t\tR0CVZnJLSb7c++BI9fRJjE2caZ+2RkHwkLATR28= )\n\n;; Query time: 138 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\n;; WHEN: Mon Aug 25 13:37:11 CEST 2025\n;; MSG SIZE  rcvd: 239\nThis time:\nThere is no\nad\nflag set in the answer.\nThe status of the query is a generic\nSERVFAIL\n, and zero answers were provided.\nWe can tell the local stub resolver (systemd-resolved running at\n@127.0.0.53\naddress) that we don’t want it to perform DNSSEC validation. We do that by setting the\n+cd\n(check disabled) flag. Then things change in our answer:\n$ dig @127.0.0.53 -t A +dnssec +cd +multiline dnssec-failed.org\n\n; <<>> DiG 9.18.30-0ubuntu0.24.04.2-Ubuntu <<>> @127.0.0.53 -t A +dnssec +cd +multiline dnssec-failed.org\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55352\n;; flags: qr rd ra cd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags: do; udp: 65494\n; EDE: 9 (DNSKEY Missing)\n;; QUESTION SECTION:\n;dnssec-failed.org.\tIN A\n\n;; ANSWER SECTION:\ndnssec-failed.org.\t181 IN A 96.99.227.255\ndnssec-failed.org.\t181 IN RRSIG A 5 2 300 (\n\t\t\t\t20250904145125 20250818144625 44973 dnssec-failed.org.\n\t\t\t\tUF75l9JkH/AZ9ApNF86stA81B+L36j0F/L8ENvMknsfK\n\t\t\t\tFwll6cLEJWBalKeyhK7p3U/Lqet9L3Oti8H7RudmgZ4v\n\t\t\t\tkdYMDrb9mqnXscY1R/kmSrtu3gOO1ob+khKinyAVwKLb\n\t\t\t\tR0CVZnJLSb7c++BI9fRJjE2caZ+2RkHwkLATR28= )\n\n;; Query time: 3 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\n;; WHEN: Mon Aug 25 13:39:09 CEST 2025\n;; MSG SIZE  rcvd: 245\nLooks like we have some sort of answer, but:\nThere is no\nad\nflag in the answer, so this data was not authenticated.\nThe status is\nNOERROR\n, and we got two answers.\nNote there is a\ncd\nflag in the answer, meaning “check disabled”. No attempt at validating the answer was done by the resolver, as requested.\nIn none of these cases, though, did\ndig\nperform DNSSEC validation: it just presented the results provided by the resolver, which in some cases was no validation at all (via the\n+cd\nflag). To perform the validation ourselves, we have to use a different tool.\nDigging a bit deeper:\ndelv\n¶\nThe\ndelv\ntool is very similar to\ndig\n, and can perform the same DNS queries, but with a crucial difference: it can also validate DNSSEC. This is very useful in troubleshooting, because rather than returning a generic\nSERVFAIL\nerror when something goes wrong with DNSSEC validation, it can tell us what was wrong in more detail.\nBut to bring the responsibility of doing DNSSEC validation to the tool itself, we use the\n+cd\nflag in our queries, to tell the resolver to not attempt that validation. Otherwise we will just get back a generic\nSERVFAIL\nerror:\n$ delv @127.0.0.53 -t A +dnssec +multiline dnssec-failed.org\n;; resolution failed: SERVFAIL\nWith the\n+cd\nflag present, however,\ndelv\nitself will do the validation. It will fail again, but now with a DNSSEC-specific error:\n$ delv @127.0.0.53 -t A +dnssec +cd +multiline dnssec-failed.org\n;; validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\n;; no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 127.0.0.1#53\n;; broken trust chain resolving 'dnssec-failed.org/A/IN': 127.0.0.1#53\n;; resolution failed: broken trust chain\nIf needed,\ndelv\ncan be told to not perform DNSSEC validation at all, by passing the\n-i\nflag. Together with the\n+cd\nflag, which instructs the Validating Resolver to not perform validation either, we get this result:\n$ delv @127.0.0.53 -i -t A +dnssec +cd +multiline dnssec-failed.org\n; answer not validated\ndnssec-failed.org.      100 IN A 96.99.227.255\nFor a good DNSSEC domain,\ndelv\nwill return a validated answer:\n$ delv @127.0.0.53 -t A +multiline +cd isc.org\n; fully validated\nisc.org.\t\t299 IN A 151.101.2.217\nisc.org.\t\t299 IN A 151.101.66.217\nisc.org.\t\t299 IN A 151.101.130.217\nisc.org.\t\t299 IN A 151.101.194.217\nisc.org.\t\t299 IN RRSIG A 13 2 300 (\n\t\t\t\t20250908112301 20250825104017 27566 isc.org.\n\t\t\t\t9oclTno0Ub2NmUEXdyLv0zqwPBbbUVmT3RX4aP4BQQ+h\n\t\t\t\t4g839JXuCKHufXSPkWh/GJe/MveP83dDvJkrMmEIzg== )\nGiven that above we used the\n+cd\nflag, this means that the validation was done by\ndelv\nitself. We will get the same result without that flag if the resolver also succeeds in the DNSSEC validation, and provides an answer.\nClient-side tooling: resolvectl\n¶\nThe local stub resolver\nsystemd-resolved\ncan perform DNSSEC validation locally using its high-level\nresolvectl\ntool. The local cache and state about DNS servers should be reset in systemd-resolved, to get reliable results.\nFlush systemd-resolved caches & state and confirm DNSSEC is enabled:\n$ sudo resolvectl flush-caches\n$ sudo resolvectl reset-server-features\n$ sudo resolvectl dnssec eth0 yes\n$ resolvectl dnssec\nGlobal: no\nLink 44 (eth0): yes\nQuery a DNSSEC enabled domain and confirm the data was fetched from the network and authenticated, as displayed by the\nData\nis\nauthenticated\nand\nData\nfrom\nfields:\n$ resolvectl query --type=MX isc.org\nisc.org IN MX 5 mx.pao1.isc.org                             -- link: eth0\nisc.org IN MX 10 mx.ams1.isc.org                            -- link: eth0\n\n-- Information acquired via protocol DNS in 3.0ms.\n-- Data is authenticated: yes; Data was acquired via local or encrypted transport: no\n-- Data from: network\nIncorrect time\n¶\nAs with everything related to cryptography, having an accurate measurement of time is of crucial importance. In a nutshell, digital signatures and keys have expiration dates.\nAn RRSIG record (a digital signature of a Resource Record) has a validity. For example, this record:\nnoble.example.internal. 86400 IN RRSIG A 13 3 86400 (\n                                20241106131533 20241023195023 48112 example.internal.\n                                5fL4apIwCD9kt4XbzzlLxMXY3mj8Li1WZu3qzlcBpERp\n                                lXPgLODbRrWyp7L81xEFnfhecKtEYv+6Y0Xa5iVRug== )\nHas this validity range:\nvalid until: 20241106131533 (2024-11-06 13:15:33 UTC)\nvalid since: 20241023195023 (2024-10-23 19:50:23 UTC)\nIf the DNSSEC validator has an incorrect clock, outside of the validity range, the DNSSEC validation will fail. For example, with the clock incorrectly set to before the beginning of the validity period,\ndelv\nwill complain like this:\n$ date\nTue Oct 10 10:10:19 UTC 2000\n\n$ delv @10.10.17.229 -a example.internal.key +root=example.internal +multiline noble.example.internal\n;; validating example.internal/DNSKEY: verify failed due to bad signature (keyid=48112): RRSIG validity period has not begun\n;; validating example.internal/DNSKEY: no valid signature found (DS)\n;; no valid RRSIG resolving 'example.internal/DNSKEY/IN': 10.10.17.229#53\n;; broken trust chain resolving 'noble.example.internal/A/IN': 10.10.17.229#53\n;; resolution failed: broken trust chain\nAny other Validating Resolver will fail in a similar way, and should indicate this error in its logs.\nBIND9 will complain loudly if it’s running on a system with an incorrect clock, as the root zones will fail validation:\nnamed[3593]: managed-keys-zone: DNSKEY set for zone '.' could not be verified with current keys\nnamed[3593]:   validating ./DNSKEY: verify failed due to bad signature (keyid=20326): RRSIG validity period has not begun\nnamed[3593]:   validating ./DNSKEY: no valid signature found (DS)\nnamed[3593]: broken trust chain resolving './NS/IN': 199.7.83.42#53\nnamed[3593]: resolver priming query complete: broken trust chain\nThird-party Web-based diagnostics\n¶\nThere are some public third-party web-based tools that will check the status of DNSSEC of a public domain. Here are some:\nhttps://dnsviz.net\n: Returns a graphical diagram showing the chain of trust and where it breaks down, if that’s the case.\nhttps://dnssec-debugger.verisignlabs.com\n: A DNSSEC debugger which also shows the chain of trust and where it breaks down, in a table format.\nFurther reading\n¶\nbind9’s guide to DNSSEC troubleshooting\nHow to validate DNSSEC using the command line\ndelv(1)\nmanpage\ndig(1)\nmanpage", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:20Z", "original_len_words": 2047}}
{"id": "dabbea026e", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/dpdk-with-open-vswitch/", "title": "How to use Open vSwitch with DPDK - Ubuntu Server documentation", "text": "How to use Open vSwitch with DPDK - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to use Open vSwitch with DPDK\n¶\nSince\nDPDK is\njust\na library\n, it doesn’t do a lot on its own so it depends on emerging projects making use of it. One consumer of the library that is already part of Ubuntu is Open vSwitch with DPDK (OvS-DPDK) support in the package\nopenvswitch-switch-dpdk\n.\nHere is a brief example of how to install and configure a basic Open vSwitch using DPDK for later use via\nlibvirt\n/\nqemu-kvm\n.\nsudo\napt\n-\nget\ninstall\nopenvswitch\n-\nswitch\n-\ndpdk\nsudo\nupdate\n-\nalternatives\n--\nset\novs\n-\nvswitchd\n/\nusr\n/\nlib\n/\nopenvswitch\n-\nswitch\n-\ndpdk\n/\novs\n-\nvswitchd\n-\ndpdk\novs\n-\nvsctl\nset\nOpen_vSwitch\n.\n\"other_config:dpdk-init=true\"\n# run on core 0 only\novs\n-\nvsctl\nset\nOpen_vSwitch\n.\n\"other_config:dpdk-lcore-mask=0x1\"\n# Allocate 2G huge pages (not Numa node aware)\novs\n-\nvsctl\nset\nOpen_vSwitch\n.\n\"other_config:dpdk-alloc-mem=2048\"\n# limit to one whitelisted device\novs\n-\nvsctl\nset\nOpen_vSwitch\n.\n\"other_config:dpdk-extra=--pci-whitelist=0000:04:00.0\"\nsudo\nservice\nopenvswitch\n-\nswitch\nrestart\nNote\nYou need to assign devices to DPDK-compatible drivers before restarting –  see the DPDK section on\nunassigning the default kernel drivers\n.\nPlease note that the section\n_dpdk-alloc-mem=2048_\nin the above example is the most basic non-uniform memory access (NUMA) setup for a single socket system. If you have multiple sockets you may want to define how the memory should be split among them. More details about these options are outlined in\nOpen vSwitch setup\n.\nAttach DPDK ports to Open vSwitch\n¶\nThe Open vSwitch you started above supports all the same port types as Open vSwitch usually does,\nplus\nDPDK port types. The following example shows how to create a bridge and – instead of a normal external port – add an external DPDK port to it. When doing so you can specify the associated device.\novs\n-\nvsctl\nadd\n-\nbr\novsdpdkbr0\n--\nset\nbridge\novsdpdkbr0\ndatapath_type\n=\nnetdev\novs\n-\nvsctl\nadd\n-\nport\novsdpdkbr0\ndpdk0\n--\nset\nInterface\ndpdk0\ntype\n=\ndpdk\n\"options:dpdk-devargs=$\n{OVSDEV_PCIID}\n\"\nYou can tune this further by setting options:\novs\n-\nvsctl\nset\nInterface\ndpdk0\n\"options:n_rxq=2\"\nOpen vSwitch DPDK to KVM guests\n¶\nIf you are not building some sort of software-defined networking (SDN) switch or NFV on top of DPDK, it is very likely that you want to forward traffic to KVM guests. The good news is; with the new\nqemu\n/\nlibvirt\n/\ndpdk\n/\nopenvswitch\nversions in Ubuntu this is no longer about manually appending a command line string. This section demonstrates a basic setup to connect a KVM guest to an Open vSwitch DPDK instance.\nThe recommended way to get to a KVM guest is using\nvhost_user_client\n. This will cause OvS-DPDK to connect to a socket created by QEMU. In this way, we can avoid old issues like “guest failures on OvS restart”. Here is an example of how to add such a port to the bridge you created above.\novs\n-\nvsctl\nadd\n-\nport\novsdpdkbr0\nvhost\n-\nuser\n-\n1\n--\nset\nInterface\nvhost\n-\nuser\n-\n1\ntype\n=\ndpdkvhostuserclient\n\"options:vhost-server-path=/var/run/vhostuserclient/vhost-user-client-1\"\nThis will connect to the specified path that has to be created by a guest listening for it.\nTo let\nlibvirt\n/\nkvm\nconsume this socket and create a guest VirtIO network device for it, add the following snippet to your guest definition as the network definition.\n<\ninterface\ntype\n=\n'vhostuser'\n>\n<\nsource\ntype\n=\n'unix'\npath\n=\n'/var/run/vhostuserclient/vhost-user-client-1'\nmode\n=\n'server'\n/>\n<\nmodel\ntype\n=\n'virtio'\n/>\n</\ninterface\n>\nTuning Open vSwitch-DPDK\n¶\nDPDK has plenty of options – in combination with Open vSwitch-DPDK the two most commonly used are:\novs\n-\nvsctl\nset\nOpen_vSwitch\n.\nother_config\n:\nn\n-\ndpdk\n-\nrxqs\n=\n2\novs\n-\nvsctl\nset\nOpen_vSwitch\n.\nother_config\n:\npmd\n-\ncpu\n-\nmask\n=\n0x6\nThe first line selects how many Rx Queues are to be used for each DPDK interface, while the second controls how many poll mode driver (PMD) threads to run (and where to run them). The example above will use two Rx Queues, and run PMD threads on CPU 1 and 2.\nSee also\nCheck the links to\nEAL\nCommand-line Options and “Open vSwitch DPDK installation” at the end of this document for more information.\nAs usual with tunings, you need to know your system and workload really well - so please verify any tunings with workloads matching your real use case.\nSupport and troubleshooting\n¶\nDPDK is a fast-evolving project. In any search for support and/or further guides, we highly recommended first checking to see if they apply to the current version.\nYou can check if your issues is known on:\nDPDK Mailing Lists\nFor OpenVswitch-DPDK\nOpenStack Mailing Lists\nKnown issues in\nDPDK Launchpad Area\nJoin the IRC channels #DPDK or #openvswitch on\nfreenode\n.\nIssues are often due to missing small details in the general setup. Later on, these missing details cause problems which can be hard to track down to their root cause.\nA common case seems to be the “could not open network device dpdk0 (No such device)” issue. This occurs rather late when setting up a port in Open vSwitch with DPDK, but the root cause (most of the time) is very early in the setup and initialization. Here is an example of how proper initialization of a device looks - this can be found in the\nsyslog/journal\nwhen starting Open vSwitch with DPDK enabled.\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\ndevice\n0000\n:\n04\n:\n00.1\non\nNUMA\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nprobe\ndriver\n:\n8086\n:\n1528\nrte_ixgbe_pmd\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\nmemory\nmapped\nat\n0x7f2140000000\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\nmemory\nmapped\nat\n0x7f2140200000\nIf this is missing, either by ignored cards, failed initialisation or other reasons, later on there will be no DPDK device to refer to. Unfortunately, the logging is spread across\nsyslog/journal\nand the\nopenvswitch\nlog. To enable some cross-checking, here is an example of what can be found in these logs, relative to the entered command.\n#Note: This log was taken with dpdk 2.2 and openvswitch 2.5 but still looks quite similar (a bit extended) these days\nCaptions\n:\nCMD\n:\nthat\nyou\nenter\nSYSLOG\n:\n(\nInlcuding\nEAL\nand\nOVS\nMessages\n)\nOVS\n-\nLOG\n:\n(\nOpenvswitch\nmessages\n)\n#PREPARATION\nBind\nan\ninterface\nto\nDPDK\nUIO\ndrivers\n,\nmake\nHugepages\navailable\n,\nenable\nDPDK\non\nOVS\nCMD\n:\nsudo\nservice\nopenvswitch\n-\nswitch\nrestart\nSYSLOG\n:\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n31.372\nZ\n|\n00003\n|\ndaemon_unix\n(\nmonitor\n)\n|\nINFO\n|\npid\n3329\ndied\n,\nkilled\n(\nTerminated\n),\nexiting\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n33.377\nZ\n|\n00002\n|\nvlog\n|\nINFO\n|\nopened\nlog\nfile\n/\nvar\n/\nlog\n/\nopenvswitch\n/\novs\n-\nvswitchd\n.\nlog\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n33.381\nZ\n|\n00003\n|\novs_numa\n|\nINFO\n|\nDiscovered\n12\nCPU\ncores\non\nNUMA\nnode\n0\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n33.381\nZ\n|\n00004\n|\novs_numa\n|\nINFO\n|\nDiscovered\n1\nNUMA\nnodes\nand\n12\nCPU\ncores\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n33.381\nZ\n|\n00005\n|\nreconnect\n|\nINFO\n|\nunix\n:\n/\nvar\n/\nrun\n/\nopenvswitch\n/\ndb\n.\nsock\n:\nconnecting\n...\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n33.383\nZ\n|\n00006\n|\nreconnect\n|\nINFO\n|\nunix\n:\n/\nvar\n/\nrun\n/\nopenvswitch\n/\ndb\n.\nsock\n:\nconnected\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n33.386\nZ\n|\n00007\n|\nbridge\n|\nINFO\n|\novs\n-\nvswitchd\n(\nOpen\nvSwitch\n)\n2.5.0\nOVS\n-\nLOG\n:\nsystemd\n[\n1\n]:\nStopping\nOpen\nvSwitch\n...\nsystemd\n[\n1\n]:\nStopped\nOpen\nvSwitch\n.\nsystemd\n[\n1\n]:\nStopping\nOpen\nvSwitch\nInternal\nUnit\n...\novs\n-\nctl\n[\n3541\n]:\n*\nKilling\novs\n-\nvswitchd\n(\n3329\n)\novs\n-\nctl\n[\n3541\n]:\n*\nKilling\novsdb\n-\nserver\n(\n3318\n)\nsystemd\n[\n1\n]:\nStopped\nOpen\nvSwitch\nInternal\nUnit\n.\nsystemd\n[\n1\n]:\nStarting\nOpen\nvSwitch\nInternal\nUnit\n...\novs\n-\nctl\n[\n3560\n]:\n*\nStarting\novsdb\n-\nserver\novs\n-\nvsctl\n:\novs\n|\n00001\n|\nvsctl\n|\nINFO\n|\nCalled\nas\novs\n-\nvsctl\n--\nno\n-\nwait\n--\ninit\n--\nset\nOpen_vSwitch\n.\ndb\n-\nversion\n=\n7.12.1\novs\n-\nvsctl\n:\novs\n|\n00001\n|\nvsctl\n|\nINFO\n|\nCalled\nas\novs\n-\nvsctl\n--\nno\n-\nwait\nset\nOpen_vSwitch\n.\novs\n-\nversion\n=\n2.5.0\n\"external-ids:system-id=\n\\\"\ne7c5ba80-bb14-45c1-b8eb-628f3ad03903\n\\\"\n\"\n\"system-type=\n\\\"\nUbuntu\n\\\"\n\"\n\"system-version=\n\\\"\n16.04-xenial\n\\\"\n\"\novs\n-\nctl\n[\n3560\n]:\n*\nConfiguring\nOpen\nvSwitch\nsystem\nIDs\novs\n-\nctl\n[\n3560\n]:\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n31\nZ\n|\n00001\n|\ndpdk\n|\nINFO\n|\nNo\n-\nvhost_sock_dir\nprovided\n-\ndefaulting\nto\n/\nvar\n/\nrun\n/\nopenvswitch\novs\n-\nvswitchd\n:\novs\n|\n00001\n|\ndpdk\n|\nINFO\n|\nNo\n-\nvhost_sock_dir\nprovided\n-\ndefaulting\nto\n/\nvar\n/\nrun\n/\nopenvswitch\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n0\nas\ncore\n0\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n1\nas\ncore\n1\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n2\nas\ncore\n2\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n3\nas\ncore\n3\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n4\nas\ncore\n4\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n5\nas\ncore\n5\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n6\nas\ncore\n0\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n7\nas\ncore\n1\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n8\nas\ncore\n2\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n9\nas\ncore\n3\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n10\nas\ncore\n4\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\nlcore\n11\nas\ncore\n5\non\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nSupport\nmaximum\n128\nlogical\ncore\n(\ns\n)\nby\nconfiguration\n.\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nDetected\n12\nlcore\n(\ns\n)\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nVFIO\nmodules\nnot\nall\nloaded\n,\nskip\nVFIO\nsupport\n...\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nSetting\nup\nphysically\ncontiguous\nmemory\n...\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nAsk\na\nvirtual\narea\nof\n0x100000000\nbytes\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nVirtual\narea\nfound\nat\n0x7f2040000000\n(\nsize\n=\n0x100000000\n)\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nRequesting\n4\npages\nof\nsize\n1024\nMB\nfrom\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nTSC\nfrequency\nis\n~\n2397202\nKHz\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nTSC\nfrequency\nis\n~\n2397202\nKHz\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nMaster\nlcore\n0\nis\nready\n(\ntid\n=\nfc6cbb00\n;\ncpuset\n=\n[\n0\n])\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nPCI\ndevice\n0000\n:\n04\n:\n00.0\non\nNUMA\nsocket\n0\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nprobe\ndriver\n:\n8086\n:\n1528\nrte_ixgbe_pmd\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nNot\nmanaged\nby\na\nsupported\nkernel\ndriver\n,\nskipped\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nPCI\ndevice\n0000\n:\n04\n:\n00.1\non\nNUMA\nsocket\n0\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nprobe\ndriver\n:\n8086\n:\n1528\nrte_ixgbe_pmd\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nPCI\nmemory\nmapped\nat\n0x7f2140000000\novs\n-\nvswitchd\n[\n3592\n]:\nEAL\n:\nPCI\nmemory\nmapped\nat\n0x7f2140200000\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nMaster\nlcore\n0\nis\nready\n(\ntid\n=\nfc6cbb00\n;\ncpuset\n=\n[\n0\n])\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\ndevice\n0000\n:\n04\n:\n00.0\non\nNUMA\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nprobe\ndriver\n:\n8086\n:\n1528\nrte_ixgbe_pmd\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nNot\nmanaged\nby\na\nsupported\nkernel\ndriver\n,\nskipped\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\ndevice\n0000\n:\n04\n:\n00.1\non\nNUMA\nsocket\n0\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nprobe\ndriver\n:\n8086\n:\n1528\nrte_ixgbe_pmd\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\nmemory\nmapped\nat\n0x7f2140000000\novs\n-\nctl\n[\n3560\n]:\nEAL\n:\nPCI\nmemory\nmapped\nat\n0x7f2140200000\novs\n-\nvswitchd\n[\n3592\n]:\nPMD\n:\neth_ixgbe_dev_init\n():\nMAC\n:\n4\n,\nPHY\n:\n3\novs\n-\nvswitchd\n[\n3592\n]:\nPMD\n:\neth_ixgbe_dev_init\n():\nport\n0\nvendorID\n=\n0x8086\ndeviceID\n=\n0x1528\novs\n-\nctl\n[\n3560\n]:\nPMD\n:\neth_ixgbe_dev_init\n():\nMAC\n:\n4\n,\nPHY\n:\n3\novs\n-\nctl\n[\n3560\n]:\nPMD\n:\neth_ixgbe_dev_init\n():\nport\n0\nvendorID\n=\n0x8086\ndeviceID\n=\n0x1528\novs\n-\nctl\n[\n3560\n]:\nZone\n0\n:\nname\n:\n<\nRG_MP_log_history\n>\n,\nphys\n:\n0x83fffdec0\n,\nlen\n:\n0x2080\n,\nvirt\n:\n0x7f213fffdec0\n,\nsocket_id\n:\n0\n,\nflags\n:\n0\novs\n-\nctl\n[\n3560\n]:\nZone\n1\n:\nname\n:\n<\nMP_log_history\n>\n,\nphys\n:\n0x83fd73d40\n,\nlen\n:\n0x28a0c0\n,\nvirt\n:\n0x7f213fd73d40\n,\nsocket_id\n:\n0\n,\nflags\n:\n0\novs\n-\nctl\n[\n3560\n]:\nZone\n2\n:\nname\n:\n<\nrte_eth_dev_data\n>\n,\nphys\n:\n0x83fd43380\n,\nlen\n:\n0x2f700\n,\nvirt\n:\n0x7f213fd43380\n,\nsocket_id\n:\n0\n,\nflags\n:\n0\novs\n-\nctl\n[\n3560\n]:\n*\nStarting\novs\n-\nvswitchd\novs\n-\nctl\n[\n3560\n]:\n*\nEnabling\nremote\nOVSDB\nmanagers\nsystemd\n[\n1\n]:\nStarted\nOpen\nvSwitch\nInternal\nUnit\n.\nsystemd\n[\n1\n]:\nStarting\nOpen\nvSwitch\n...\nsystemd\n[\n1\n]:\nStarted\nOpen\nvSwitch\n.\nCMD\n:\nsudo\novs\n-\nvsctl\nadd\n-\nbr\novsdpdkbr0\n--\nset\nbridge\novsdpdkbr0\ndatapath_type\n=\nnetdev\nSYSLOG\n:\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.344\nZ\n|\n00008\n|\nmemory\n|\nINFO\n|\n37256\nkB\npeak\nresident\nset\nsize\nafter\n24.5\nseconds\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00009\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nDatapath\nsupports\nrecirculation\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00010\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nMPLS\nlabel\nstack\nlength\nprobed\nas\n3\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00011\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nDatapath\nsupports\nunique\nflow\nids\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00012\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nDatapath\ndoes\nnot\nsupport\nct_state\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00013\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nDatapath\ndoes\nnot\nsupport\nct_zone\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00014\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nDatapath\ndoes\nnot\nsupport\nct_mark\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.346\nZ\n|\n00015\n|\nofproto_dpif\n|\nINFO\n|\nnetdev\n@ovs\n-\nnetdev\n:\nDatapath\ndoes\nnot\nsupport\nct_label\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.360\nZ\n|\n00016\n|\nbridge\n|\nINFO\n|\nbridge\novsdpdkbr0\n:\nadded\ninterface\novsdpdkbr0\non\nport\n65534\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.361\nZ\n|\n00017\n|\nbridge\n|\nINFO\n|\nbridge\novsdpdkbr0\n:\nusing\ndatapath\nID\n00005\na4a1ed0a14d\n2016\n-\n01\n-\n22\nT08\n:\n58\n:\n56.361\nZ\n|\n00018\n|\nconnmgr\n|\nINFO\n|\novsdpdkbr0\n:\nadded\nservice\ncontroller\n\"punix:/var/run/openvswitch/ovsdpdkbr0.mgmt\"\nOVS\n-\nLOG\n:\novs\n-\nvsctl\n:\novs\n|\n00001\n|\nvsctl\n|\nINFO\n|\nCalled\nas\novs\n-\nvsctl\nadd\n-\nbr\novsdpdkbr0\n--\nset\nbridge\novsdpdkbr0\ndatapath_type\n=\nnetdev\nsystemd\n-\nudevd\n[\n3607\n]:\nCould\nnot\ngenerate\npersistent\nMAC\naddress\nfor\novs\n-\nnetdev\n:\nNo\nsuch\nfile\nor\ndirectory\nkernel\n:\n[\n50165.886554\n]\ndevice\novs\n-\nnetdev\nentered\npromiscuous\nmode\nkernel\n:\n[\n50165.901261\n]\ndevice\novsdpdkbr0\nentered\npromiscuous\nmode\nCMD\n:\nsudo\novs\n-\nvsctl\nadd\n-\nport\novsdpdkbr0\ndpdk0\n--\nset\nInterface\ndpdk0\ntype\n=\ndpdk\nSYSLOG\n:\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n06.369\nZ\n|\n00019\n|\nmemory\n|\nINFO\n|\npeak\nresident\nset\nsize\ngrew\n155\n%\nin\nlast\n10.0\nseconds\n,\nfrom\n37256\nkB\nto\n95008\nkB\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n06.369\nZ\n|\n00020\n|\nmemory\n|\nINFO\n|\nhandlers\n:\n4\nports\n:\n1\nrevalidators\n:\n2\nrules\n:\n5\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n30.989\nZ\n|\n00021\n|\ndpdk\n|\nINFO\n|\nPort\n0\n:\n8\nc\n:\ndc\n:\nd4\n:\nb3\n:\n6\nd\n:\ne9\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n31.520\nZ\n|\n00022\n|\ndpdk\n|\nINFO\n|\nPort\n0\n:\n8\nc\n:\ndc\n:\nd4\n:\nb3\n:\n6\nd\n:\ne9\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n31.521\nZ\n|\n00023\n|\ndpif_netdev\n|\nINFO\n|\nCreated\n1\npmd\nthreads\non\nnuma\nnode\n0\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n31.522\nZ\n|\n00001\n|\ndpif_netdev\n(\npmd16\n)\n|\nINFO\n|\nCore\n0\nprocessing\nport\n'dpdk0'\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n31.522\nZ\n|\n00024\n|\nbridge\n|\nINFO\n|\nbridge\novsdpdkbr0\n:\nadded\ninterface\ndpdk0\non\nport\n1\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n31.522\nZ\n|\n00025\n|\nbridge\n|\nINFO\n|\nbridge\novsdpdkbr0\n:\nusing\ndatapath\nID\n00008\ncdcd4b36de9\n2016\n-\n01\n-\n22\nT08\n:\n59\n:\n31.523\nZ\n|\n00002\n|\ndpif_netdev\n(\npmd16\n)\n|\nINFO\n|\nCore\n0\nprocessing\nport\n'dpdk0'\nOVS\n-\nLOG\n:\novs\n-\nvsctl\n:\novs\n|\n00001\n|\nvsctl\n|\nINFO\n|\nCalled\nas\novs\n-\nvsctl\nadd\n-\nport\novsdpdkbr0\ndpdk0\n--\nset\nInterface\ndpdk0\ntype\n=\ndpdk\novs\n-\nvswitchd\n[\n3595\n]:\nPMD\n:\nixgbe_dev_tx_queue_setup\n():\nsw_ring\n=\n0x7f211a79ebc0\nhw_ring\n=\n0x7f211a7a6c00\ndma_addr\n=\n0x81a7a6c00\novs\n-\nvswitchd\n[\n3595\n]:\nPMD\n:\nixgbe_set_tx_function\n():\nUsing\nsimple\ntx\ncode\npath\novs\n-\nvswitchd\n[\n3595\n]:\nPMD\n:\nixgbe_set_tx_function\n():\nVector\ntx\nenabled\n.\novs\n-\nvswitchd\n[\n3595\n]:\nPMD\n:\nixgbe_dev_rx_queue_setup\n():\nsw_ring\n=\n0x7f211a78a6c0\nsw_sc_ring\n=\n0x7f211a786580\nhw_ring\n=\n0x7f211a78e800\ndma_addr\n=\n0x81a78e800\novs\n-\nvswitchd\n[\n3595\n]:\nPMD\n:\nixgbe_set_rx_function\n():\nVector\nrx\nenabled\n,\nplease\nmake\nsure\nRX\nburst\nsize\nno\nless\nthan\n4\n(\nport\n=\n0\n)\n.\novs\n-\nvswitchd\n[\n3595\n]:\nPMD\n:\nixgbe_dev_tx_queue_setup\n():\nsw_ring\n=\n0x7f211a79ebc0\nhw_ring\n=\n0x7f211a7a6c00\ndma_addr\n=\n0x81a7a6c00\n...\nCMD\n:\nsudo\novs\n-\nvsctl\nadd\n-\nport\novsdpdkbr0\nvhost\n-\nuser\n-\n1\n--\nset\nInterface\nvhost\n-\nuser\n-\n1\ntype\n=\ndpdkvhostuser\nOVS\n-\nLOG\n:\n2016\n-\n01\n-\n22\nT09\n:\n00\n:\n35.145\nZ\n|\n00026\n|\ndpdk\n|\nINFO\n|\nSocket\n/\nvar\n/\nrun\n/\nopenvswitch\n/\nvhost\n-\nuser\n-\n1\ncreated\nfor\nvhost\n-\nuser\nport\nvhost\n-\nuser\n-\n1\n2016\n-\n01\n-\n22\nT09\n:\n00\n:\n35.145\nZ\n|\n00003\n|\ndpif_netdev\n(\npmd16\n)\n|\nINFO\n|\nCore\n0\nprocessing\nport\n'dpdk0'\n2016\n-\n01\n-\n22\nT09\n:\n00\n:\n35.145\nZ\n|\n00004\n|\ndpif_netdev\n(\npmd16\n)\n|\nINFO\n|\nCore\n0\nprocessing\nport\n'vhost-user-1'\n2016\n-\n01\n-\n22\nT09\n:\n00\n:\n35.145\nZ\n|\n00027\n|\nbridge\n|\nINFO\n|\nbridge\novsdpdkbr0\n:\nadded\ninterface\nvhost\n-\nuser\n-\n1\non\nport\n2\nSYSLOG\n:\novs\n-\nvsctl\n:\novs\n|\n00001\n|\nvsctl\n|\nINFO\n|\nCalled\nas\novs\n-\nvsctl\nadd\n-\nport\novsdpdkbr0\nvhost\n-\nuser\n-\n1\n--\nset\nInterface\nvhost\n-\nuser\n-\n1\ntype\n=\ndpdkvhostuser\novs\n-\nvswitchd\n[\n3595\n]:\nVHOST_CONFIG\n:\nsocket\ncreated\n,\nfd\n:\n46\novs\n-\nvswitchd\n[\n3595\n]:\nVHOST_CONFIG\n:\nbind\nto\n/\nvar\n/\nrun\n/\nopenvswitch\n/\nvhost\n-\nuser\n-\n1\nEventually\nwe\ncan\nsee\nthe\npoll\nthread\nin\ntop\nPID\nUSER\nPR\nNI\nVIRT\nRES\nSHR\nS\n%\nCPU\n%\nMEM\nTIME\n+\nCOMMAND\n3595\nroot\n10\n-\n10\n4975344\n103936\n9916\nS\n100.0\n0.3\n33\n:\n13.56\novs\n-\nvswitchd\nResources\n¶\nDPDK documentation\nRelease Notes matching the version packages in Ubuntu 16.04\nLinux DPDK user getting started\nEAL command-line options\nDPDK API documentation\nOpen Vswitch DPDK installation\nWikipedia’s definition of DPDK", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:20Z", "original_len_words": 3556}}
{"id": "0b9f44483a", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/ftp/", "title": "Set up an FTP server - Ubuntu Server documentation", "text": "Set up an FTP server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSet up an FTP server\n¶\nFile Transfer Protocol (FTP) is a TCP protocol for downloading files between computers. In the past, it has also been used for uploading but, as that method does not use encryption, user credentials as well as data transferred in the clear and are easily intercepted. So if you are here looking for a way to upload and download files securely, see the\nOpenSSH documentation\ninstead.\nFTP works on a client/server model. The server component is called an\nFTP daemon\n. It continuously listens for FTP requests from remote clients. When a request is received, it manages the login and sets up the connection. For the duration of the session it executes any of commands sent by the FTP client.\nAccess to an FTP server can be managed in two ways:\nAnonymous\nAuthenticated\nIn the Anonymous mode, remote clients can access the FTP server by using the default user account called “anonymous” or “ftp” and sending an email address as the password. In the Authenticated mode a user must have an account and a password. This latter choice is very insecure and should not be used except in special circumstances. If you are looking to transfer files securely see SFTP in the section on OpenSSH-Server. User access to the FTP server directories and files is dependent on the permissions defined for the account used at login. As a general rule, the FTP daemon will hide the root directory of the FTP server and change it to the FTP Home directory. This hides the rest of the file system from remote sessions.\nvsftpd - FTP Server Installation\n¶\nvsftpd is an FTP daemon available in Ubuntu. It is easy to install, set up, and maintain. To install vsftpd you can run the following command:\nsudo apt install vsftpd\nAnonymous FTP Configuration\n¶\nBy default vsftpd is\nnot\nconfigured to allow anonymous download. If you wish to enable anonymous download edit\n/etc/vsftpd.conf\nby changing:\nanonymous_enable=YES\nDuring installation a\nftp\nuser is created with a home directory of\n/srv/ftp\n. This is the default FTP directory.\nIf you wish to change this location, to\n/srv/files/ftp\nfor example, simply create a directory in another location and change the\nftp\nuser’s home directory:\nsudo mkdir -p /srv/files/ftp\nsudo usermod -d /srv/files/ftp ftp\nAfter making the change restart vsftpd:\nsudo systemctl restart vsftpd.service\nFinally, copy any files and directories you would like to make available through anonymous FTP to\n/srv/files/ftp\n, or\n/srv/ftp\nif you wish to use the default.\nUser Authenticated FTP Configuration\n¶\nBy default vsftpd is configured to authenticate system users and allow them to download files. If you want users to be able to upload files, edit\n/etc/vsftpd.conf\n:\nwrite_enable=YES\nNow restart vsftpd:\nsudo systemctl restart vsftpd.service\nNow when system users login to FTP they will start in their\nhome\ndirectories where they can download, upload, create directories, etc.\nSimilarly, by default, anonymous users are not allowed to upload files to FTP server. To change this setting, you should uncomment the following line, and restart vsftpd:\nanon_upload_enable=YES\nWarning\nEnabling anonymous FTP upload can be an extreme security risk. It is best to not enable anonymous upload on servers accessed directly from the Internet.\nThe configuration file consists of many configuration parameters. The information about each parameter is available in the configuration file. Alternatively, you can refer to the man page,\nman\n5\nvsftpd.conf\nfor details of each parameter.\nSecuring FTP\n¶\nThere are options in\n/etc/vsftpd.conf\nto help make vsftpd more secure. For example users can be limited to their home directories by uncommenting:\nchroot_local_user=YES\nYou can also limit a specific list of users to just their home directories:\nchroot_list_enable=YES\nchroot_list_file=/etc/vsftpd.chroot_list\nWarning\nIf chroot_local_user is set to YES (setting all users to be limited a to just their home directories), /etc/vsftpd.chroot_list becomes a list of users which are NOT limited a to just their home directories\nAfter uncommenting the above options, create a\n/etc/vsftpd.chroot_list\ncontaining a list of users one per line. Then restart vsftpd:\nsudo systemctl restart vsftpd.service\nAlso, the\n/etc/ftpusers\nfile is a list of users that are\ndisallowed\nFTP access. The default list includes root, daemon, nobody, etc. To disable FTP access for additional users simply add them to the list.\nFTP can also be encrypted using\nFTPS\n. Different from\nSFTP\n,\nFTPS\nis FTP over Secure Socket Layer (SSL).\nSFTP\nis a FTP like session over an encrypted\nSSH\nconnection. A major difference is that users of SFTP need to have a\nshell\naccount on the system, instead of a\nnologin\nshell. Providing all users with a shell may not be ideal for some environments, such as a shared web host. However, it is possible to restrict such accounts to only SFTP and disable shell interaction.\nTo configure\nFTPS\n, edit\n/etc/vsftpd.conf\nand at the bottom add:\nssl_enable=YES\nAlso, notice the certificate and key related options:\nrsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem\nrsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key\nBy default these options are set to the certificate and key provided by the ssl-cert package. In a production environment these should be replaced with a certificate and key generated for the specific host. For more information on certificates see\nSecurity - Certificates\n.\nNow restart vsftpd, and non-anonymous users will be forced to use\nFTPS\n:\nsudo systemctl restart vsftpd.service\nTo allow users with a shell of\n/usr/sbin/nologin\naccess to FTP, but have no shell access, edit\n/etc/shells\nadding the\nnologin\nshell:\n# /etc/shells: valid login shells\n/bin/csh\n/bin/sh\n/usr/bin/es\n/usr/bin/ksh\n/bin/ksh\n/usr/bin/rc\n/usr/bin/tcsh\n/bin/tcsh\n/usr/bin/esh\n/bin/dash\n/bin/bash\n/bin/rbash\n/usr/bin/screen\n/usr/sbin/nologin\nThis is necessary because, by default vsftpd uses PAM for authentication, and the\n/etc/pam.d/vsftpd\nconfiguration file contains:\nauth    required        pam_shells.so\nThe\nshells\nPAM module restricts access to shells listed in the\n/etc/shells\nfile.\nMost popular FTP clients can be configured to connect using FTPS. The lftp command line FTP client has the ability to use FTPS as well.\nReferences\n¶\nSee the\nvsftpd website\nfor more information.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:20Z", "original_len_words": 1020}}
{"id": "669d54db17", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/install-dns/", "title": "Domain Name Service (DNS) - Ubuntu Server documentation", "text": "Domain Name Service (DNS) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nDomain Name Service (DNS)\n¶\nDomain Name Service (DNS) is an Internet service that maps IP addresses and\nfully qualified domain names (FQDN)\nto one another. In this way, DNS alleviates the need to remember IP addresses. Computers that run DNS are called\nname servers\n. Ubuntu ships with the Berkley Internet Naming Daemon (BIND), the most common program used for maintaining a name server on Linux.\nInstall DNS\n¶\nAt a terminal prompt, run the following command to install the\nbind9\npackage:\nsudo\napt\ninstall\nbind9\nA useful package for testing and troubleshooting DNS issues is the\ndnsutils\npackage. Very often these tools will be installed already, but to check and/or install\ndnsutils\nenter the following:\nsudo\napt\ninstall\ndnsutils\nDNS configuration overview\n¶\nThere are many ways to configure BIND9. Some of the most common configurations include a caching nameserver, primary server, and secondary server.\nWhen configured as a\ncaching nameserver\n, BIND9 will find the answer to name queries and remember the answer when the domain is queried again.\nAs a\nprimary server\n, BIND9 reads the data for a zone from a file on its host, and is authoritative for that zone.\nAs a\nsecondary server\n, BIND9 gets the zone data from another nameserver that is authoritative for the zone.\nConfiguration files\n¶\nThe DNS configuration files are stored in the\n/etc/bind\ndirectory. The primary configuration file is\n/etc/bind/named.conf\n, which in the layout provided by the package just includes these files:\n/etc/bind/named.conf.options\n: Global DNS options\n/etc/bind/named.conf.local\n: For your zones\n/etc/bind/named.conf.default-zones\n: Default zones such as localhost, its reverse, and the root hints\nThe root nameservers used to be described in the file\n/etc/bind/db.root\n. This is now provided instead by the\n/usr/share/dns/root.hints\nfile shipped with the\ndns-root-data\npackage, and is referenced in the\nnamed.conf.default-zones\nconfiguration file above.\nIt is possible to configure the same server to be a caching name server, primary, and secondary: it all depends on the zones it is serving. A server can be the Start of Authority (SOA) for one zone, while providing secondary service for another zone. All the while providing caching services for hosts on the local LAN.\nSet up a caching nameserver\n¶\nThe default configuration acts as a caching server. Simply uncomment and edit\n/etc/bind/named.conf.options\nto set the IP addresses of your ISP’s DNS servers:\nforwarders\n{\n1.2.3.4\n;\n5.6.7.8\n;\n};\nNote\nReplace\n1.2.3.4\nand\n5.6.7.8\nwith the IP addresses of actual nameservers.\nTo enable the new configuration, restart the DNS server. From a terminal prompt, run:\nsudo\nsystemctl\nrestart\nbind9.service\nSee the dig section\nfor information on testing a caching DNS server.\nSet up a primary server\n¶\nIn this section BIND9 will be configured as the primary server for the domain\nexample.com\n. You can replace\nexample.com\nwith your FQDN (Fully Qualified Domain Name).\nForward zone file\n¶\nTo add a DNS zone to BIND9, turning BIND9 into a primary server, first edit\n/etc/bind/named.conf.local\n:\nzone\n\"example.com\"\n{\ntype\nmaster\n;\nfile\n\"/etc/bind/db.example.com\"\n;\n};\nNote\nIf BIND will be receiving automatic updates to the file as with\nDDNS\n, then use\n/var/lib/bind/db.example.com\nrather than\n/etc/bind/db.example.com\nboth here and in the copy command below.\nNow use an existing zone file as a template to create the\n/etc/bind/db.example.com\nfile:\nsudo\ncp\n/etc/bind/db.local\n/etc/bind/db.example.com\nEdit the new zone file\n/etc/bind/db.example.com\nand change\nlocalhost.\nto the FQDN of your server, including the additional\n.\nat the end. Change\n127.0.0.1\nto the nameserver’s IP address and\nroot.localhost\nto a valid email address, but with a\n.\ninstead of the usual\n@\nsymbol, again including the\n.\nat the end. Change the comment to indicate the domain that this file is for.\nCreate an\nA record\nfor the base domain,\nexample.com\n. Also, create an\nA record\nfor\nns.example.com\n, the name server in this example:\n;\n; BIND data file for example.com\n;\n$TTL    604800\n@       IN      SOA     example.com. root.example.com. (\n                              2         ; Serial\n                         604800         ; Refresh\n                          86400         ; Retry\n                        2419200         ; Expire\n                         604800 )       ; Negative Cache TTL\n\n@       IN      NS      ns.example.com.\n@       IN      A       192.168.1.10\n@       IN      AAAA    ::1\nns      IN      A       192.168.1.10\nYou must increment the\nSerial\nNumber\nevery time you make changes to the zone file. If you make multiple changes before restarting BIND9, only increment\nSerial\nonce.\nNow, you can add DNS records to the bottom of the zone file. See\nCommon Record Types\nfor details.\nNote\nMany admins like to use the “last edited” date as the Serial of a zone, such as\n2020012100\nwhich is\nyyyymmddss\n(where\nss\nis the Serial Number)\nOnce you have made changes to the zone file, BIND9 needs to be restarted for the changes to take effect:\nsudo\nsystemctl\nrestart\nbind9.service\nReverse zone file\n¶\nNow that the zone is set up and resolving names to IP Addresses, a\nreverse zone\nneeds to be added to allow DNS to resolve an address to a name.\nEdit\n/etc/bind/named.conf.local\nand add the following:\nzone\n\"1.168.192.in-addr.arpa\"\n{\ntype\nmaster\n;\nfile\n\"/etc/bind/db.192\"\n;\n};\nNote\nReplace\n1.168.192\nwith the first three octets of whatever network you are using. Also, name the zone file\n/etc/bind/db.192\nappropriately. It should match the first octet of your network.\nNow create the\n/etc/bind/db.192\nfile:\nsudo\ncp\n/etc/bind/db.127\n/etc/bind/db.192\nNext edit\n/etc/bind/db.192\n, changing the same options as\n/etc/bind/db.example.com\n:\n;\n; BIND reverse data file for local 192.168.1.XXX net\n;\n$TTL    604800\n@       IN      SOA     ns.example.com. root.example.com. (\n                              2         ; Serial\n                         604800         ; Refresh\n                          86400         ; Retry\n                        2419200         ; Expire\n                         604800 )       ; Negative Cache TTL\n;\n@       IN      NS      ns.\n10      IN      PTR     ns.example.com.\nThe\nSerial\nNumber\nin the reverse zone needs to be incremented on each change as well. For each\nA record\nyou configure in\n/etc/bind/db.example.com\nthat is for a different address, you will need to create a\nPTR record\nin\n/etc/bind/db.192\n.\nAfter creating the reverse zone file restart BIND9:\nsudo\nsystemctl\nrestart\nbind9.service\nSet up a secondary server\n¶\nOnce a primary server has been configured, a\nsecondary server\nis highly recommended. This will maintain the availability of the domain if the primary becomes unavailable.\nFirst, on the primary server, the zone transfer needs to be allowed. Add the\nallow-transfer\noption to the example\nForward\nand\nReverse\nzone definitions in\n/etc/bind/named.conf.local\n:\nzone\n\"example.com\"\n{\ntype\nmaster\n;\nfile\n\"/etc/bind/db.example.com\"\n;\nallow\n-\ntransfer\n{\n192.168.1.11\n;\n};\n};\nzone\n\"1.168.192.in-addr.arpa\"\n{\ntype\nmaster\n;\nfile\n\"/etc/bind/db.192\"\n;\nallow\n-\ntransfer\n{\n192.168.1.11\n;\n};\n};\nNote\nReplace\n192.168.1.11\nwith the IP address of your secondary nameserver.\nRestart BIND9 on the primary server:\nsudo\nsystemctl\nrestart\nbind9.service\nNext, on the secondary server, install the\nbind9\npackage the same way as on the primary. Then edit the\n/etc/bind/named.conf.local\nand add the following declarations for the Forward and Reverse zones:\nzone\n\"example.com\"\n{\ntype\nsecondary\n;\nfile\n\"db.example.com\"\n;\nmasters\n{\n192.168.1.10\n;\n};\n};\nzone\n\"1.168.192.in-addr.arpa\"\n{\ntype\nsecondary\n;\nfile\n\"db.192\"\n;\nmasters\n{\n192.168.1.10\n;\n};\n};\nOnce again, replace\n192.168.1.10\nwith the IP address of your primary nameserver, then restart BIND9 on the secondary server:\nsudo\nsystemctl\nrestart\nbind9.service\nIn\n/var/log/syslog\nyou should see something similar to the following (some lines have been split to fit the format of this document):\nclient 192.168.1.10#39448: received notify for zone '1.168.192.in-addr.arpa'\nzone 1.168.192.in-addr.arpa/IN: Transfer started.\ntransfer of '100.18.172.in-addr.arpa/IN' from 192.168.1.10#53:\n connected using 192.168.1.11#37531\nzone 1.168.192.in-addr.arpa/IN: transferred serial 5\ntransfer of '100.18.172.in-addr.arpa/IN' from 192.168.1.10#53:\n Transfer completed: 1 messages, \n6 records, 212 bytes, 0.002 secs (106000 bytes/sec)\nzone 1.168.192.in-addr.arpa/IN: sending notifies (serial 5)\n    \nclient 192.168.1.10#20329: received notify for zone 'example.com'\nzone example.com/IN: Transfer started.\ntransfer of 'example.com/IN' from 192.168.1.10#53: connected using 192.168.1.11#38577\nzone example.com/IN: transferred serial 5\ntransfer of 'example.com/IN' from 192.168.1.10#53: Transfer completed: 1 messages, \n8 records, 225 bytes, 0.002 secs (112500 bytes/sec)\nNote\nA zone is only transferred if the\nSerial\nNumber\non the primary is larger than the one on the secondary. If you want to have your primary DNS notify other secondary DNS servers of zone changes, you can add\nalso-notify\n{\nipaddress;\n};\nto\n/etc/bind/named.conf.local\nas shown in the example below:\nzone\n\"example.com\"\n{\ntype\nmaster\n;\nfile\n\"/etc/bind/db.example.com\"\n;\nallow\n-\ntransfer\n{\n192.168.1.11\n;\n};\nalso\n-\nnotify\n{\n192.168.1.11\n;\n};\n};\nzone\n\"1.168.192.in-addr.arpa\"\n{\ntype\nmaster\n;\nfile\n\"/etc/bind/db.192\"\n;\nallow\n-\ntransfer\n{\n192.168.1.11\n;\n};\nalso\n-\nnotify\n{\n192.168.1.11\n;\n};\n};\nNote\nThe default directory for non-authoritative zone files is\n/var/cache/bind/\n. This directory is also configured in AppArmor to allow the named daemon to write to it. See this page for\nmore information on AppArmor\n.\nTesting your setup\n¶\nresolv.conf\n¶\nThe first step in testing BIND9 is to add the nameserver’s IP address to a\nhosts resolver\n. The Primary nameserver should be configured as well as another host to double check things. Refer to\nDNS client configuration\nfor details on adding nameserver addresses to your network clients. In the end your\nnameserver\nline in\n/etc/resolv.conf\nshould be pointing at\n127.0.0.53\nand you should have a\nsearch\nparameter for your domain. Something like this:\nnameserver  127.0.0.53\nsearch example.com\nTo check which DNS server your local resolver is using, run:\nresolvectl\nstatus\nNote\nYou should also add the IP address of the secondary nameserver to your client configuration in case the primary becomes unavailable.\ndig\n¶\nIf you installed the\ndnsutils\npackage you can test your setup using the DNS lookup utility\ndig\n:\nAfter installing BIND9 use\ndig\nagainst the loopback interface to make sure it is listening on port 53. From a terminal prompt:\ndig\n-x\n127\n.0.0.1\nYou should see lines similar to the following in the command output:\n;;\nQuery\ntime\n:\n1\nmsec\n;;\nSERVER\n:\n192.168.1.10\n#53(192.168.1.10)\nIf you have configured BIND9 as a caching nameserver, “dig” an outside domain to check the query time:\ndig\nubuntu.com\nNote the query time toward the end of the command output:\n;;\nQuery\ntime\n:\n49\nmsec\nAfter a second\ndig\nthere should be improvement:\n;;\nQuery\ntime\n:\n1\nmsec\nping\n¶\nNow let’s demonstrate how applications make use of DNS to resolve a host name, by using the\nping\nutility to send an ICMP echo request:\nping\nexample.com\nThis tests if the nameserver can resolve the name\nns.example.com\nto an IP address. The command output should resemble:\nPING\nns\n.\nexample\n.\ncom\n(\n192.168.1.10\n)\n56\n(\n84\n)\nbytes\nof\ndata\n.\n64\nbytes\nfrom\n192.168.1.10\n:\nicmp_seq\n=\n1\nttl\n=\n64\ntime\n=\n0.800\nms\n64\nbytes\nfrom\n192.168.1.10\n:\nicmp_seq\n=\n2\nttl\n=\n64\ntime\n=\n0.813\nms\nnamed-checkzone\n¶\nA great way to test your zone files is by using the\nnamed-checkzone\nutility installed with the\nbind9\npackage. This utility allows you to make sure the configuration is correct before restarting BIND9 and making the changes live.\nTo test our example forward zone file, enter the following from a command prompt:\nnamed-checkzone\nexample.com\n/etc/bind/db.example.com\nIf everything is configured correctly you should see output similar to:\nzone\nexample\n.\ncom\n/\nIN\n:\nloaded\nserial\n6\nOK\nSimilarly, to test the reverse zone file enter the following:\nnamed\n-\ncheckzone\n1.168.192\n.\nin\n-\naddr\n.\narpa\n/\netc\n/\nbind\n/\ndb\n.192\nThe output should be similar to:\nzone\n1.168.192\n.\nin\n-\naddr\n.\narpa\n/\nIN\n:\nloaded\nserial\n3\nOK\nNote\nThe Serial Number of your zone file will probably be different.\nQuick temporary query logging\n¶\nWith the\nrndc\ntool, you can quickly turn query logging on and off, without restarting the service or changing the configuration file.\nTo turn query logging on, run:\nsudo\nrndc\nquerylog\non\nLikewise, to turn it off, run:\nsudo\nrndc\nquerylog\noff\nThe logs will be sent to\nsyslog\nand will show up in\n/var/log/syslog\nby default:\nJan\n20\n19\n:\n40\n:\n50\nnew\n-\nn1\nnamed\n[\n816\n]:\nreceived\ncontrol\nchannel\ncommand\n'querylog on'\nJan\n20\n19\n:\n40\n:\n50\nnew\n-\nn1\nnamed\n[\n816\n]:\nquery\nlogging\nis\nnow\non\nJan\n20\n19\n:\n40\n:\n57\nnew\n-\nn1\nnamed\n[\n816\n]:\nclient\n@\n0x7f48ec101480\n192.168.1.10\n#36139 (ubuntu.com): query: ubuntu.com IN A +E(0)K (192.168.1.10)\nNote\nThe amount of logs generated by enabling\nquerylog\ncould be huge!\nLogging\n¶\nBIND9 has a wide variety of logging configuration options available, but the two main ones are\nchannel\nand\ncategory\n, which configure where logs go, and what information gets logged, respectively.\nIf no logging options are configured the default configuration is:\nlogging\n{\ncategory\ndefault\n{\ndefault_syslog\n;\ndefault_debug\n;\n};\ncategory\nunmatched\n{\nnull\n;\n};\n};\nLet’s instead configure BIND9 to send\ndebug\nmessages related to DNS queries to a separate file.\nWe need to configure a\nchannel\nto specify which file to send the messages to, and a\ncategory\n. In this example, the category will log all queries. Edit\n/etc/bind/named.conf.local\nand add the following:\nlogging\n{\nchannel\nquery\n.\nlog\n{\nfile\n\"/var/log/named/query.log\"\n;\nseverity\ndebug\n3\n;\n};\ncategory\nqueries\n{\nquery\n.\nlog\n;\n};\n};\nNote\nThe\ndebug\noption can be set from 1 to 3. If a level isn’t specified, level 1 is the default.\nSince the\nnamed daemon\nruns as the\nbind\nuser, the\n/var/log/named\ndirectory must be created and the ownership changed:\nsudo\nmkdir\n/var/log/named\nsudo\nchown\nbind:bind\n/var/log/named\nNow restart BIND9 for the changes to take effect:\nsudo\nsystemctl\nrestart\nbind9.service\nYou should see the file\n/var/log/named/query.log\nfill with query information. This is a simple example of the BIND9 logging options. For coverage of advanced options see the “Further Reading” section at the bottom of this page.\nCommon record types\n¶\nThis section covers some of the most common DNS record types.\nA\nrecord\nThis record maps an IP address to a\nhostname\n.\nwww\nIN\nA\n192.168.1.12\nCNAME\nrecord\nUsed to create an alias to an existing A record. You cannot create a\nCNAME\nrecord pointing to another\nCNAME\nrecord.\nweb\nIN\nCNAME\nwww\nMX\nrecord\nUsed to define where emails should be sent to. Must point to an\nA\nrecord, not a\nCNAME\n.\n@\nIN\nMX\n1\nmail\n.\nexample\n.\ncom\n.\nmail\nIN\nA\n192.168.1.13\nNS\nrecord\nUsed to define which servers serve copies of a zone. It must point to an\nA\nrecord, not a\nCNAME\n. This is where primary and secondary servers are defined.\n@\nIN\nNS\nns\n.\nexample\n.\ncom\n.\n@\nIN\nNS\nns2\n.\nexample\n.\ncom\n.\nns\nIN\nA\n192.168.1.10\nns2\nIN\nA\n192.168.1.11\nFurther reading\n¶\nInstalling DNS Security Extensions (DNSSEC)\nUpstream BIND9 Documentation\nDNS and BIND\nis a popular book now in its fifth edition. There is now also a\nDNS and BIND on IPv6\nbook.\nA great place to ask for BIND9 assistance, and get involved with the Ubuntu Server community, is the\n#ubuntu-server\nIRC channel on\nLibera Chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:21Z", "original_len_words": 2517}}
{"id": "f8610465eb", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/install-dnssec/", "title": "Installing DNS Security Extensions (DNSSEC) - Ubuntu Server documentation", "text": "Installing DNS Security Extensions (DNSSEC) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstalling DNS Security Extensions (DNSSEC)\n¶\nDNSSEC is a set of security extensions to\nDNS\nwhich allow DNS data to be verified for authenticity and integrity.\nThis guide will show you how to enable DNSSEC for an existing zone in your BIND9 DNS server deployment, by serving additional records like\nDS\n,\nDNSKEY\nor\nRRSIG\nfor signed zones.\nStarting point\n¶\nThe starting point for this how-to is an existing BIND9 DNS server deployed with an authoritative zone. For details on how to deploy BIND9 in this fashion, please see the\nDNS How-To\n. One key difference from that guide, however, is that we need the zone file to be in a directory where the server can write to, like\n/var/lib/bind\n, instead of\n/etc/bind\n.\nNevertheless, here is a quick set of steps to reach that state for an example domain called\nexample.internal\n.\nFirst, install the\nbind9\npackage:\nsudo apt install bind9 -y\nEdit\n/etc/bind/named.conf.local\nand add this\nzone\nblock:\nzone \"example.internal\" {\n    type master;\n    file \"/var/lib/bind/db.example.internal\";\n};\nCreate the file\n/var/lib/bind/db.example.internal\nwith these contents:\n$TTL 86400      ; 1 day\nexample.internal.       IN SOA  example.internal. root.example.internal. (\n                                1          ; serial\n                                43200      ; refresh (12 hours)\n                                900        ; retry (15 minutes)\n                                1814400    ; expire (3 weeks)\n                                7200       ; minimum (2 hours)\n                                )\nexample.internal.       IN  NS      ns.example.internal.\nns                      IN  A       192.168.1.10\nnoble                   IN  A       192.168.1.11\nRestart the service:\nsudo systemctl restart named\nCheck if the service can resolve the name\nnoble.example.internal\n:\n$ dig @127.0.0.1 +short noble.example.internal\n192.168.1.11\nEnabling DNSSEC\n¶\nEnabling DNSSEC for a zone involves multiple steps. Thankfully for us, the BIND9 DNS server takes care of all of them by default, automatically, leaving very little for us to do. Converting a zone in this way means at least generating new keys, and signing all the Resource Records of the zone. But that is only “day 1”: such a zone must be maintained properly. Keys must be rotated, expiration dates must be set, etc. The current versions of BIND9 take care of that with a DNSSEC policy, and of course there is a default one that can be used from the start.\nTo migrate our\nexample.internal\nzone to DNSSEC, we just need to add two lines to its definition in\n/etc/bind/named.conf.local\n, so that it looks like this:\nzone \"example.internal\" {\n    type master;\n    file \"/var/lib/bind/db.example.internal\";\n    dnssec-policy default;\n    inline-signing yes;\n};\nWhat was added:\ndnssec-policy\ndefault\n: Use the default DNSSEC policy. A DNSSEC policy includes settings for key rotation, default TTL, and many others.\ninline-signing\nyes\n: keep a separate file for the signed zone.\nAfter this change, there is no need to restart the service, but it needs to be told to reload its configuration. This can be done with the\nrndc\ntool:\nsudo rndc reconfig\nThe server will immediately notice the new configuration and start the process to sign the\nexample.internal\nzone. The journal logs will show the progress, and can be inspected with the command:\nsudo journalctl -u named.service -f\nThe logs will show something similar to this:\nnamed[3246]: zone example.internal/IN (unsigned): loaded serial 1\nnamed[3246]: zone example.internal/IN (signed): loaded serial 1\nnamed[3246]: zone example.internal/IN (signed): receive_secure_serial: unchanged\nnamed[3246]: zone example.internal/IN (signed): sending notifies (serial 1)\nnamed[3246]: zone example.internal/IN (signed): reconfiguring zone keys\nnamed[3246]: keymgr: DNSKEY example.internal/ECDSAP256SHA256/44911 (CSK) created for policy default\nnamed[3246]: Fetching example.internal/ECDSAP256SHA256/44911 (CSK) from key repository.\nnamed[3246]: DNSKEY example.internal/ECDSAP256SHA256/44911 (CSK) is now published\nnamed[3246]: DNSKEY example.internal/ECDSAP256SHA256/44911 (CSK) is now active\nnamed[3246]: zone example.internal/IN (signed): next key event: 23-Oct-2024 22:47:12.544\nnamed[3246]: any newly configured zones are now loaded\nnamed[3246]: running\nnamed[3246]: resolver priming query complete: success\nnamed[3246]: managed-keys-zone: Key 20326 for zone . is now trusted (acceptance timer complete)\nnamed[3246]: zone example.internal/IN (signed): sending notifies (serial 3)\nDepending on the zone size, signing all records can take longer.\nA few interesting events can be seen in the logs above:\nKeys were generated for the\nexample.internal\nzone.\nThe\nexample.internal\nzone became signed.\nA\nkey event\nwas scheduled. This is BIND9 also taking care of the maintenance tasks of the signed zone and its keys.\nSince the zone changed, its serial number was incremented (started as 1, now it’s 3).\nThe DNSSEC keys are kept in\n/var/cache/bind\n:\n-rw-r--r-- 1 bind bind  413 Oct 23 20:50 Kexample.internal.+013+48112.key\n-rw------- 1 bind bind  215 Oct 23 20:50 Kexample.internal.+013+48112.private\n-rw-r--r-- 1 bind bind  647 Oct 23 20:50 Kexample.internal.+013+48112.state\nThis is the bulk of the work. This zone is now signed, and maintained automatically by BIND9 using the\ndefault\nDNSSEC policy.\nVerification\n¶\nThe zone that was just signed is almost ready to serve DNSSEC. Let’s perform some verification steps.\nAs it is now, this zone\nexample.internal\nis “disconnected” from the parent zone. Its name was made up for this guide, but even if it represented a real domain, it would still be missing the connection to the parent zone. Remember that DNSSEC relies on the chain of trust, and the parent of our zone needs to be able to vouch for it.\nBefore taking that step, however, it’s important to verify if everything else is working. In particular, we would want to perform some DNSSEC queries, and perform validation. A good tool to perform this validation is\ndelv\n.\ndelv\nis similar to\ndig\n, but it will perform validation on the results using the same internal resolver and validator logic as the BIND9 server itself.\nSince our zone is disconnected, we need to tell\ndelv\nto use the public key created for the zone as a trusted anchor, and to not try to reach out to the root servers of the internet.\nFirst, copy the public zone key somewhere else so it can be edited. For example:\ncp /var/cache/bind/Kexample.internal.+013+48112.key /tmp/example.key\nThat file will have some comments at the top, and then have a line that starts with the zone name, like this (the full key was truncated below for brevity):\nexample.internal. 3600 IN DNSKEY 257 3 13 jkmS5hfyY3nSww....\nWe need to make some changes here:\nRemove the comment lines from the top.\nEdit that line and replace the\n3600\nIN\nDNSKEY\ntext with\nstatic-key\n.\nThe key material after the\n13\nnumber must be enclosed in double quotes (\n\"\n).\nThe line needs to end with a\n;\n.\nAnd finally the line needs to be inside a\ntrust-anchors\nblock.\nIn the end, the\n/tmp/example.key\nfile should look like this:\ntrust-anchors {\n    example.internal. static-key 257 3 13 \"jkmS5hfyY3nSww....\";\n};\nNow\ndelv\ncan be used to query the\nexample.internal\nzone and perform DNSSEC validation:\n$ delv @127.0.0.1 -a /tmp/example.key +root=example.internal noble.example.internal +multiline\n; fully validated\nnoble.example.internal. 86400 IN A 192.168.1.11\nnoble.example.internal. 86400 IN RRSIG A 13 3 86400 (\n                                20241106131533 20241023195023 48112 example.internal.\n                                5fL4apIwCD9kt4XbzzlLxMXY3mj8Li1WZu3qzlcBpERp\n                                lXPgLODbRrWyp7L81xEFnfhecKtEYv+6Y0Xa5iVRug== )\nThis output shows important DNSSEC attributes:\n;\nfully\nvalidated\n: The DNSSEC validation was completed successfully, and the presented data is authenticated.\nRRSIG\n: This is the signature Resource Record that accompanies the\nA\nrecord from the result.\nConnecting the dots: the parent zone\n¶\nIn order to complete the chain of trust, the parent zone needs to be able to vouch for the child zone we just signed. How this is exactly done varies, depending on who is the administrator of the parent zone. It could be as simple as just pasting the DS or DNSKEY records in a form.\nTypically what is needed is either a DS record, or a DNSKEY record. Here is how to produce them, ready to be sent to the parent.\nDS record format\n¶\nA DS record can be produced from the zone public key via the\ndnssec-dsfromkey\ntool. For example:\n$ dnssec-dsfromkey /var/cache/bind/Kexample.internal.+013+48112.key\nexample.internal. IN DS 48112 13 2 1212DE7DA534556F1E11898F2C7A66736D5107962A19A0BFE1C2A67D6841962A\nDNSKEY record format\n¶\nThe DNSKEY format doesn’t need tooling, and can be found directly in the zone’s public key file, after the lines that start with the comment delimiter\n;\n:\n$ cat /var/cache/bind/Kexample.internal.+013+48112.key\n; This is a key-signing key, keyid 48112, for example.internal.\n; Created: 20241023205023 (Wed Oct 23 20:50:23 2024)\n; Publish: 20241023205023 (Wed Oct 23 20:50:23 2024)\n; Activate: 20241023205023 (Wed Oct 23 20:50:23 2024)\n; SyncPublish: 20241024215523 (Thu Oct 24 21:55:23 2024)\nexample.internal. 3600 IN DNSKEY 257 3 13 jkmS5hfyY3nSww4tD9Fy5d+GGc3A/zR1CFUxmN8T2TKTkgGWp8dusllM 7TrIZTEg6wZxmMs754/ftoTA6jmM1g==\nTaking that line, the actual record to send to the parent zone just needs a little bit of tweaking so it looks like this:\nexample.internal. 3600 IN DNSKEY 257 3 13 (jkmS5hfyY3nSww4tD9Fy5d+GGc3A/zR1CFUxmN8T2TKTkgGWp8dusllM 7TrIZTEg6wZxmMs754/ftoTA6jmM1g==);\nFurther reading\n¶\nThe DNSSEC Guide from bind9\nEasy-Start Guide for Signing Authoritative Zones\nCreating a Custom DNSSEC Policy\nDetailed DNSSEC chapter from the bind9 documentation\ndelv(1)\nmanual page\nWorking with the parent zone", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:21Z", "original_len_words": 1454}}
{"id": "46090a1ae2", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/install-isc-dhcp-server/", "title": "How to install and configure isc-dhcp-server - Ubuntu Server documentation", "text": "How to install and configure isc-dhcp-server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure isc-dhcp-server\n¶\nNote\nAlthough Ubuntu still supports\nisc-dhcp-server\n, this software is\nno longer supported by its vendor\n. It has been replaced by\nKea\n.\nIn this guide we show how to install and configure\nisc-dhcp-server\n, which installs the dynamic host configuration protocol daemon,\nDHCPD\n. For\nisc-kea\ninstructions,\nrefer to this guide instead\n.\nInstall isc-dhcp-server\n¶\nAt a terminal prompt, enter the following command to install\nisc-dhcp-server\n:\nsudo\napt\ninstall\nisc-dhcp-server\nNote\nYou can find diagnostic messages from\ndhcpd\nin\nsyslog\n.\nConfigure isc-dhcp-server\n¶\nYou will probably need to change the default configuration by editing\n/etc/dhcp/dhcpd.conf\nto suit your needs and particular configuration.\nMost commonly, what you want to do is assign an IP address randomly. This can be done with\n/etc/dhcp/dhcpd.conf\nsettings as follows:\n# minimal sample /etc/dhcp/dhcpd.conf\ndefault-lease-time 600;\nmax-lease-time 7200;\n    \nsubnet 192.168.1.0 netmask 255.255.255.0 {\n range 192.168.1.150 192.168.1.200;\n option routers 192.168.1.254;\n option domain-name-servers 192.168.1.1, 192.168.1.2;\n option domain-name \"mydomain.example\";\n}\nThis will result in the DHCP server giving clients an IP address from the range\n192.168.1.150\n-\n192.168.1.200\n. It will lease an IP address for 600 seconds if the client doesn’t ask for a specific time frame. Otherwise the maximum (allowed) lease will be 7200 seconds. The server will also “advise” the client to use\n192.168.1.254\nas the default-gateway and\n192.168.1.1\nand\n192.168.1.2\nas its\nDNS\nservers.\nYou also may need to edit\n/etc/default/isc-dhcp-server\nto specify the interfaces\ndhcpd\nshould listen to.\nIn the example below,\neth4\nis used, but you should replace this with the appropriate interface for your system. The name of the network interface can vary depending on your setup. For instance, it could be\neth0\n,\nens33\n, or any other name depending on the device you’re using.\nINTERFACESv4\n=\n\"eth4\"\nAfter changing the config files you need to restart the\ndhcpd\nservice:\nsudo\nsystemctl\nrestart\nisc\n-\ndhcp\n-\nserver\n.\nservice\nFurther reading\n¶\nThe\nisc-dhcp-server Ubuntu Wiki\npage has more information.\nFor more\n/etc/dhcp/dhcpd.conf\noptions see the\ndhcpd.conf(5)\nmanual page\nISC dhcp-server", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:21Z", "original_len_words": 379}}
{"id": "c225444a89", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/install-isc-kea/", "title": "How to install and configure isc-kea - Ubuntu Server documentation", "text": "How to install and configure isc-kea - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure isc-kea\n¶\nIn this guide we show how to install and configure\nisc-kea\nin Ubuntu 23.04\nor greater.\nKea\nis the\nDHCP\nserver developed by ISC to replace\nisc-dhcp\n. It is newer and designed for more modern network environments.\nFor\nisc-dhcp-server\ninstructions,\nrefer to this guide instead\n.\nInstall isc-kea\n¶\nAt a terminal prompt, enter the following command to install\nisc-kea\n:\nsudo\napt\ninstall\nkea\nThis will also install a few binary packages, including\nkea-dhcp4-server\n: The IPv4 DHCP server (the one we will configure in this guide).\nkea-dhcp6-server\n: The IPv6 DHCP server.\nkea-ctrl-agent\n: A REST API service for Kea.\nkea-dhcp-ddns-server\n: A Dynamic\nDNS\nservice to update DNS based on DHCP lease events.\nSince the\nkea-ctrl-agent\nservice has some administrative rights to the Kea\nservices, we need to ensure regular users are not allowed to use the API\nwithout permissions. Ubuntu does it by requiring user authentication to access\nthe\nkea-ctrl-agent\nAPI service (\nLP: #2007312 has more details on this\n).\nTherefore, the installation process described above will get a\ndebconf\n“high” priority prompt with 3 options:\nno action (default);\nconfigure with a random password; or\nconfigure with a given password.\nIf there is no password, the\nkea-ctrl-agent\nwill\nnot\nstart.\nThe password is expected to be in\n/etc/kea/kea-api-password\n, with ownership\nroot:_kea\nand permissions\n0640\n. To change it, run\ndpkg-reconfigure\nkea-ctrl-agent\n(which will present the same 3 options from above again), or just edit the file\nmanually.\nConfigure kea-dhcp4\n¶\nThe\nkea-dhcp4\nservice can be configured by editing\n/etc/kea/kea-dhcp4.conf\n.\nMost commonly, what you want to do is let Kea assign an IP address from a\npre-configured IP address pool. This can be done with settings as follows:\n{\n\"Dhcp4\"\n:\n{\n\"interfaces-config\"\n:\n{\n\"interfaces\"\n:\n[\n\"eth4\"\n]\n},\n\"control-socket\"\n:\n{\n\"socket-type\"\n:\n\"unix\"\n,\n\"socket-name\"\n:\n\"/run/kea/kea4-ctrl-socket\"\n},\n\"lease-database\"\n:\n{\n\"type\"\n:\n\"memfile\"\n,\n\"lfc-interval\"\n:\n3600\n},\n\"valid-lifetime\"\n:\n600\n,\n\"max-valid-lifetime\"\n:\n7200\n,\n\"subnet4\"\n:\n[{\n\"id\"\n:\n1\n,\n\"subnet\"\n:\n\"192.168.1.0/24\"\n,\n\"pools\"\n:\n[{\n\"pool\"\n:\n\"192.168.1.150 - 192.168.1.200\"\n}],\n\"option-data\"\n:\n[{\n\"name\"\n:\n\"routers\"\n,\n\"data\"\n:\n\"192.168.1.254\"\n},\n{\n\"name\"\n:\n\"domain-name-servers\"\n,\n\"data\"\n:\n\"192.168.1.1, 192.168.1.2\"\n},\n{\n\"name\"\n:\n\"domain-name\"\n,\n\"data\"\n:\n\"mydomain.example\"\n}\n]\n}]\n}\n}\nThis will result in the DHCP server listening on interface “eth4”, giving clients an IP address from the range\n192.168.1.150\n-\n192.168.1.200\n. It will lease an IP address for 600 seconds if the client doesn’t ask for a specific time frame. Otherwise the maximum (allowed) lease will be 7200 seconds. The server will also “advise” the client to use\n192.168.1.254\nas the default-gateway and\n192.168.1.1\nand\n192.168.1.2\nas its DNS servers.\nAfter changing the config file you can reload the server configuration through\nkea-shell\nwith the following command (considering you have the\nkea-ctrl-agent\nrunning as described above):\nkea-shell\n--host\n127\n.0.0.1\n--port\n8000\n--auth-user\nkea-api\n--auth-password\n$(\ncat\n/etc/kea/kea-api-password\n)\n--service\ndhcp4\nconfig-reload\nThen, press\nctrl\n-\nd\n. The server should respond with:\n[\n{\n\"result\"\n:\n0\n,\n\"text\"\n:\n\"Configuration successful.\"\n}\n]\nmeaning your configuration was received by the server.\nThe\nkea-dhcp4-server\nservice logs should contain an entry similar to:\nDHCP4_DYNAMIC_RECONFIGURATION_SUCCESS\ndynamic\nserver\nreconfiguration\nsucceeded\nwith\nfile\n:\n/\netc\n/\nkea\n/\nkea\n-\ndhcp4\n.\nconf\nsignaling that the server was successfully reconfigured.\nYou can read\nkea-dhcp4-server\nservice logs with\njournalctl\n:\njournalctl\n-u\nkea-dhcp4-server\nAlternatively, instead of reloading the DHCP4 server configuration through\nkea-shell\n,  you can restart the\nkea-dhcp4-service\nwith:\nsystemctl\nrestart\nkea-dhcp4-server\nFurther reading\n¶\nISC Kea Documentation", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:21Z", "original_len_words": 643}}
{"id": "c22008e6ef", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/install-nfs/", "title": "Network File System (NFS) - Ubuntu Server documentation", "text": "Network File System (NFS) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nNetwork File System (NFS)\n¶\nNFS allows a system to share directories and files with others over a network. By using NFS, users and programs can access files on remote systems almost as if they were local files.\nSome of the most notable benefits that NFS can provide are:\nLocal workstations use less disk space because commonly used data can be stored on a single machine and still remain accessible to others over the network.\nThere is no need for users to have separate home directories on every network machine. Home directories could be set up on the NFS server and made available throughout the network.\nStorage devices such as floppy disks, CDROM drives, and USB Thumb drives can be used by other machines on the network. This may reduce the number of removable media drives throughout the network.\nWarning\nIf you use\nNFS and authd\nat the same time, you must add a Kerberos configuration on both the client and the server. Otherwise, you will encounter permission issues due to mismatched user and group identifiers.\nFor details, see\nUsing authd with NFS\n.\nInstallation\n¶\nAt a terminal prompt enter the following command to install the NFS Server:\nsudo apt install nfs-kernel-server\nTo start the NFS server, you can run the following command at a terminal prompt:\nsudo systemctl start nfs-kernel-server.service\nConfiguration\n¶\nYou can configure the directories to be exported by adding them to the\n/etc/exports\nfile. For example:\n/srv     *(ro,sync,subtree_check)\n/home    *.hostname.com(rw,sync,no_subtree_check)\n/scratch *(rw,async,no_subtree_check,no_root_squash)\nMake sure any custom mount points you’re adding have been created (/srv and /home will already exist):\nsudo mkdir /scratch\nApply the new config via:\nsudo exportfs -a\nYou can replace * with one of the\nhostname\nformats. Make the hostname declaration as specific as possible so unwanted systems cannot access the NFS mount.  Be aware that\n*.hostname.com\nwill match\nfoo.hostname.com\nbut not\nfoo.bar.my-domain.com\n.\nThe\nsync\n/\nasync\noptions control whether changes are guaranteed to be committed to stable storage before replying to requests.\nasync\nthus gives a performance benefit but risks data loss or corruption.  Even though\nsync\nis the default, it’s worth setting since exportfs will issue a warning if it’s left unspecified.\nsubtree_check\nand\nno_subtree_check\nenables or disables a security verification that subdirectories a client attempts to mount for an exported\nfilesystem\nare ones they’re permitted to do so.  This verification step has some performance implications for some use cases, such as home directories with frequent file renames.  Read-only filesystems are more suitable to enable\nsubtree_check\non.  Like with sync, exportfs will warn if it’s left unspecified.\nThere are a number of optional settings for NFS mounts for tuning performance, tightening security, or providing conveniences.  These settings each have their own trade-offs so it is important to use them with care, only as needed for the particular use case.\nno_root_squash\n, for example, adds a convenience to allow root-owned files to be modified by any client system’s root user; in a multi-user environment where executables are allowed on a shared mount point, this could lead to security problems.\nNFS Client Configuration\n¶\nTo enable NFS support on a client system, enter the following command at the terminal prompt:\nsudo apt install nfs-common\nUse the mount command to mount a shared NFS directory from another machine, by typing a command line similar to the following at a terminal prompt:\nsudo mkdir /opt/example\nsudo mount example.hostname.com:/srv /opt/example\nWarning\nThe mount point directory\n/opt/example\nmust exist. There should be no files or subdirectories in the\n/opt/example\ndirectory, else they will become inaccessible until the nfs filesystem is unmounted.\nAn alternate way to mount an NFS share from another machine is to add a line to the\n/etc/fstab\nfile. The line must state the hostname of the NFS server, the directory on the server being exported, and the directory on the local machine where the NFS share is to be mounted.\nThe general syntax for the line in\n/etc/fstab\nfile is as follows:\nexample.hostname.com:/srv /opt/example nfs rsize=8192,wsize=8192,timeo=14,intr\nAdvanced Configuration\n¶\nNFS is comprised of several services, both on the server and the client. Each one of these services can have its own default configuration, and depending on the Ubuntu Server release you have installed, this configuration is done in different files, and with a different syntax.\nUbuntu Server 22.04 LTS (“jammy”)\n¶\nAll NFS related services read a single configuration file:\n/etc/nfs.conf\n. This is a INI-style config file, see the\nnfs.conf(5)\nmanual page for details. Furthermore, there is a\n/etc/nfs.conf.d\ndirectory which can hold\n*.conf\nsnippets that can override settings from previous snippets or from the\nnfs.conf\nmain config file itself.\nThere is a new command-line tool called\nnfsconf(8)\nwhich can be used to query or even set configuration parameters in\nnfs.conf\n. In particular, it has a\n--dump\nparameter which will show the effective configuration including all changes done by\n/etc/nfs.conf.d/*.conf\nsnippets.\nFor Ubuntu Server 20.04 LTS (“focal”) and earlier\n¶\nEarlier Ubuntu releases use the traditional configuration mechanism for the NFS services via\n/etc/defaults/\nconfiguration files. These are\n/etc/default/nfs-common\nand\n/etc/default/nfs/kernel-server\n, and are used basically to adjust the command-line options given to each daemon.\nEach file has a small explanation about the available settings.\nWarning\nThe\nNEED_*\nparameters have no effect on systemd-based installations, like Ubuntu 20.04 LTS (“focal”) and Ubuntu 18.04 LTS (“bionic”).\nIn those systems, to control whether a service should be running or not, use\nsystemctl\nenable\nor\nsystemctl\ndisable\n, respectively.\nUpgrading to Ubuntu 22.04 LTS (“jammy”)\n¶\nThe main change to the NFS packages in Ubuntu 22.04 LTS (“jammy”) is the configuration file. Instead of multiple files sourced by startup scripts from\n/etc/default/nfs-*\n, now there is one main configuration file in\n/etc/nfs.conf\n, with an INI-style syntax.\nWhen upgrading to Ubuntu 22.04 LTS (“jammy”) from a release that still uses the\n/etc/defaults/nfs-*\nconfiguration files, the following will happen:\na default\n/etc/nfs.conf\nconfiguration file will be installed\nif the\n/etc/default/nfs-*\nfiles have been modified, a conversion script will be run and it will create\n/etc/nfs.conf.d/local.conf\nwith the local modifications.\nIf this conversion script fails, then the package installation will fail. This can happen if the\n/etc/default/nfs-*\nfiles have an option that the conversion script wasn’t prepared to handle, or a syntax error for example. In such cases, please file a bug using this link:\nhttps://bugs.launchpad.net/ubuntu/+source/nfs-utils/+filebug\nYou can run the conversion tool manually to gather more information about the error: it’s in\n/usr/share/nfs-common/nfsconvert.py\nand must be run as\nroot\n.\nIf all goes well, as it should in most cases, the system will have\n/etc/nfs.conf\nwith the defaults, and\n/etc/nfs.conf.d/local.conf\nwith the changes. You can merge these two together manually, and then delete\nlocal.conf\n, or leave it as is. Just keep in mind that\n/etc/nfs.conf\nis not the whole story: always inspect\n/etc/nfs.conf.d\nas well, as it may contain files overriding the defaults.\nYou can always run\nnfsconf\n--dump\nto check the final settings, as it merges together all configuration files and shows the resulting non-default settings.\nRestarting NFS services\n¶\nSince NFS is comprised of several individual services, it can be difficult to determine what to restart after a certain configuration change.\nThe tables below summarize all available services, which “meta” service they are linked to, and which configuration file each service uses.\nService\nnfs-utils.service\nnfs-server.service\nconfig file (22.04)\nconfig file (< 22.04) /etc/default/nfs-*\nnfs-blkmap\nPartOf\nnfs.conf\nnfs-mountd\nBindsTo\nnfs.conf\nnfs-kernel-server\nnfsdcld\nnfs-idmapd\nBindsTo\nnfs.conf, idmapd.conf\nidmapd.conf\nrpc-gssd\nPartOf\nnfs.conf\nrpc-statd\nPartOf\nnfs.conf\nnfs-common\nrpc-svcgssd\nPartOf\nBindsTo\nnfs.conf\nnfs-kernel-server\nFor example,\nsystemctl\nrestart\nnfs-server.service\nwill restart\nnfs-mountd\n,\nnfs-idmapd\nand\nrpc-svcgssd\n(if running). On the other hand, restarting\nnfs-utils.service\nwill restart\nnfs-blkmap\n,\nrpc-gssd\n,\nrpc-statd\nand\nrpc-svcgssd\n.\nOf course, each service can still be individually restarted with the usual\nsystemctl\nrestart\n<service>\n.\nThe\nnfs.systemd(7)\nmanpage has more details on the several systemd units available with the NFS packages.\nNFS with Kerberos\n¶\nKerberos with NFS adds an extra layer of security on top of NFS. It can be just a stronger authentication mechanism, or it can also be used to sign and encrypt the NFS traffic.\nThis section will assume you already have setup a Kerberos server, with a running KDC and admin services. Setting that up is explained elsewhere in the Ubuntu Server Guide.\nNFS server (using kerberos)\n¶\nThe NFS server will have the usual\nnfs-kernel-server\npackage and its dependencies, but we will also have to install kerberos packages. The kerberos packages are not strictly necessary, as the necessary keys can be copied over from the KDC, but it makes things much easier.\nFor this example, we will use:\n.vms\nDNS\ndomain\nVMS\nKerberos realm\nj-nfs-server.vms\nfor the NFS server\nj-nfs-client.vms\nfor the NFS client\nubuntu/admin\nprincipal has admin privileges on the KDC\nAdjust these names according to your setup.\nFirst, install the\nkrb5-user\npackage:\nsudo apt install krb5-user\nThen, with an admin principal, let’s create a key for the NFS server:\n$ sudo kadmin -p ubuntu/admin -q \"addprinc -randkey nfs/j-nfs-server.vms\"\nAnd extract the key into the local keytab:\n$ sudo kadmin -p ubuntu/admin -q \"ktadd nfs/j-nfs-server.vms\"\nAuthenticating as principal ubuntu/admin with password.\nPassword for ubuntu/admin@VMS:\nEntry for principal nfs/j-nfs-server.vms with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab FILE:/etc/krb5.keytab.\nEntry for principal nfs/j-nfs-server.vms with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab FILE:/etc/krb5.keytab.\nConfirm the key is available:\n$ sudo klist -k\nKeytab name: FILE:/etc/krb5.keytab\nKVNO Principal\n---- --------------------------------------------------------------------------\n   2 nfs/j-nfs-server.vms@VMS\n   2 nfs/j-nfs-server.vms@VMS\nNow install the NFS server:\n$ sudo apt install nfs-kernel-server\nThis will already automatically start the kerberos-related nfs services, because of the presence of\n/etc/krb5.keytab\n.\nNow populate\n/etc/exports\n, restricting the exports to krb5 authentication. For example, exporting\n/storage\nusing\nkrb5p\n:\n/storage *(rw,sync,no_subtree_check,sec=krb5p)\nRefresh the exports:\n$ sudo exportfs -rav\nexporting *:/storage\nThe security options are explained in the\nexports(5)\nmanpage, but generally they are:\nkrb5\n: use kerberos for authentication only (non-auth traffic is in clear text)\nkrb5i\n: use kerberos for authentication and integrity checks (non-auth traffic is in clear text)\nkrb5p\n: use kerberos for authentication, integrity and privacy protection (non-auth traffic is encrypted)\nNFS client (using kerberos)\n¶\nThe NFS client has a similar set of steps. First we will prepare the client’s keytab, so that when we install the NFS client package it will start the extra kerberos services automatically just by detecting the presence of the keytab:\nsudo apt install krb5-user\nTo allow the\nroot\nuser to mount NFS shares via kerberos without a password, we have to create a host key for the NFS client:\nsudo kadmin -p ubuntu/admin -q \"addprinc -randkey host/j-nfs-client.vms\"\nAnd extract it:\n$ sudo kadmin -p ubuntu/admin -q \"ktadd host/j-nfs-client.vms\"\nNow install the NFS client package:\n$ sudo apt install nfs-common\nAnd you should be able to do your first NFS kerberos mount:\n$ sudo mount j-nfs-server:/storage /mnt\nIf you are using a machine credential, then the above mount will work without having a kerberos ticket, i.e.,\nklist\nwill show no tickets:\n# mount j-nfs-server:/storage /mnt\n# ls -l /mnt/*\n-rw-r--r-- 1 root root 0 Apr  5 14:50 /mnt/hello-from-nfs-server.txt\n# klist\nklist: No credentials cache found (filename: /tmp/krb5cc_0)\nNotice the above was done with\nroot\n. Let’s try accessing that existing mount with the\nubuntu\nuser, without acquiring a kerberos ticket:\n# sudo -u ubuntu -i\n$ ls -l /mnt/*\nls: cannot access '/mnt/*': Permission denied\nThe\nubuntu\nuser will only be able to access that mount if they have a kerberos ticket:\n$ kinit\nPassword for ubuntu@VMS: \n$ ls -l /mnt/*\n-rw-r--r-- 1 root root 0 Apr  5 14:50 /mnt/hello-from-nfs-server.txt\nAnd now we have not only the TGT, but also a ticket for the NFS service:\n$ klist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: ubuntu@VMS\n\nValid starting     Expires            Service principal\n04/05/22 17:48:50  04/06/22 03:48:50  krbtgt/VMS@VMS\n        renew until 04/06/22 17:48:48\n04/05/22 17:48:52  04/06/22 03:48:50  nfs/j-nfs-server.vms@\n        renew until 04/06/22 17:48:48\n        Ticket server: nfs/j-nfs-server.vms@VMS\nOne drawback of using a machine credential for mounts done by the\nroot\nuser is that you need a persistent secret (the\n/etc/krb5.keytab\nfile) in the filesystem. Some sites may not allow such a persistent secret to be stored in the filesystem. An alternative is to use\nrpc.gssd\ns\n-n\noption. From\nrpc.gssd(8)\n:\n-n\n: when specified, UID 0 is forced to obtain user credentials which are used instead of the local system’s machine credentials.\nWhen this option is enabled and\nrpc.gssd\nrestarted, then even the\nroot\nuser will need to obtain a kerberos ticket to perform an NFS kerberos mount.\nWarning\nNote that this prevents automatic NFS mounts via\n/etc/fstab\n, unless a kerberos ticket is obtained before.\nIn Ubuntu 22.04 LTS (“jammy”), this option is controlled in\n/etc/nfs.conf\nin the\n[gssd]\nsection:\n[gssd]\nuse-machine-creds=0\nIn older Ubuntu releases, the command line options for the\nrpc.gssd\ndaemon are not exposed in\n/etc/default/nfs-common\n, therefore a systemd override file needs to be created. You can either run:\n$ sudo systemctl edit rpc-gssd.service\nAnd paste the following into the editor that will open:\n[Service]\nExecStart=\nExecStart=/usr/sbin/rpc.gssd $GSSDARGS -n\nOr manually create the file\n/etc/systemd/system/rpc-gssd.service.d/override.conf\nand any needed directories up to it, with the contents above.\nAfter you restart the service with\nsystemctl\nrestart\nrpc-gssd.service\n, the\nroot\nuser won’t be able to mount the NFS kerberos share without obtaining a ticket first.\nReferences\n¶\nLinux NFS wiki\nLinux NFS faq\nUbuntu Wiki NFS Howto\nUbuntu Wiki NFSv4 Howto", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:22Z", "original_len_words": 2252}}
{"id": "1817da0e53", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/serve-ntp-with-chrony/", "title": "How to serve the Network Time Protocol with Chrony - Ubuntu Server documentation", "text": "How to serve the Network Time Protocol with Chrony - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to serve the Network Time Protocol with Chrony\n¶\nAs of Ubuntu 25.10\nchrony\nis installed by default,\nfunctioning as a client\nand keeping your time in sync. However, if you also want to serve NTP information then you need an NTP server.\nBetween\nchrony\n, the now-deprecated\nntpd\n, and\nopen-ntp\n, there are plenty of options. The solution we recommend is\nchrony\n.\nThe NTP daemon\nchronyd\ncalculates the drift and offset of your system clock and continuously adjusts it, so there are no large corrections that could lead to inconsistent logs, for instance. The cost is a little processing power and memory, but for a modern server this is usually negligible.\nInstall\nchronyd\n¶\nTo install\nchrony\n, run the following command from a terminal prompt:\nsudo\napt\ninstall\nchrony\nThis will provide two binaries:\nchronyd\n- the actual daemon to sync and serve via the Network Time Protocol\nchronyc\n- command-line interface for the\nchrony\ndaemon\nEnable serving the Network Time Protocol\n¶\nYou can install\nchrony\n(above) and configure special Hardware (below) for a local synchronization\nand as-installed that is the default to stay on the secure and conservative side. But if you want to\nserve\nNTP you need adapt your configuration.\nTo enable serving NTP you’ll need to at least set the\nallow\nrule. This controls which clients/networks you want\nchrony\nto serve NTP to.\nAn example would be:\nallow 1.2.3.4\nSee the section “NTP server” in the\nchrony.conf(5)\nmanual page for more details on how you can control and restrict access to your NTP server.\nView\nchrony\nstatus\n¶\nYou can use\nchronyc\nto see query the status of the\nchrony\ndaemon. For example, to get an overview of the currently available and selected time sources, run\nchronyc\nsources\n, which provides output like this:\nMS Name/IP address         Stratum Poll Reach LastRx Last sample\n===============================================================================\n^+ gamma.rueckgr.at              2   8   377   135  -1048us[-1048us] +/-   29ms\n^- 2b.ncomputers.org             2   8   377   204  -1141us[-1124us] +/-   50ms\n^+ www.kashra.com                2   8   377   139  +3483us[+3483us] +/-   18ms\n^+ stratum2-4.NTP.TechFak.U>     2   8   377   143  -2090us[-2073us] +/-   19ms\n^- zepto.mcl.gg                  2   7   377     9   -774us[ -774us] +/-   29ms\n^- mirrorhost.pw                 2   7   377    78   -660us[ -660us] +/-   53ms\n^- atto.mcl.gg                   2   7   377     8   -823us[ -823us] +/-   50ms\n^- static.140.107.46.78.cli>     2   8   377     9  -1503us[-1503us] +/-   45ms\n^- 4.53.160.75                   2   8   377   137    -11ms[  -11ms] +/-  117ms\n^- 37.44.185.42                  3   7   377    10  -3274us[-3274us] +/-   70ms\n^- bagnikita.com                 2   7   377    74  +3131us[+3131us] +/-   71ms\n^- europa.ellipse.net            2   8   377   204   -790us[ -773us] +/-   97ms\n^- tethys.hot-chilli.net         2   8   377   141   -797us[ -797us] +/-   59ms\n^- 66-232-97-8.static.hvvc.>     2   7   377   206  +1669us[+1686us] +/-  133ms\n^+ 85.199.214.102                1   8   377   205   +175us[ +192us] +/-   12ms\n^* 46-243-26-34.tangos.nl        1   8   377   141   -123us[ -106us] +/-   10ms\n^- pugot.canonical.com           2   8   377    21    -95us[  -95us] +/-   57ms\n^- alphyn.canonical.com          2   6   377    23  -1569us[-1569us] +/-   79ms\n^- golem.canonical.com           2   7   377    92  -1018us[-1018us] +/-   31ms\n^- chilipepper.canonical.com     2   8   377    21  -1106us[-1106us] +/-   27ms\nYou can also make use of the\nchronyc\nsourcestats\ncommand, which produces output like this:\n210 Number of sources = 20\nName/IP Address            NP  NR  Span  Frequency  Freq Skew  Offset  Std Dev\n==============================================================================\ngamma.rueckgr.at           25  15   32m     -0.007      0.142   -878us   106us\n2b.ncomputers.org          26  16   35m     -0.132      0.283  -1169us   256us\nwww.kashra.com             25  15   32m     -0.092      0.259  +3426us   195us\nstratum2-4.NTP.TechFak.U>  25  14   32m     -0.018      0.130  -2056us    96us\nzepto.mcl.gg               13  11   21m     +0.148      0.196   -683us    66us\nmirrorhost.pw               6   5   645     +0.117      0.445   -591us    19us\natto.mcl.gg                21  13   25m     -0.069      0.199   -904us   103us\nstatic.140.107.46.78.cli>  25  18   34m     -0.005      0.094  -1526us    78us\n4.53.160.75                25  10   32m     +0.412      0.110    -11ms    84us\n37.44.185.42               24  12   30m     -0.983      0.173  -3718us   122us\nbagnikita.com              17   7   31m     -0.132      0.217  +3527us   139us\neuropa.ellipse.net         26  15   35m     +0.038      0.553   -473us   424us\ntethys.hot-chilli.net      25  11   32m     -0.094      0.110   -864us    88us\n66-232-97-8.static.hvvc.>  20  11   35m     -0.116      0.165  +1561us   109us\n85.199.214.102             26  11   35m     -0.054      0.390   +129us   343us\n46-243-26-34.tangos.nl     25  16   32m     +0.129      0.297   -307us   198us\npugot.canonical.com        25  14   34m     -0.271      0.176   -143us   135us\nalphyn.canonical.com       17  11  1100     -0.087      0.360  -1749us   114us\ngolem.canonical.com        23  12   30m     +0.057      0.370   -988us   229us\nchilipepper.canonical.com  25  18   34m     -0.084      0.224  -1116us   169us\nCertain\nchronyc\ncommands are privileged and cannot be run via the network without explicitly allowing them. See the\nCommand and monitoring access\nsection in\nman\nchrony.conf\nfor more details. A local admin can use\nsudo\nsince this will grant access to the local admin socket\n/var/run/chrony/chronyd.sock\n.\nPulse-Per-Second (PPS) support\n¶\nChrony\nsupports various PPS types natively. It can use kernel PPS API as well as Precision Time Protocol (PTP) hardware clocks. Most general GPS receivers can be leveraged via\nGPSD\n. The latter (and potentially more) can be accessed via\nSHM\nor via a\nsocket\n(recommended). All of the above can be used to augment\nchrony\nwith additional high quality time sources for better accuracy,\njitter\n, drift, and longer- or shorter-term accuracy. Usually, each kind of clock type is good at one of those, but non-perfect at the others. For more details on configuration see some of the external PPS/GPSD resources listed below.\nNote\nAs of the release of 20.04, there was a bug which - until fixed - you might want to\nadd this content\nto your\n/etc/apparmor.d/local/usr.sbin.gpsd\n.\nExample configuration for GPSD to feed\nchrony\n¶\nFor the installation and setup you will first need to run the following command in your terminal window:\nsudo\napt\ninstall\ngpsd\nchrony\nHowever, since you will want to test/debug your setup (especially the GPS reception), you should also install:\nsudo\napt\ninstall\npps\n-\ntools\ngpsd\n-\nclients\nGPS devices usually communicate via serial interfaces. The most common type these days are USB GPS devices, which have a serial converter behind USB. If you want to use one of these devices for PPS then please be aware that the majority do not signal PPS via USB. Check the\nGPSD hardware\nlist for details. The examples below were run with a Navisys GR701-W.\nWhen plugging in such a device (or at boot time)\ndmesg\nshould report a serial connection of some sort, as in this example:\n[   52.442199] usb 1-1.1: new full-speed USB device number 3 using xhci_hcd\n[   52.546639] usb 1-1.1: New USB device found, idVendor=067b, idProduct=2303, bcdDevice= 4.00\n[   52.546654] usb 1-1.1: New USB device strings: Mfr=1, Product=2, SerialNumber=0\n[   52.546665] usb 1-1.1: Product: USB-Serial Controller D\n[   52.546675] usb 1-1.1: Manufacturer: Prolific Technology Inc. \n[   52.602103] usbcore: registered new interface driver usbserial_generic\n[   52.602244] usbserial: USB Serial support registered for generic\n[   52.609471] usbcore: registered new interface driver pl2303\n[   52.609503] usbserial: USB Serial support registered for pl2303\n[   52.609564] pl2303 1-1.1:1.0: pl2303 converter detected\n[   52.618366] usb 1-1.1: pl2303 converter now attached to ttyUSB0\nWe see in this example that the device appeared as\nttyUSB0\n. So that\nchrony\nlater accepts being fed time information by this device, we have to set it up in\n/etc/chrony/chrony.conf\n(please replace\nUSB0\nwith whatever applies to your setup):\nrefclock\nSHM\n0\nrefid\nGPS\nprecision\n1e-1\noffset\n0.9999\ndelay\n0.2\nrefclock\nSOCK\n/\nvar\n/\nrun\n/\nchrony\n.\nttyUSB0\n.\nsock\nrefid\nPPS\nNext, we need to restart\nchrony\nto make the socket available and have it waiting.\nsudo\nsystemctl\nrestart\nchrony\nWe then need to tell\ngpsd\nwhich device to manage. Therefore, in\n/etc/default/gpsd\nwe set:\nDEVICES\n=\n\"/dev/ttyUSB0\"\nIt should be noted that since the\ndefault\nuse-case of\ngpsd\nis, well, for\ngps position tracking\n, it will normally not consume any CPU since it is just waiting on a\nsocket\nfor clients. Furthermore, the client will tell\ngpsd\nwhat it requests, and\ngpsd\nwill only provide what is asked for.\nFor the use case of\ngpsd\nas a PPS-providing-daemon, you want to set the option to:\nImmediately start (even without a client connected). This can be set in\nGPSD_OPTIONS\nof\n/etc/default/gpsd\n:\nGPSD_OPTIONS=\"-n\"\nEnable the service itself and not wait for a client to reach the socket in the future:\nsudo\nsystemctl\nenable\n/lib/systemd/system/gpsd.service\nRestarting\ngpsd\nwill now initialize the PPS from GPS and in\ndmesg\nyou will see:\npps_ldisc: PPS line discipline registered\n pps pps0: new PPS source usbserial0\n pps pps0: source \"/dev/ttyUSB0\" added\nIf you have multiple PPS sources, the tool\nppsfind\nmay be useful to help identify which PPS belongs to which GPS. In our example, the command\nsudo\nppsfind\n/dev/ttyUSB0\nwould return the following:\npps0: name=usbserial0 path=/dev/ttyUSB0\nNow we have completed the basic setup. To proceed, we now need our GPS to get a lock. Tools like\ncgps\nor\ngpsmon\nneed to report a 3D “fix” in order to provide accurate data. Let’s run the command\ncgps\n, which in our case returns:\n...\n│ Status:         3D FIX (7 secs) ...\nYou would then want to use\nppstest\nin order to check that you are really receiving PPS data. So, let us run the command\nsudo\nppstest\n/dev/pps0\n, which will produce an output like this:\ntrying PPS source \"/dev/pps0\"\nfound PPS source \"/dev/pps0\"\nok, found 1 source(s), now start fetching data...\nsource 0 - assert 1588140739.099526246, sequence: 69 - clear  1588140739.999663721, sequence: 70\nsource 0 - assert 1588140740.099661485, sequence: 70 - clear  1588140739.999663721, sequence: 70\nsource 0 - assert 1588140740.099661485, sequence: 70 - clear  1588140740.999786664, sequence: 71\nsource 0 - assert 1588140741.099792447, sequence: 71 - clear  1588140740.999786664, sequence: 71\nOk,\ngpsd\nis now running, the GPS reception has found a fix, and it has fed this into\nchrony\n. Let’s check on that from the point of view of\nchrony\n.\nInitially, before\ngpsd\nhas started or before it has a lock, these sources will be new and “untrusted” - they will be marked with a “?” as shown in the example below. If your devices remain in the “?” state (even after some time) then\ngpsd\nis not feeding any data to\nchrony\nand you will need to debug why.\nchronyc> sources\n210 Number of sources = 10\nMS Name/IP address         Stratum Poll Reach LastRx Last sample               \n===============================================================================\n#? GPS                           0   4     0     -     +0ns[   +0ns] +/-    0ns\n#? PPS                           0   4     0     -     +0ns[   +0ns] +/-    0ns\nOver time,\nchrony\nwill classify all of the unknown sources as “good” or “bad”.\nIn the example below, the raw GPS had too much deviation (+- 200ms) but the PPS is good (+- 63us).\nchronyc> sources\n210 Number of sources = 10\nMS Name/IP address        Stratum Poll Reach LastRx Last sample\n===============================================================================\n#x GPS                         0    4   177    24 -876ms[ -876ms] +/- 200ms\n#- PPS                         0    4   177    21 +916us[ +916us] +/- 63us\n^- chilipepper.canonical.com   2    6    37    53  +33us[ +33us]  +/- 33ms\nFinally, after a while it used the hardware PPS input (as it was better):\nchronyc> sources\n210 Number of sources = 10\nMS Name/IP address         Stratum Poll Reach LastRx Last sample\n===============================================================================\n#x GPS                           0   4   377    20   -884ms[ -884ms] +/-  200ms\n#* PPS                           0   4   377    18  +6677ns[  +52us] +/-   58us\n^- alphyn.canonical.com          2   6   377    20  -1303us[-1258us] +/-  114ms\nThe PPS might also be OK – but used in a combined way with the selected server, for example. See\nman\nchronyc\nfor more details about how these combinations can look:\nchronyc> sources\n210 Number of sources = 11\nMS Name/IP address         Stratum Poll Reach LastRx Last sample               \n===============================================================================\n#? GPS                           0   4     0     -     +0ns[   +0ns] +/-    0ns\n#+ PPS                           0   4   377    22   +154us[ +154us] +/- 8561us\n^* chilipepper.canonical.com     2   6   377    50   -353us[ -300us] +/-   44ms\nIf you’re wondering if your SHM-based GPS data is any good, you can check on that as well.\nchrony\nwill not only tell you if the data is classified as good or bad – using\nsourcestats\nyou can also check the details:\nchronyc> sourcestats\n210 Number of sources = 10\nName/IP Address            NP  NR  Span  Frequency  Freq Skew  Offset  Std Dev\n==============================================================================\nGPS                        20   9   302     +1.993     11.501   -868ms  1208us\nPPS                         6   3    78     +0.324      5.009  +3365ns    41us\ngolem.canonical.com        15  10   783     +0.859      0.509   -750us   108us\nYou can also track the raw data that\ngpsd\nor other\nntpd\n-compliant reference clocks are sending via shared memory by using\nntpshmmon\n. Let us run the command\nsudo\nntpshmmon\n-o\n, which should provide the following output:\nntpshmmon: version 3.20\n#      Name          Offset       Clock                 Real                 L Prc\nsample NTP1          0.000223854  1588265805.000223854  1588265805.000000000 0 -10\nsample NTP0          0.125691783  1588265805.125999851  1588265805.000308068 0 -20\nsample NTP1          0.000349341  1588265806.000349341  1588265806.000000000 0 -10\nsample NTP0          0.130326636  1588265806.130634945  1588265806.000308309 0 -20\nsample NTP1          0.000485216  1588265807.000485216  1588265807.000000000 0 -10\nNTS Support\n¶\nIn Chrony 4.0 (which first appeared in Ubuntu 21.04 Hirsute) support for\nNetwork Time Security “NTS”\nwas added.\nNTS server\n¶\nTo set up your server with NTS you’ll need certificates so that the server can authenticate itself and, based on that, allow the encryption and verification of NTP traffic.\nIn addition to the\nallow\nstatement that any\nchrony\n(while working as an NTP server) needs there are two mandatory config entries that will be needed. Example certificates for those entries would look like:\nntsservercert /etc/chrony/fullchain.pem\nntsserverkey /etc/chrony/privkey.pem\nIt is important to note that for isolation reasons\nchrony\n, by default, runs as user and group\n_chrony\n. Therefore you need to grant access to the certificates for that user, by running the following command:.\nsudo\nchown\n_chrony:_chrony\n/etc/chrony/*.pem\nThen restart\nchrony\nwith\nsystemctl\nrestart\nchrony\nand it will be ready to provide NTS-based time services.\nA running\nchrony\nserver measures various statistics. One of them counts the number of NTS connections that were established (or dropped) – we can check this by running\nsudo\nchronyc\n-N\nserverstats\n, which shows us the statistics:\nNTP packets received       : 213\nNTP packets dropped        : 0\nCommand packets received   : 117\nCommand packets dropped    : 0\nClient log records dropped : 0\nNTS-KE connections accepted: 2\nNTS-KE connections dropped : 0\nAuthenticated NTP packets  : 197\nThere is also a per-client statistic which can be enabled by the\n-p\noption of the\nclients\ncommand.\nsudo\nchronyc\n-N\nclients\n-k\nThis provides output in the following form:\nHostname                      NTP   Drop Int IntL Last  NTS-KE   Drop Int  Last\n    ===============================================================================\n    10.172.196.173                197      0  10   -   595       2      0   5   48h\n    ...\nFor more complex scenarios there are many more advanced options for configuring NTS. These are documented in the\nchrony.conf(5)\nmanual page.\nNote\nAbout certificate placement\nChrony, by default, is isolated via AppArmor and uses a number of\nprotect*\nfeatures of\nsystemd\n. Due to that, there are not many paths\nchrony\ncan access for the certificates. But\n/etc/chrony/*\nis allowed as read-only and that is enough.\nCheck\n/etc/apparmor.d/usr.sbin.chronyd\nif you want other paths or allow custom paths in\n/etc/apparmor.d/local/usr.sbin.chronyd\n.\nReferences\n¶\nChrony FAQ\nntp.org: home of the Network Time Protocol project\npool.ntp.org: project of virtual cluster of timeservers\nFreedesktop.org info on timedatectl\nFreedesktop.org info on systemd-timesyncd service\nFeeding chrony from GPSD\nSee the\nUbuntu Time\nwiki page for more information.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:22Z", "original_len_words": 2514}}
{"id": "ba3be6bb74", "source_url": "https://documentation.ubuntu.com/server/how-to/networking/timedatectl-and-timesyncd/", "title": "Synchronize time using timedatectl and timesyncd - Ubuntu Server documentation", "text": "Synchronize time using timedatectl and timesyncd - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSynchronize time using timedatectl and timesyncd\n¶\nUbuntu can use\ntimedatectl\nand\ntimesyncd\nfor synchronizing time, which can be installed as follows.\nsystemd-timesyncd\nused to be part of the default installation, but was replaced by\nchrony\nsince Ubuntu 25.10. You can optionally use\nchrony\nas a\nNetwork Time Security (NTS) client\nor to\nserve the Network Time Protocol\n.\nsudo\napt-mark\nauto\nchrony\n&&\napt\ninstall\nsystemd-timesyncd\nIn this guide, we will show you how to configure these services.\nNote\nIf\nchrony\nis installed,\ntimedatectl\nsteps back to let\nchrony\nhandle timekeeping. This ensures that no two time-syncing services will be in conflict.\nCheck status of\ntimedatectl\n¶\nThe current status of time and time configuration via\ntimedatectl\nand\ntimesyncd\ncan be checked with the\ntimedatectl\nstatus\ncommand, which will produce output like this:\nLocal time: Wed 2023-06-14 12:05:11 BST\n           Universal time: Wed 2023-06-14 11:05:11 UTC\n                 RTC time: Wed 2023-06-14 11:05:11\n                Time zone: Europe/Isle_of_Man (BST, +0100)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\nIf\nchrony\nis running, it will automatically switch to:\n[...]\n systemd-timesyncd.service active: no\nConfigure\ntimedatectl\n¶\nBy using\ntimedatectl\n, an admin can control the timezone, how the system clock should relate to the\nhwclock\nand whether permanent synchronization should be enabled. See\nman\ntimedatectl\nfor more details.\nCheck status of\ntimesyncd\n¶\ntimesyncd\nitself is a normal service, so you can check its status in more detail using:\nsystemctl\nstatus\nsystemd\n-\ntimesyncd\nThe output produced will look something like this:\nsystemd\n-\ntimesyncd\n.\nservice\n-\nNetwork\nTime\nSynchronization\nLoaded\n:\nloaded\n(\n/\nlib\n/\nsystemd\n/\nsystem\n/\nsystemd\n-\ntimesyncd\n.\nservice\n;\nenabled\n;\nvendor\npreset\n:\nenabled\n)\nActive\n:\nactive\n(\nrunning\n)\nsince\nFri\n2018\n-\n02\n-\n23\n08\n:\n55\n:\n46\nUTC\n;\n10\ns\nago\nDocs\n:\nman\n:\nsystemd\n-\ntimesyncd\n.\nservice\n(\n8\n)\nMain\nPID\n:\n3744\n(\nsystemd\n-\ntimesyn\n)\nStatus\n:\n\"Synchronized to time server 91.189.89.198:123 (ntp.ubuntu.com).\"\nTasks\n:\n2\n(\nlimit\n:\n4915\n)\nCGroup\n:\n/\nsystem\n.\nslice\n/\nsystemd\n-\ntimesyncd\n.\nservice\n|-\n3744\n/\nlib\n/\nsystemd\n/\nsystemd\n-\ntimesyncd\nFeb\n23\n08\n:\n55\n:\n46\nbionic\n-\ntest\nsystemd\n[\n1\n]:\nStarting\nNetwork\nTime\nSynchronization\n...\nFeb\n23\n08\n:\n55\n:\n46\nbionic\n-\ntest\nsystemd\n[\n1\n]:\nStarted\nNetwork\nTime\nSynchronization\n.\nFeb\n23\n08\n:\n55\n:\n46\nbionic\n-\ntest\nsystemd\n-\ntimesyncd\n[\n3744\n]:\nSynchronized\nto\ntime\nserver\n91.189.89.198\n:\n123\n(\nntp\n.\nubuntu\n.\ncom\n)\n.\nConfigure\ntimesyncd\n¶\nThe server from which to fetch time for\ntimedatectl\nand\ntimesyncd\ncan be specified in\n/etc/systemd/timesyncd.conf\n. Additional config files can be stored in\n/etc/systemd/timesyncd.conf.d/\n. The entries for\nNTP=\nand\nFallbackNTP=\nare space-separated lists. See\nman\ntimesyncd.conf\nfor more details.\nNext steps\n¶\nIf you would now like to serve the Network Time Protocol via\nchrony\n, this guide will walk you through\nhow to install and configure your setup\n.\nReferences\n¶\nFreedesktop.org info on timedatectl\nFreedesktop.org info on systemd-timesyncd service\nSee the\nUbuntu Time wiki page\nfor more information.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:22Z", "original_len_words": 565}}
{"id": "a1407c1165", "source_url": "https://documentation.ubuntu.com/server/how-to/observability/", "title": "Observability - Ubuntu Server documentation", "text": "Observability - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nObservability\n¶\nIn Ubuntu, it is recommended to use the\nCanonical Observability Stack\nto monitor your infrastructure.\nHowever, you can also use the classic Logging, Monitoring and Alerting (LMA) stack.\nSet up your LMA stack\nIf you do not need a full LMA stack, there are some other supported tools that can provide similar capabilities.\nInstall Logwatch\nInstall Munin\nInstall Nagios Core 3\nUse Nagios with Munin", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:22Z", "original_len_words": 98}}
{"id": "5bc3bbfc13", "source_url": "https://documentation.ubuntu.com/server/how-to/observability/install-logwatch/", "title": "How to install and configure Logwatch - Ubuntu Server documentation", "text": "How to install and configure Logwatch - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure Logwatch\n¶\nLogs are an invaluable source of information about problems that may arise in your server.\nLogwatch\nkeeps an eye on your logs for you, flags items that may be of interest, and reports them via email.\nInstall Logwatch\n¶\nInstall\nlogwatch\nusing the following command:\nsudo\napt\ninstall\nlogwatch\nYou will also need to manually create a temporary directory in order for it to work:\nsudo\nmkdir\n/var/cache/logwatch\nConfigure\nlogwatch\n¶\nLogwatch’s default configuration is kept in\n/usr/share/logwatch/default.conf/logwatch.conf\n. However, configuration changes made directly to that file can be overwritten during updates, so instead the file should be copied into\n/etc\nand modified there:\nsudo\ncp\n/\nusr\n/\nshare\n/\nlogwatch\n/\ndefault\n.\nconf\n/\nlogwatch\n.\nconf\n/\netc\n/\nlogwatch\n/\nconf\n/\nWith your favorite editor, open\n/etc/logwatch/conf/logwatch.conf\n.  The uncommented lines indicate the default configuration values.  First, lets customize some of the basics:\nOutput = mail\nMailTo = me@mydomain.org\nMailFrom = logwatch@host1.mydomain.org\nDetail = Low\nService = All\nThis assumes you’ve already set up mail services on\nhost1\nthat will allow mail to be delivered to your\nme@mydomain.org\naddress. These emails will be addressed from\nlogwatch@host1.mydomain.org\n.\nThe\nDetail\nlevel defines how much information is included in the reports. Possible values are:\nLow\n,\nMedium\n, and\nHigh\n.\nLogwatch will then monitor logs for all services on the system, unless specified otherwise with the\nService\nparameter.  If there are undesired services included in the reports, they can be disabled by removing them with additional\nService\nfields. E.g.:\nService = \"-http\"\nService = \"-eximstats\"\nNext, run\nlogwatch\nmanually to verify your configuration changes are valid:\nsudo\nlogwatch\n--detail\nLow\n--range\ntoday\nThe report produced should look something like this:\n################### Logwatch 7.4.3 (12/07/16) ####################\n       Processing Initiated: Fri Apr 24 16:58:14 2020\n       Date Range Processed: today\n                             ( 2020-Apr-24 )\n                             Period is day.\n       Detail Level of Output: 0\n       Type of Output/Format: stdout / text\n       Logfiles for Host: `host1.mydomain.org`\n##################################################################\n \n--------------------- pam_unix Begin ------------------------\n \nsudo:\n   Sessions Opened:\n      bryce -> root: 1 Time(s)\n \n \n---------------------- pam_unix End -------------------------\n \n \n--------------------- rsnapshot Begin ------------------------\n \nERRORS:\n    /usr/bin/rsnapshot hourly: completed, but with some errors: 5 Time(s)\n    /usr/bin/rsync returned 127 while processing root@host2:/etc/: 5 Time(s)\n    /usr/bin/rsync returned 127 while processing root@host2:/home/: 5 Time(s)\n    /usr/bin/rsync returned 127 while processing root@host2:/proc/uptime: 5 Time(s)\n    /usr/bin/rsync returned 127 while processing root@host3:/etc/: 5 Time(s)\n    /usr/bin/rsync returned 127 while processing root@host3:/home/: 5 Time(s)\n    /usr/bin/rsync returned 127 while processing root@host3:/proc/uptime: 5 Time(s)\n \n \n---------------------- rsnapshot End -------------------------\n \n \n--------------------- SSHD Begin ------------------------\n \n \nUsers logging in through sshd:\n   bryce:\n      192.168.1.123 (`host4.mydomain.org`): 1 time\n \n---------------------- SSHD End -------------------------\n \n \n--------------------- Sudo (secure-log) Begin ------------------------\n \n \nbryce => root\n\\-------------\n/bin/bash                      -   1 Time(s).\n \n---------------------- Sudo (secure-log) End -------------------------\n \n \n--------------------- Disk Space Begin ------------------------\n \nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sdc1       220G   19G  190G   9% /\n/dev/loop1      157M  157M     0 100% /snap/gnome-3-28-1804/110\n/dev/loop11     1.0M  1.0M     0 100% /snap/gnome-logs/81\n/dev/md5        9.1T  7.3T  1.8T  81% /srv/Products\n/dev/md6        9.1T  5.6T  3.5T  62% /srv/Archives\n/dev/loop14     3.8M  3.8M     0 100% /snap/gnome-system-monitor/127\n/dev/loop17      15M   15M     0 100% /snap/gnome-characters/399\n/dev/loop18     161M  161M     0 100% /snap/gnome-3-28-1804/116\n/dev/loop6       55M   55M     0 100% /snap/core18/1668\n/dev/md1        1.8T  1.3T  548G  71% /srv/Staff\n/dev/md0        3.6T  3.5T   84G  98% /srv/Backup\n/dev/loop2      1.0M  1.0M     0 100% /snap/gnome-logs/93\n/dev/loop5       15M   15M     0 100% /snap/gnome-characters/495\n/dev/loop8      3.8M  3.8M     0 100% /snap/gnome-system-monitor/135\n/dev/md7        3.6T  495G  3.0T  15% /srv/Customers\n/dev/loop9       55M   55M     0 100% /snap/core18/1705\n/dev/loop10      94M   94M     0 100% /snap/core/8935\n/dev/loop0       55M   55M     0 100% /snap/gtk-common-themes/1502\n/dev/loop4       63M   63M     0 100% /snap/gtk-common-themes/1506\n/dev/loop3       94M   94M     0 100% /snap/core/9066\n\n/srv/Backup (/dev/md0) => 98% Used. Warning. Disk Filling up.\n \n---------------------- Disk Space End -------------------------\n \n \n###################### Logwatch End #########################\nFurther reading\n¶\nThe Ubuntu\nlogwatch(8)\nmanpage contains many more detailed options.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:23Z", "original_len_words": 646}}
{"id": "d6edb2aa59", "source_url": "https://documentation.ubuntu.com/server/how-to/observability/install-munin/", "title": "How to install and configure Munin - Ubuntu Server documentation", "text": "How to install and configure Munin - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure Munin\n¶\nThe monitoring of essential servers and services is an important part of system administration. This guide will show you how to set up\nMunin\nfor performance monitoring.\nIn this example, we will use two servers with\nhostnames\n:\nserver01\nand\nserver02\n.\nServer01\nwill be set up with the\nmunin\npackage to gather information from the network. Using the\nmunin-node\npackage,\nserver02\nwill be configured to send information to\nserver01\n.\nPrerequisites\n¶\nBefore installing Munin on\nserver01\n,\nApache2 will need to be installed\n. The default configuration is fine for running a\nmunin\nserver.\nInstall\nmunin\nand\nmunin-node\n¶\nFirst, on\nserver01\ninstall the\nmunin\npackage. In a terminal enter the following command:\nsudo\napt\ninstall\nmunin\nNow on\nserver02\n, install the\nmunin-node\npackage:\nsudo\napt\ninstall\nmunin-node\nConfigure\nmunin\n¶\nOn\nserver01\nedit the\n/etc/munin/munin.conf\nfile, adding the IP address for\nserver02\n:\n## First our \"normal\" host.\n[server02]\n       address 172.18.100.101\nNote\nReplace\nserver02\nand\n172.18.100.101\nwith the actual hostname and IP address for your server.\nConfigure\nmunin-node\n¶\nNext, configure\nmunin-node\non\nserver02\n. Edit\n/etc/munin/munin-node.conf\nto allow access by\nserver01\n:\nallow ^172\\.18\\.100\\.100$\nNote\nReplace\n^172\\.18\\.100\\.100$\nwith the IP address for your\nmunin\nserver.\nNow restart\nmunin-node\non\nserver02\nfor the changes to take effect:\nsudo\nsystemctl\nrestart\nmunin-node.service\nTest the setup\n¶\nIn a browser, go to\nhttp://server01/munin\n, and you should see links to nice graphs displaying information from the standard\nmunin-plugins\nfor disk, network, processes, and system. However, it should be noted that since this is a new installation, it may take some time for the graphs to display anything useful.\nAdditional Plugins\n¶\nThe\nmunin-plugins-extra\npackage contains performance checks and additional services such as\nDNS\n,\nDHCP\n, and Samba, etc. To install the package, from a terminal enter:\nsudo\napt\ninstall\nmunin-plugins-extra\nBe sure to install the package on both the server and node machines.\nReferences\n¶\nSee the\nMunin\nwebsite for more details.\nSpecifically the\nMunin Documentation\npage includes information on additional plugins, writing plugins, etc.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:23Z", "original_len_words": 381}}
{"id": "9e84a80e1a", "source_url": "https://documentation.ubuntu.com/server/how-to/observability/install-nagios/", "title": "How to install and configure Nagios Core 3 - Ubuntu Server documentation", "text": "How to install and configure Nagios Core 3 - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure Nagios Core 3\n¶\nNote\nNagios Core 3 has been deprecated and is now replaced by Nagios Core 4. The\nnagios3\npackage was last supported in Bionic, so subsequent releases should use\nnagios4\ninstead.\nThe monitoring of essential servers and services is an important part of system administration. This guide walks through how to install and configure Nagios Core 3 for availability monitoring.\nThe example in this guide uses two servers with\nhostnames\n:\nserver01\nand\nserver02\n.\nServer01\nwill be configured with Nagios to monitor services on itself and on\nserver02\n, while\nserver02\nwill be configured to send data to\nserver01\n.\nInstall\nnagios3\non\nserver01\n¶\nFirst, on\nserver01\n, install the\nnagios3\npackage by entering the following command into your terminal:\nsudo\napt\ninstall\nnagios3\nnagios-nrpe-plugin\nYou will be asked to enter a password for the\nnagiosadmin\nuser. The user’s credentials are stored in\n/etc/nagios3/htpasswd.users\n. To change the nagiosadmin password, or add more users to the Nagios CGI scripts, use the\nhtpasswd\nthat is part of the\napache2-utils\npackage.\nFor example, to change the password for the nagiosadmin user, enter:\nsudo\nhtpasswd\n/etc/nagios3/htpasswd.users\nnagiosadmin\nTo add a user:\nsudo\nhtpasswd\n/etc/nagios3/htpasswd.users\nsteve\nInstall\nnagios-nrpe-server\non\nserver02\n¶\nNext, on\nserver02\ninstall the\nnagios-nrpe-server\npackage. From a terminal on\nserver02\nenter:\nsudo\napt\ninstall\nnagios-nrpe-server\nNote\nNRPE allows you to execute local checks on remote hosts. There are other ways of accomplishing this through other Nagios plugins, as well as other checks.\nConfiguration overview\n¶\nThere are a couple of directories containing Nagios configuration and check files.\n/etc/nagios3\n: Contains configuration files for the operation of the Nagios daemon, CGI files, hosts, etc.\n/etc/nagios-plugins\n: Contains configuration files for the service checks.\n/etc/nagios\n: On the remote host, contains the\nnagios-nrpe-server\nconfiguration files.\n/usr/lib/nagios/plugins/\n: Where the check binaries are stored. To see the options of a check use the\n-h\noption. For example:\n/usr/lib/nagios/plugins/check_dhcp\n-h\nThere are multiple checks Nagios can be configured to execute for any given host. For this example, Nagios will be configured to check disk space,\nDNS\n, and a MySQL\nhostgroup\n. The DNS check will be on\nserver02\n, and the MySQL hostgroup will include both\nserver01\nand\nserver02\n.\nNote\nSee these guides for details on\nsetting up Apache\n,\nDomain Name Service\n, and\nMySQL\n.\nAdditionally, there are some terms that once explained will hopefully make understanding Nagios configuration easier:\nHost\n: A server, workstation, network device, etc. that is being monitored.\nHost Group\n: A group of similar hosts. For example, you could group all web servers, file server, etc.\nService\n: The service being monitored on the host, such as HTTP, DNS, NFS, etc.\nService Group\n: Allows you to group multiple services together. This is useful for grouping multiple HTTP for example.\nContact\n: Person to be notified when an event takes place. Nagios can be configured to send emails, SMS messages, etc.\nBy default, Nagios is configured to check HTTP, disk space, SSH, current users, processes, and load on the\nlocalhost\n. Nagios will also ping check the\ngateway\n.\nLarge Nagios installations can be quite complex to configure. It is usually best to start small, with one or two hosts, to get things configured the way you want before expanding.\nConfigure Nagios\n¶\nCreate host config file for server02\n¶\nFirst, create a\nhost\nconfiguration file for\nserver02\n. Unless otherwise specified, run all these commands on\nserver01\n. In a terminal enter:\nsudo\ncp\n/etc/nagios3/conf.d/localhost_nagios2.cfg\n\\\n/etc/nagios3/conf.d/server02.cfg\nNote\nIn all command examples, replace “\nserver01\n”, “\nserver02\n”,\n172.18.100.100\n, and\n172.18.100.101\nwith the host names and IP addresses of your servers.\nEdit the host config file\n¶\nNext, edit\n/etc/nagios3/conf.d/server02.cfg\n:\ndefine host{\n        use                     generic-host  ; Name of host template to use\n        host_name               server02\n        alias                   Server 02\n        address                 172.18.100.101\n}\n        \n# check DNS service.\ndefine service {\n        use                             generic-service\n        host_name                       server02\n        service_description             DNS\n        check_command                   check_dns!172.18.100.101\n}\nRestart the Nagios daemon to enable the new configuration:\nsudo\nsystemctl\nrestart\nnagio3.service\nAdd service definition\n¶\nNow add a service definition for the MySQL check by adding the following to\n/etc/nagios3/conf.d/services_nagios2.cfg\n:\n# check MySQL servers.\ndefine service {\n        hostgroup_name        mysql-servers\n        service_description   MySQL\n        check_command         check_mysql_cmdlinecred!nagios!secret!$HOSTADDRESS\n        use                   generic-service\n        notification_interval 0 ; set > 0 if you want to be renotified\n}\nA\nmysql-servers\nhostgroup now needs to be defined. Edit\n/etc/nagios3/conf.d/hostgroups_nagios2.cfg\nand add the following:\n# MySQL hostgroup.\ndefine hostgroup {\n        hostgroup_name  mysql-servers\n                alias           MySQL servers\n                members         localhost, server02\n        }\nThe Nagios check needs to authenticate to MySQL. To add a\nnagios\nuser to MySQL enter:\nmysql\n-u\nroot\n-p\n-e\n\"create user nagios identified by 'secret';\"\nNote\nThe\nnagios\nuser will need to be added to all hosts in the\nmysql-servers\nhostgroup.\nRestart nagios to start checking the MySQL servers.\nsudo systemctl restart nagios3.service\nConfigure NRPE\n¶\nLastly configure NRPE to check the disk space on\nserver02\n.\nOn\nserver01\nadd the service check to\n/etc/nagios3/conf.d/server02.cfg\n:\n# NRPE disk check.\ndefine service {\n        use                     generic-service\n        host_name               server02\n        service_description     nrpe-disk\n        check_command           check_nrpe_1arg!check_all_disks!172.18.100.101\n}\nNow on\nserver02\nedit\n/etc/nagios/nrpe.cfg\nchanging:\nallowed_hosts=172.18.100.100\nAnd below, in the command definition area, add:\ncommand[check_all_disks]=/usr/lib/nagios/plugins/check_disk -w 20% -c 10% -e\nFinally, restart\nnagios-nrpe-server\n:\nsudo\nsystemctl\nrestart\nnagios-nrpe-server.service\nAlso, on\nserver01\nrestart Nagios:\nsudo\nsystemctl\nrestart\nnagios3.service\nYou should now be able to see the host and service checks in the Nagios CGI files. To access them, point a browser to\nhttp://server01/nagios3\n. You will then be prompted for the\nnagiosadmin\nusername and password.\nFurther reading\n¶\nThis section has just scratched the surface of Nagios’ features. The\nnagios-plugins-extra\nand\nnagios-snmp-plugins\ncontain many more service checks.\nFor more information about Nagios, see\nthe Nagios website\n.\nThe\nNagios Core Documentation\nand\nNagios Core 3 Documentation\nmay also be useful.\nThey also provide a\nlist of books\nrelated to Nagios and network monitoring.\nThe\nNagios Ubuntu Wiki\npage also has more details.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:23Z", "original_len_words": 1027}}
{"id": "b06d3eb7c3", "source_url": "https://documentation.ubuntu.com/server/how-to/observability/set-up-your-lma-stack/", "title": "Set up your LMA stack - Ubuntu Server documentation", "text": "Set up your LMA stack - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSet up your LMA stack\n¶\nNote\nLMA to COS\nThe LMA stack is being succeeded by the Canonical Observability Stack (COS). While the current LMA still works, most users are recommended to consider COS instead. For more information, refer to\nthis COS topic\n. In environments with more limited resources, there is also\nCOS lite\n.\nLogging, Monitoring, and Alerting (LMA) is a collection of tools that guarantee the availability of your running infrastructure. Your LMA stack will help point out issues in load, networking, and other resources before they become a failure point.\nArchitectural overview\n¶\nCanonical’s LMA stack involves several discrete software services acting in concert.\nTelegraf\ncollects metrics from the operating system, running software, and other inputs. Its plugin system permits export of data in any arbitrary format; for this system we collect the data in a central data manager called\nPrometheus\n.\nPrometheus works as a hub, polling data from different Telegraf nodes and sending it to various outputs, including persistent storage. For this LMA stack, visualization is handled via\nGrafana\nand email/pager alerts are generated via the Prometheus\nAlertmanager\nplugin. See\nPrometheus Alertmanager\nfor more details.\nGetting started\n¶\nLet’s set up a basic demonstration with two\nnodes\n, the first acting as a placeholder load with Telegraf installed - the “Workload”, and the second acting as our data visualization system - the “Monitor”. This will help us familiarize ourselves with the various components and how they inter-operate.\nNote\nFor clarity, we’ll refer to these two hosts as named:\nworkload\nand\nmonitor\n. If you use other\nhostnames\n, substitute your preferred names as we go through this guide.\nThe Workload node will be running Telegraf to collect metrics from whatever load we’re monitoring. For demonstration purposes we’ll just read the CPU/memory data from the node. In a real environment, we’d have multiple hosts (each with their own Telegraf instance) collecting hardware, network, and software statuses particular to that node.\nOur Monitor node will double as both a data store and a web UI, receiving data from the Workload, storing it to disk, and displaying it for analysis.\nPorts\n¶\nAs reference, here are the ports we’ll be binding for each service:\nPrometheus\nmonitor:9090\nAlertmanager\nmonitor:9093\nGrafana\nmonitor:3000\nTelegraf\nworkload:9273\nSet up the Workload node\n¶\nFirst, let’s set up the Workload. We’ll be using LXD as our container technology in this guide, but any VM, container, or bare metal host should work, so long as it’s running Ubuntu 20.10. With LXD installed on our host we can use its\nlxc\ncommand line tool to create our containers:\n$\nlxc\nlaunch\nubuntu:20.10\nworkload\nCreating\nworkload\nStarting\nworkload\n\n$\nlxc\nexec\nworkload\n--\nbash\nworkload:~#\nOn the Workload, install Telegraf:\nworkload:~#\napt\nupdate\nworkload:~#\napt\ninstall\ntelegraf\nTelegraf processes input data to transform, filter, and decorate it, and then performs selected aggregation functions on it such as tallies, averages, etc. The results are published for collection by external services; in our case Prometheus will be collecting the CPU/memory data from the Monitor node.\nOpen\n/etc/telegraf/telegraf.conf\nand scroll down to the “INPUT PLUGINS” section. What we’ll need is the following configuration settings, which you should find already enabled by default:\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\nLooking at the config file you’ll notice it’s almost entirely commented out. There are three different types of sections in the file:\n[[inputs]]\n, which we set above;\n[[outputs]]\n, which we’ll set up next; and the\n[[agent]]\nsetting, with several performance tuning parameters such as the collection interval, which we’re setting to 10 seconds. The agent defaults are fine for our example and for basic use.\nFinally, we need to define where Telegraf will provide its output. Open\n/etc/telegraf/telegraf.conf\nand scroll down to the “OUTPUT PLUGINS” section and add the following output configuration:\n[[outputs.prometheus_client]]\n  listen = \"workload:9273\"\n  metric_version = 2\n\n#[[outputs.influxdb]]\nWe won’t be using\nInfluxdb\n, so you can comment that section out (if it’s enabled).\nNow restart the Telegraf service:\nworkload:~#\nsystemctl\nrestart\ntelegraf\nworkload:~#\nsystemctl\nstatus\ntelegraf\n●\ntelegraf.service\n-\nThe\nplugin-driven\nserver\nagent\nfor\nreporting\nmetrics\ninto\nInfluxDB\nLoaded:\nloaded\n(\n/lib/systemd/system/telegraf.service\n;\nenabled\n;\nvendor\npreset:\nenabled\n)\nActive:\nactive\n(\nrunning\n)\nsince\nSat\n2020\n-10-31\n02\n:17:57\nUTC\n;\n6s\nago\nDocs:\nhttps://github.com/influxdata/telegraf\nMain\nPID:\n2562\n(\ntelegraf\n)\nTasks:\n17\n(\nlimit:\n77021\n)\nMemory:\n42\n.2M\nCGroup:\n/system.slice/telegraf.service\n└─2562\n/usr/bin/telegraf\n-config\n/etc/telegraf/telegraf.conf\n-config-directory\n/etc/telegraf/telegraf.d\n\n...I!\nLoaded\ninputs:\nswap\nsystem\ncpu\ndisk\ndiskio\nkernel\nmem\nprocesses\n...I!\nLoaded\noutputs:\nhttp\nprometheus_client\n...I!\n[\nagent\n]\nConfig:\nInterval:10s,\nQuiet:false,\nHostname:\n\"workload\"\n,\nFlush\nInterval:10s\n...I!\n[\noutputs.prometheus_client\n]\nListening\non\nhttp://127.0.0.1:9273/metrics\nVerify that it is collecting metrics by connecting to Telegraf’s web interface:\nworkload:~#\nwget\n-O-\nhttp://workload:9273/metrics\n# HELP cpu_usage_guest Telegraf collected metric\n# TYPE cpu_usage_guest gauge\ncpu_usage_guest\n{\ncpu\n=\n\"cpu-total\"\n,host\n=\n\"workload\"\n}\n0\ncpu_usage_guest\n{\ncpu\n=\n\"cpu0\"\n,host\n=\n\"workload\"\n}\n0\ncpu_usage_guest\n{\ncpu\n=\n\"cpu1\"\n,host\n=\n\"workload\"\n}\n0\ncpu_usage_guest\n{\ncpu\n=\n\"cpu10\"\n,host\n=\n\"workload\"\n}\n0\n...\ncpu_usage_idle\n{\ncpu\n=\n\"cpu-total\"\n,host\n=\n\"workload\"\n}\n92\n.74914376428686\ncpu_usage_idle\n{\ncpu\n=\n\"cpu0\"\n,host\n=\n\"workload\"\n}\n86\n.72897196325539\ncpu_usage_idle\n{\ncpu\n=\n\"cpu1\"\n,host\n=\n\"workload\"\n}\n90\n.11857707405758\ncpu_usage_idle\n{\ncpu\n=\n\"cpu10\"\n,host\n=\n\"workload\"\n}\n95\n.95141700494543\nSet up the Monitor node\n¶\nNow let’s create the Monitor. As before, we’ll be using LXD as the container technology but feel free to adapt these steps to your chosen alternative:\n$\nlxc\nlaunch\nubuntu:20.10\nmonitor\nCreating\nmonitor\nStarting\nmonitor\n$\nlxc\nexec\nmonitor\n--\nbash\nmonitor:~#\nMake a note of the newly created container’s IP address, which we’ll need later on;\nmonitor:~#\nip\naddr\n|\ngrep\n'inet .* global'\ninet\n10\n.69.244.104/24\nbrd\n10\n.69.244.255\nscope\nglobal\ndynamic\neth0\nVerify the Workload’s Telegraf instance can be reached from the Monitor:\nmonitor:~#\nwget\n-O-\nhttp://workload:9273/metrics\nWe’ll be setting up a few components to run on this node using their respective Snap packages. LXD images should normally have snap pre-installed, but if not, install it manually:\nmonitor:~#\napt\ninstall\nsnapd\nInstall Prometheus\n¶\nPrometheus will be our data manager. It collects data from external sources – Telegraf in our case – and distributes it to various destinations such as email/pager alerts, web UIs, API clients, remote storage services, etc. We’ll get into those shortly.\nLet’s install Prometheus itself, and the Prometheus Alertmanager plugin for alerts, along with the required dependencies:\nmonitor:~#\nsnap\ninstall\nprometheus\nmonitor:~#\nsnap\ninstall\nprometheus-alertmanager\nThe snap will automatically configure and start the service. To verify this, run:\nmonitor:~#\nsnap\nservices\nService\nStartup\nCurrent\nNotes\nlxd.activate\nenabled\ninactive\n-\nlxd.daemon\nenabled\ninactive\nsocket-activated\nprometheus.prometheus\nenabled\nactive\n-\nprometheus-alertmanager.alertmanager\nenabled\nactive\n-\nVerify that Prometheus is listening on the port as we expect:\nvisualizer:~#\nss\n-tulpn\n|\ngrep\nprometheus\ntcp\nLISTEN\n0\n128\n*:9090\n*:*\nusers:\n((\n\"prometheus\"\n,pid\n=\n618\n,fd\n=\n8\n))\njournalctl\ncan be also used to review the state of Snap services if more detail is needed. For example, to see where Prometheus is loading its config from:\nmonitor:~#\njournalctl\n|\ngrep\n\"prometheus.*config\"\n...\n...msg\n=\n\"Completed loading of configuration file\"\nfilename\n=\n/var/snap/prometheus/32/prometheus.yml\nAlthough the file name points to a specific Snap revision (\n32\n, in this case), we can use the generic config file\n/var/snap/prometheus/current/prometheus.yml\nhere in order to make things more general. Edit this config file to register the targets we’ll be reading data from. This will go under the\nscrape_configs\nsection of the file:\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: 'prometheus'\n\n\t# metrics_path defaults to '/metrics'\n\t# scheme defaults to 'http'.\n\n\tstatic_configs:\n\t- targets: ['localhost:9090']\n\n  - job_name: 'telegraf'\n\tstatic_configs:\n\t- targets: ['workload:9273']\nThen restart Prometheus:\nmonitor:~#\nsnap\nrestart\nprometheus\nWhile we’ll be using Grafana for visualization, Prometheus also has a web interface for viewing and interacting with the collected data. At this stage, we can load it to verify that our setup is working properly. In a web browser, navigate to the Monitor’s IP address, and port\n9090\n. You should see Prometheus’ interface, as in the following image:\nIn the entry box, enter\ncpu_usage_system\n, select the “Graph” tab and click “Execute”. This should show a graph of our collected CPU data so far. Prometheus also has a secondary web UI using\nReact.js\n.\nConfigure Alertmanager\n¶\nLet’s tackle the Alert Manager next. Edit\n/var/snap/prometheus/current/prometheus.yml\nagain, adding the following to the\nalerting\nand\nrules_files\nsections:\n## /var/snap/prometheus/current/prometheus.yml\n#...\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n  - static_configs:\n\t- targets:\n\t  - 127.0.0.1:9093\nrule_files:\n  - 'alerts.yml'\nNow create\n/var/snap/prometheus/current/alerts.yml\nwith the following contents:\n## /var/snap/prometheus/current/alerts.yml\ngroups:\n- name: demo-alerts\n  rules:\n  - alert: HighLoad\n\texpr: node_load1 > 2.0\n\tfor: 60m\n\tlabels:\n\t  severity: normal\n\tannotations:\n\t  description: '{{ $labels.instance }} of job {{ $labels.job }} is under high load.'\n\t  summary: Instance {{ $labels.instance }} under high load.\n\t  value: '{{ $value }}'\n\n  - alert: InstanceDown\n\texpr: up == 0\n\tfor: 5m\n\tlabels:\n\t  severity: major\n\tannotations:\n\t  summary: \"Instance {{ $labels.instance }} down\"\n\t  description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\nThis adds two alerts: one for high processor load, and one to report if the node has been unreachable for over 5 minutes. We’re considering high CPU to be a load of 2 or higher for an hour; this would need to be set to something more sensible for the style of workloads your production system experiences.\nWith the alerts themselves now defined, we next need to instruct Alertmanager how to handle them. There is a sample configuration installed to\n/var/snap/prometheus-alertmanager/current/alertmanager.yml\n, however it’s full of example data. Instead, replace it entirely with this content:\n## /var/snap/prometheus-alertmanager/current/alertmanager.yml\nglobal:\n  resolve_timeout: 5m\n\nroute:\n  group_by: ['alertname']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 1h\n\ninhibit_rules:\n  - source_match:\n\t  severity: 'critical'\n\ttarget_match:\n\t  severity: 'warning'\n\tequal: ['alertname', 'dev', 'instance']\nRestart Alertmanager after making the configuration change:\nworkload:~#\nsnap\nrestart\nprometheus-alertmanager\nInstall Grafana\n¶\nGrafana provides our main dashboard, from which we can generate graphs and other visuals to study the collected metrics. Grafana can read its data directly from log files, but we’ll focus on using Prometheus as its principle data source. Grafana is available as a Snap and can be installed like this:\nmonitor:~#\nsnap\ninstall\ngrafana\ngrafana\n6\n.7.4\nfrom\nAlvaro\nUría\n(\naluria\n)\ninstalled\nIt uses port\n3000\n:\n# ss -tulpn | grep grafana\ntcp\nLISTEN\n0\n128\n*:3000\n*:*\nusers:\n((\n\"grafana-server\"\n,pid\n=\n1449\n,fd\n=\n10\n))\nWe next need to know where it expects its configuration:\nmonitor:~#\njournalctl\n|\ngrep\n\"grafana.*conf\"\n...\nmsg\n=\n\"Config loaded from\"\nlogger\n=\nsettings\nfile\n=\n/snap/grafana/36/conf/defaults.ini\n...\nmsg\n=\n\"Config overridden from Environment variable\"\nlogger\n=\nsettings\nvar\n=\n\"GF_PATHS_PROVISIONING=/var/snap/grafana/common/conf/provisioning\"\n...\nerror\n=\n\"open /var/snap/grafana/common/conf/provisioning/datasources: no such file or directory\"\n...\nWe can see it is getting its defaults from\n/snap/grafana/36/conf/\n, but\n/snap/\nis a read-only directory and therefore we cannot edit the file. Instead, we should put our customisations inside\n/var/snap/grafana/36/conf/grafana.ini\n. You can also use the generic path\n/var/snap/grafana/current/conf/grafana.ini\n.\nFor a production installation, the\ndefaults.ini\nhas numerous parameters we’d want to customize for our site, however for the demo we’ll accept all the defaults. We do need to configure our data sources, but can do this via the web interface:\n$\nfirefox\nhttp://10.69.244.104:3000\nLog in with ‘admin’ and ‘admin’ as the username and password. This should bring you to the main Grafana page, where you can find links to tutorials and documentation. Delete any example data sources and/or dashboards.\nSelect the button to add a new data source and select “Prometheus”. On the “Data Sources / Prometheus” edit page, set:\nthe name to Prometheus\nthe URL to\nhttp://localhost:9090\n‘Access’ to “Server (default)” to make Grafana pull data from the Prometheus service we set up.\nThe remaining settings can be left as defaults. Click “Save & Test”.\nReturning to the Grafana home page, next set up a “New Dashboard”. A dashboard can hold one or more panels, each of which can be connected to one or more data queries. Let’s add a panel for CPU data. For the query, enter “cpu_usage_system” in the Metrics field.\nOn the left you can see four buttons to configure four elements of the panel: data source, visualization, general settings, and alerts. The general settings page allows us to set a title for the panel, for instance. Make any other customizations you want, and then save the dashboard using the save icon at the top of the page.\nUsing the same procedure, add additional panels for processor load and memory usage. Panels can be used to present other types of data as well, such as numerical indicators, logs, newsfeeds, or markdown-formatted documentation. For example, you can add a panel to display the system uptime, such as in the following image:\nTry also adding a panel with the “Text” visualization option for entering descriptive text about our demo. Save, and then view the final dashboard:", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:23Z", "original_len_words": 2199}}
{"id": "b8c3a0dcb8", "source_url": "https://documentation.ubuntu.com/server/how-to/observability/use-nagios-with-munin/", "title": "How to use Nagios with Munin - Ubuntu Server documentation", "text": "How to use Nagios with Munin - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to use Nagios with Munin\n¶\nNote\nNagios Core 3 has been deprecated and is now replaced by Nagios Core 4. The\nnagios3\npackage was last supported in Bionic, so subsequent releases should use\nnagios4\ninstead.\nThe monitoring of essential servers and services is an important part of system administration. Most network services are monitored for performance, availability, or both. This section will cover installation and configuration of Nagios 3 for availability monitoring alongside Munin for performance monitoring.\nThe examples in this section will use two servers with hostnames\nserver01\nand\nserver02\n.\nServer01\nwill be configured with Nagios 3 to monitor services on both itself and\nserver02\n.\nServer01\nwill also be set up with the Munin package to gather information from the network. Using the\nmunin-node\npackage,\nserver02\nwill be configured to send information to\nserver01\n.\nInstall Nagios 3\n¶\nOn server01\n¶\nFirst, on\nserver01\n, install the\nnagios3\npackage. In a terminal, enter:\nsudo\napt\ninstall\nnagios3\nnagios-nrpe-plugin\nYou will be asked to enter a password for the\nnagiosadmin\nuser. The user’s credentials are stored in\n/etc/nagios3/htpasswd.users\n. To change the\nnagiosadmin\npassword, or add additional users to the Nagios CGI scripts, use the\nhtpasswd\nthat is part of the\napache2-utils\npackage.\nFor example, to change the password for the\nnagiosadmin\nuser enter:\nsudo\nhtpasswd\n/etc/nagios3/htpasswd.users\nnagiosadmin\nTo add a user:\nsudo\nhtpasswd\n/etc/nagios3/htpasswd.users\nsteve\nOn server02\n¶\nNext, on\nserver02\ninstall the\nnagios-nrpe-server\npackage. From a terminal on\nserver02\n, enter:\nsudo\napt\ninstall\nnagios-nrpe-server\nNote\nNRPE allows you to run local checks on remote hosts. There are other ways of accomplishing this, including through other Nagios plugins.\nConfiguration overview\n¶\nThere are a few directories containing Nagios configuration and check files.\n/etc/nagios3\n: contains configuration files for the operation of the Nagios daemon, CGI files, hosts, etc.\n/etc/nagios-plugins\n: houses configuration files for the service checks.\n/etc/nagios\n: is located on the remote host and contains the\nnagios-nrpe-server\nconfiguration files.\n/usr/lib/nagios/plugins/\n: where the check binaries are stored. To see the options of a check use the\n-h\noption.\nFor example:\n/usr/lib/nagios/plugins/check_dhcp\n-h\nThere are many checks Nagios can be configured to run for any particular host. In this example, Nagios will be configured to check disk space,\nDNS\n, and a MySQL\nhost group\n. The DNS check will be on\nserver02\n, and the MySQL host group will include both\nserver01\nand\nserver02\n.\nNote\nSee these additional guides for details on setting up\nApache\n,\nDomain Name Service (DNS)\n, and\nMySQL\n.\nAdditionally, there are some terms that once explained will hopefully make understanding Nagios configuration easier:\nHost\n: a server, workstation, network device, etc that is being monitored.\nHost group\n: a group of similar hosts. For example, you could group all web servers, file servers, etc.\nService\n: the service being monitored on the host, such as HTTP, DNS, NFS, etc.\nService group\n: allows you to group multiple services together. This is useful for grouping, e.g.,  multiple HTTP.\nContact\n: the person to be notified when an event takes place. Nagios can be configured to send emails, SMS messages, etc.\nBy default Nagios is configured to check HTTP, disk space, SSH, current users, processes, and load on the\nlocalhost\n. Nagios will also ping-check the\ngateway\n.\nLarge Nagios installations can be quite complex to configure. It is usually best to start small (i.e. with one or two hosts), get things configured the way you like, and then expand.\nConfigure Nagios\n¶\nFirst, create a\nhost\nconfiguration file for\nserver02\n. Unless otherwise specified, run all these commands on\nserver01\n. In a terminal enter:\nsudo\ncp\n/etc/nagios3/conf.d/localhost_nagios2.cfg\n\\\n/etc/nagios3/conf.d/server02.cfg\nNote\nIn the above and following command examples, replace “\nserver01\n”, “\nserver02\n”,\n172.18.100.100\n, and\n172.18.100.101\nwith the host names and IP addresses of your servers.\nNext, edit\n/etc/nagios3/conf.d/server02.cfg\n:\ndefine host{\n        use                     generic-host  ; Name of host template to use\n        host_name               server02\n        alias                   Server 02\n        address                 172.18.100.101\n}\n        \n# check DNS service.\ndefine service {\n        use                             generic-service\n        host_name                       server02\n        service_description             DNS\n        check_command                   check_dns!172.18.100.101\n}\nRestart the Nagios daemon to enable the new configuration:\nsudo\nsystemctl\nrestart\nnagio3.service\nNow add a service definition for the MySQL check by adding the following to\n/etc/nagios3/conf.d/services_nagios2.cfg\n:\n# check MySQL servers.\ndefine service {\n        hostgroup_name        mysql-servers\n        service_description   MySQL\n        check_command         check_mysql_cmdlinecred!nagios!secret!$HOSTADDRESS\n        use                   generic-service\n        notification_interval 0 ; set > 0 if you want to be renotified\n}\nA\nmysql-servers\nhost group now needs to be defined. Edit\n/etc/nagios3/conf.d/hostgroups_nagios2.cfg\nadding:\n# MySQL hostgroup.\ndefine hostgroup {\n        hostgroup_name  mysql-servers\n                alias           MySQL servers\n                members         localhost, server02\n}\n\nThe Nagios check needs to authenticate to MySQL. To add a `nagios` user to MySQL, enter:\n\n```bash\nmysql -u root -p -e \"create user nagios identified by 'secret';\"\n    \n```{note}\nThe `nagios` user will need to be added all hosts in the `mysql-servers` host group.\nRestart Nagios to start checking the MySQL servers.\nsudo\nsystemctl\nrestart\nnagios3.service\nLastly, configure NRPE to check the disk space on\nserver02\n. On\nserver01\nadd the service check to\n/etc/nagios3/conf.d/server02.cfg\n:\n# NRPE disk check.\ndefine service {\n        use                     generic-service\n        host_name               server02\n        service_description     nrpe-disk\n        check_command           check_nrpe_1arg!check_all_disks!172.18.100.101\n}\nNow on\nserver02\nedit\n/etc/nagios/nrpe.cfg\n, changing:\nallowed_hosts=172.18.100.100\nAnd below in the command definition area add:\ncommand[check_all_disks]=/usr/lib/nagios/plugins/check_disk -w 20% -c 10% -e\nFinally, restart\nnagios-nrpe-server\n:\nsudo\nsystemctl\nrestart\nnagios-nrpe-server.service\nAlso, on\nserver01\nrestart\nnagios3\n:\nsudo\nsystemctl\nrestart\nnagios3.service\nYou should now be able to see the host and service checks in the Nagios CGI files. To access them, point a browser to\nhttp://server01/nagios3\n. You will then be prompted for the\nnagiosadmin\nusername and password.\nInstall Munin\n¶\nBefore installing Munin on\nserver01\nApache2 will need to be installed. The default configuration is fine for running a Munin server. For more information see\nsetting up Apache\n.\nOn server01\n¶\nFirst, on\nserver01\ninstall\nmunin\nby running the following command in a terminal:\nsudo\napt\ninstall\nmunin\nOn server02\n¶\nNow on\nserver02\ninstall the\nmunin-node\npackage:\nsudo\napt\ninstall\nmunin-node\nConfigure Munin on server01\n¶\nOn\nserver01\nedit the\n/etc/munin/munin.conf\nto add the IP address for\nserver02\n:\n## First our \"normal\" host.\n[server02]\n       address 172.18.100.101\nNote\nReplace\nserver02\nand\n172.18.100.101\nwith the actual hostname and IP address of your server.\nConfigure munin-node on server02\n¶\nTo configure\nmunin-node\non\nserver02\n, edit\n/etc/munin/munin-node.conf\nto allow access by\nserver01\n:\nallow ^172\\.18\\.100\\.100$\nNote\nReplace\n^172\\.18\\.100\\.100$\nwith IP address for your Munin server.\nNow restart\nmunin-node\non\nserver02\nfor the changes to take effect:\nsudo\nsystemctl\nrestart\nmunin-node.service\nFinally, in a browser go to\nhttp://server01/munin\n, and you should see links to some graphs displaying information from the standard\nmunin-plugins\nfor disk, network, processes, and system.\nNote\nSince this is a new install it may take some time for the graphs to display anything useful.\nAdditional plugins\n¶\nThe\nmunin-plugins-extra\npackage contains performance checks and additional services such as DNS,\nDHCP\n, Samba, etc. To install the package, from a terminal enter:\nsudo\napt\ninstall\nmunin-plugins-extra\nBe sure to install the package on both the server and node machines.\nFurther reading\n¶\nSee the\nMunin\nand\nNagios\nwebsites for more details on these packages.\nThe\nMunin Documentation\npage includes information on additional plugins, writing plugins, etc.\nThe\nNagios Online Documentation\nsite.\nThere is also a\nlist of books\nrelated to Nagios and network monitoring.\nThe\nNagios Ubuntu Wiki\npage also has more details.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:24Z", "original_len_words": 1270}}
{"id": "23dd561209", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/", "title": "OpenLDAP - Ubuntu Server documentation", "text": "OpenLDAP - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nOpenLDAP\n¶\nThe Lightweight Directory Access Protocol, or LDAP, is a protocol for managing hierarchical data and accessing directories. The open source implementation used in Ubuntu is OpenLDAP.\nThese guides are intended to be sequential, so following them in the order presented below is suggested.\nInstall OpenLDAP\nSet up access control\nOpenLDAP with replication\nUser and group management\nOpenLDAP and TLS\nBackup and restore\nIntroduction to passthrough authentication\nPassthrough authentication with Kerberos\nSee also\n¶\nHow-to:\nHow to set up SSSD with LDAP\nExplanation:\nIntroduction to OpenLDAP", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:24Z", "original_len_words": 118}}
{"id": "d571c0b4c1", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/access-control/", "title": "Set up LDAP access control - Ubuntu Server documentation", "text": "Set up LDAP access control - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSet up LDAP access control\n¶\nThe management of what type of access (read, write, etc) users should be granted for resources is known as\naccess control\n. The configuration directives involved are called\naccess control lists\nor\nACL\ns.\nWhen we\ninstalled the slapd package\n, various ACLs were set up automatically. We will look at a few important consequences of those defaults and, in so doing, we’ll get an idea of how ACLs work and how they’re configured.\nTo get the effective ACL for an LDAP query we need to look at the ACL entries of both the database being queried, and those of the special\nfrontend\ndatabase instance. Note that the ACLs belonging to the frontend database are always appended to the database-specific ACLs, and the first match ‘wins’.\nGetting the ACLs\n¶\nThe following commands will give, respectively, the ACLs of the\nmdb\ndatabase (\ndc=example,dc=com\n) and those of the frontend database:\n$\nsudo\nldapsearch\n-Q\n-LLL\n-Y\nEXTERNAL\n-H\nldapi:///\n-b\n\\\ncn\n=\nconfig\n'(olcDatabase={1}mdb)'\nolcAccess\ndn:\nolcDatabase\n={\n1\n}\nmdb,cn\n=\nconfig\nolcAccess:\n{\n0\n}\nto\nattrs\n=\nuserPassword\nby\nself\nwrite\nby\nanonymous\nauth\nby\n*\nnone\nolcAccess:\n{\n1\n}\nto\nattrs\n=\nshadowLastChange\nby\nself\nwrite\nby\n*\nread\nolcAccess:\n{\n2\n}\nto\n*\nby\n*\nread\n$\nsudo\nldapsearch\n-Q\n-LLL\n-Y\nEXTERNAL\n-H\nldapi:///\n-b\n\\\ncn\n=\nconfig\n'(olcDatabase={-1}frontend)'\nolcAccess\ndn:\nolcDatabase\n={\n-1\n}\nfrontend,cn\n=\nconfig\nolcAccess:\n{\n0\n}\nto\n*\nby\ndn.exact\n=\ngidNumber\n=\n0\n+uidNumber\n=\n0\n,cn\n=\npeercred,cn\n=\nexternal\n,cn\n=\nauth\nmanage\nby\n*\nbreak\nolcAccess:\n{\n1\n}\nto\ndn.exact\n=\n\"\"\nby\n*\nread\nolcAccess:\n{\n2\n}\nto\ndn.base\n=\n\"cn=Subschema\"\nby\n*\nread\nNote\nThe Root\nDN\nalways has full rights to its database and does not need to be included in any ACL.\nInterpreting the results\n¶\nThe first two ACLs are crucial:\nolcAccess: {0}to attrs=userPassword by self write by anonymous auth by * none\nolcAccess: {1}to attrs=shadowLastChange by self write by * read\nThis can be represented differently for easier reading:\nto attrs=userPassword\n    by self write\n    by anonymous auth\n    by * none\n    \nto attrs=shadowLastChange\n    by self write\n    by * read\nThese ACLs enforce the following:\nAnonymous ‘auth’ access is provided to the\nuserPassword\nattribute so that users can authenticate, or\nbind\n. Perhaps counter-intuitively, ‘by anonymous auth’ is needed even when anonymous access to the\nDIT\nis unwanted, otherwise this would be a chicken-and-egg problem: before authentication, all users are anonymous.\nThe ‘by self write’ ACL grants write access to the\nuserPassword\nattribute to users who authenticated as the DN where the attribute lives. In other words, users can update the\nuserPassword\nattribute of their own entries.\nThe\nuserPassword\nattribute is otherwise inaccessible by all other users, with the exception of the Root DN, who always has access and doesn’t need to be mentioned explicitly.\nIn order for users to change their own password, using\npasswd\nor other utilities, the user’s own\nshadowLastChange\nattribute needs to be writable. All other directory users get to read this attribute’s contents.\nThis DIT can be searched anonymously because of\nto\n*\nby\n*\nread\nin this ACL, which grants read access to everything else, by anyone (including anonymous):\nto *\n    by * read\nIf this is unwanted then you need to change the ACL. To force authentication during a bind request you can alternatively (or in combination with the modified ACL) use the\nolcRequire:\nauthc\ndirective.\nSASL identity\n¶\nThere is no administrative account (“Root DN”) created for the\nslapd-config\ndatabase. There is, however, a SASL identity that is granted full access to it. It represents the localhost’s superuser (\nroot\n/\nsudo\n). Here it is:\ndn.exact=gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\nThe following command will display the ACLs of the\nslapd-config\ndatabase:\n$\nsudo\nldapsearch\n-Q\n-LLL\n-Y\nEXTERNAL\n-H\nldapi:///\n-b\n\\\ncn\n=\nconfig\n'(olcDatabase={0}config)'\nolcAccess\ndn:\nolcDatabase\n={\n0\n}\nconfig,cn\n=\nconfig\nolcAccess:\n{\n0\n}\nto\n*\nby\ndn.exact\n=\ngidNumber\n=\n0\n+uidNumber\n=\n0\n,cn\n=\npeercred,\ncn\n=\nexternal,cn\n=\nauth\nmanage\nby\n*\nbreak\nSince this is a SASL identity we need to use a SASL\nmechanism\nwhen invoking the LDAP utility in question – the\nEXTERNAL\nmechanism (see the previous command for an example). Note that:\nYou must use\nsudo\nto become the root identity in order for the ACL to match.\nThe EXTERNAL mechanism works via\nInterprocess Communication\n(IPC, UNIX domain sockets). This means you must use the\nldapi\nURI format.\nA succinct way to get all the ACLs is like this:\n$\nsudo\nldapsearch\n-Q\n-LLL\n-Y\nEXTERNAL\n-H\nldapi:///\n-b\n\\\ncn\n=\nconfig\n'(olcAccess=*)'\nolcAccess\nolcSuffix\nNext steps\n¶\nSee how to\nset up LDAP users and groups\n.\nFurther reading\n¶\nSee the manual page for\nslapd.access(5)\nThe\naccess control topic\nin the OpenLDAP administrator’s guide.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:24Z", "original_len_words": 853}}
{"id": "f2d527da0b", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/backup-and-restore/", "title": "Backup and restore OpenLDAP - Ubuntu Server documentation", "text": "Backup and restore OpenLDAP - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nBackup and restore OpenLDAP\n¶\nNow we have LDAP running just the way we want, it is time to ensure we can save all of our work and restore it as needed.\nWhat we need is a way to back up the directory database(s) – specifically the configuration backend (\ncn=config\n) and the\nDIT\n(\ndc=example,dc=com\n). If we are going to backup those databases into, say,\n/export/backup\n, we could use\nslapcat\nas shown in the following script, called\n/usr/local/bin/ldapbackup\n:\n#!/bin/bash\nset\n-e\nBACKUP_PATH\n=\n/export/backup\nSLAPCAT\n=\n/usr/sbin/slapcat\n\nnice\n${\nSLAPCAT\n}\n-b\ncn\n=\nconfig\n>\n${\nBACKUP_PATH\n}\n/config.ldif\nnice\n${\nSLAPCAT\n}\n-b\ndc\n=\nexample,dc\n=\ncom\n>\n${\nBACKUP_PATH\n}\n/example.com.ldif\nchown\nroot:root\n${\nBACKUP_PATH\n}\n/*\nchmod\n600\n${\nBACKUP_PATH\n}\n/*.ldif\nNote\nThese files are uncompressed text files containing everything in your directory including the tree layout, usernames, and every password. So, you might want to consider making\n/export/backup\nan encrypted partition and even having the script encrypt those files as it creates them. Ideally you should do both, but that depends on your security requirements.\nThen, it is just a matter of having a cron script to run this program as often as you feel comfortable with. For many, once a day suffices. For others, more often is required. Here is an example of a cron script called\n/etc/cron.d/ldapbackup\nthat is run every night at 22:45h:\nMAILTO=backup-emails@domain.com\n45 22 * * *  root    /usr/local/bin/ldapbackup\nNow the files are created, they should be copied to a backup server.\nAssuming we did a fresh reinstall of LDAP, the restore process could be something like this:\n#!/bin/bash\nset\n-e\nBACKUP_PATH\n=\n/export/backup\nSLAPADD\n=\n/usr/sbin/slapadd\nif\n[\n-n\n\"\n$(\nls\n-l\n/var/lib/ldap/*\n2\n>/dev/null\n)\n\"\n-o\n-n\n\"\n$(\nls\n-l\n/etc/ldap/slapd.d/*\n2\n>/dev/null\n)\n\"\n]\n;\nthen\necho\nRun\nthe\nfollowing\nto\nremove\nthe\nexisting\ndb:\necho\nsudo\nsystemctl\nstop\nslapd.service\necho\nsudo\nrm\n-rf\n/etc/ldap/slapd.d/*\n/var/lib/ldap/*\nexit\n1\nfi\nsudo\nsystemctl\nstop\nslapd.service\n||\n:\nsudo\nslapadd\n-F\n/etc/ldap/slapd.d\n-b\ncn\n=\nconfig\n-l\n/export/backup/config.ldif\nsudo\nslapadd\n-F\n/etc/ldap/slapd.d\n-b\ndc\n=\nexample,dc\n=\ncom\n-l\n/export/backup/example.com.ldif\nsudo\nchown\n-R\nopenldap:openldap\n/etc/ldap/slapd.d/\nsudo\nchown\n-R\nopenldap:openldap\n/var/lib/ldap/\nsudo\nsystemctl\nstart\nslapd.service\nThis is a simplistic backup strategy, of course. It’s being shown here as a reference for the basic tooling you can use for backups and restores.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:24Z", "original_len_words": 435}}
{"id": "639daa80c9", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/install-openldap/", "title": "Install and configure LDAP - Ubuntu Server documentation", "text": "Install and configure LDAP - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and configure LDAP\n¶\nLightweight Directory Access Protocol\n(LDAP) is a protocol used for managing hierarchical data. OpenLDAP is the open-source implementation of LDAP used in Ubuntu. For information about OpenLDAP and explanations of the key terminology, see\nIntroduction to OpenLDAP\n.\nInstalling\nslapd (the Stand-alone LDAP Daemon)\ncreates a minimal working configuration with a top level entry, and an administrator’s Distinguished Name (DN).\nIn particular, it creates a database instance that you can use to store your data. However, the\nsuffix\n(or\nbase DN\n) of this instance will be determined from the domain name of the host. If you want something different, you can change it right after the installation (before it contains any useful data).\nNote\nThis guide will use a database suffix of\ndc=example,dc=com\n. You can change this to match your particular setup.\nInstall slapd\n¶\nYou can install the server and the main command line utilities with the following command:\nsudo apt install slapd ldap-utils\nChange the instance suffix (optional)\n¶\nIf you want to change your Directory Information Tree (\nDIT\n) suffix, now would be a good time since changing it discards your existing one. To change the suffix, run the following command:\nsudo dpkg-reconfigure slapd\nTo switch your DIT suffix to\ndc=example,dc=com\n, for example, so you can follow this guide more closely, answer\nexample.com\nwhen asked about the\nDNS\ndomain name.\nThroughout this guide we will issue many commands with the LDAP utilities. To save some typing, we can configure the OpenLDAP libraries with certain defaults in\n/etc/ldap/ldap.conf\n(adjust these entries for your server name and directory suffix):\nBASE dc=example,dc=com\nURI ldap://ldap01.example.com\nDefault tree contents\n¶\nslapd\nis designed to be configured within the service itself by dedicating a separate DIT for that purpose. This allows for dynamic configuration of\nslapd\nwithout needing to restart the service or edit config files. This configuration database consists of a collection of text-based LDIF files located under\n/etc/ldap/slapd.d\n, but these should never be edited directly. This way of working is known by several names: the “slapd-config” method, the “Real Time Configuration (RTC)” method, or the “cn=config” method. You can still use the traditional flat-file method (\nslapd.conf\n) but that will not be covered in this guide.\nRight after installation, you will get two databases, or suffixes: one for your data, which is based on your host’s domain (\ndc=example,dc=com\n), and one for your configuration, with its root at\ncn=config\n. To change the data on each we need different credentials and access methods:\ndc=example,dc=com\nThe administrative user for this suffix is\ncn=admin,dc=example,dc=com\nand its password is the one selected during the installation of the\nslapd\npackage.\ncn=config\nThe configuration of\nslapd\nitself is stored under this suffix. Reading and writing to it can be done by the special\nDN\ngidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\n. This is how the local system’s root user (\nuid=0/gid=0\n) is seen by the directory when using\nSASL\nEXTERNAL authentication through the\nldapi:///\ntransport via the\n/run/slapd/ldapi\nUnix socket. Essentially what this means is that only the local root user can update the\ncn=config\ndatabase. More details later.\nDefault configuration tree\n¶\nThis is what the\nslapd-config\nDIT looks like via the LDAP protocol (listing only the DNs):\nTo see what the\nslapd-config\nDIT looks like via the LDAP protocol, listing only the DNs, run this command:\nsudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config dn\nThe command-line options mean the following:\n-Q\n: Quiet mode for the SASL authentication setup.\n-LLL\n: Less verbose LDIF output. One “\nL\n” restricts output to LDIFv1; another disables comments; and the third one removes the LDIF version from the output.\n-Y\nEXTERNAL\n: Select the\nEXTERNAL\nSASL mechanism for authentication.\n-H\nldapi:///\n: The URL to use to contact the server. In this case, it will use a local unix socket.\n-b\ncn=config\n: Start the search at the\ncn=config\nbase.\ndn\n: Only retrieve the\ndn\nattribute.\nThe output will be the similar to the following:\ndn: cn=config\ndn: cn=module{0},cn=config\ndn: cn=schema,cn=config\ndn: cn={0}core,cn=schema,cn=config\ndn: cn={1}cosine,cn=schema,cn=config\ndn: cn={2}nis,cn=schema,cn=config\ndn: cn={3}inetorgperson,cn=schema,cn=config\ndn: olcDatabase={-1}frontend,cn=config\ndn: olcDatabase={0}config,cn=config\ndn: olcDatabase={1}mdb,cn=config\nWhere the entries mean the following:\ncn=config\n: Global settings\ncn=module{0},cn=config\n: A dynamically loaded module\ncn=schema,cn=config\n: Contains hard-coded system-level schema\ncn={0}core,cn=schema,cn=config\n: The hard-coded\ncore\nschema\ncn={1}cosine,cn=schema,cn=config\n: The Cosine schema\ncn={2}nis,cn=schema,cn=config\n: The Network Information Services (NIS) schema\ncn={3}inetorgperson,cn=schema,cn=config\n: The InetOrgPerson schema\nolcDatabase={-1}frontend,cn=config\n:\nFrontend\ndatabase, default settings for other databases\nolcDatabase={0}config,cn=config\n:\nslapd\nconfiguration database (\ncn=config\n)\nolcDatabase={1}mdb,cn=config\n: Your database instance (\ndc=example,dc=com\n)\nSince the configuration is located under the\ncn=config\nsuffix, we can use LDAP commands to inspect or modify it.\nTo see all the configuration, run this command:\nsudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config\nThe output is too large to show here, but it will start like this:\ndn: cn=config\nobjectClass: olcGlobal\ncn: config\nolcArgsFile: /var/run/slapd/slapd.args\nolcLogLevel: none\n...\nDefault “data” tree\n¶\nAfter installing the\nslapd\npackage, a default data tree is configured, based on the detected domain name of the system. Assuming a domain of\nexample.com\n, this command can be run to show what it looks like:\nldapsearch -x -LLL -H ldap:/// -b dc=example,dc=com dn\nHere the only new command-line option is\n-x\n, and we have a new parameter for\n-H\n:\n-x\n: Use simple authentication instead of SASL, which is essentially a plain text authentication. Since no\nBind DN\nwas provided (via\n-D\n), this becomes an\nanonymous\nbind. Without\n-x\n, the default is to use a SASL bind.\n-H\nldap:///\n: Use the LDAP protocol over the network (and not over a unix socket), and since no hostname was provided, it’s assumed to be localhost. To access a server on another host, one would use\nldap://server.example.com/\nas the URL, for example.\nThe output will be the top-level entry which represents the base of the DIT.\ndn: dc=example,dc=com\nWho am I?\n¶\nA very handy tool to verify the authentication is\nldapwhoami\n, which can be used as follows:\nldapwhoami -x\nThe output will say who we connected as:\nanonymous\nNow let’s perform an authenticated call, via simple authentication:\nldapwhoami -x -D cn=admin,dc=example,dc=com -W\nThis time we will be shown our authentication DN, after the password prompt:\nEnter LDAP Password:\ndn:cn=admin,dc=example,dc=com\nWhen you use simple bind (\n-x\n) and specify a Bind DN with\n-D\nas your authentication DN, the server will look for a\nuserPassword\nattribute in the entry, and use that to verify the credentials. In this particular case above, we used the database\nRoot DN\nentry, i.e., the actual administrator, and that is a special case whose password is set in the configuration when the package is installed.\nNote\nA simple bind without some sort of transport security mechanism is\nclear text\n, meaning the credentials are transmitted in the clear. You should\nadd Transport Layer Security (TLS) support\nto your OpenLDAP server as soon as possible.\nLet’s try some SASL EXTERNAL authentication commands:\nldapwhoami -Y EXTERNAL -H ldapi:/// -Q\nThe authentication DN is quite different from the simple bind one from before:\ndn:gidNumber=1000+uidNumber=1000,cn=peercred,cn=external,cn=auth\nLet’s try as root:\nsudo ldapwhoami -Y EXTERNAL -H ldapi:/// -Q\nNotice how the\nuidNumber\nand\ngidNumber\nchanged:\ndn:gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\nWhen using SASL EXTERNAL via the\nldapi:///\ntransport, the Bind DN becomes a combination of the\nuid\nand\ngid\nof the connecting user, followed by the suffix\ncn=peercred,cn=external,cn=auth\n. The server ACLs know about this, and grant the local root user complete write access to\ncn=config\nvia the SASL mechanism.\nNote\nOpenLDAP ACLs are explained in\nSet up access control\nPopulate the directory\n¶\nLet’s introduce some content to our directory. We will add the following:\nA node called\nPeople\n, to store users\nAn entry for a user called\njohn\nA node called\nGroups\n, to store groups\nAn entry for a group called\nminers\nCreate the following LDIF file and call it\nadd_content.ldif\n:\ndn: ou=People,dc=example,dc=com\nobjectClass: organizationalUnit\nou: People\n\ndn: ou=Groups,dc=example,dc=com\nobjectClass: organizationalUnit\nou: Groups\n\ndn: cn=miners,ou=Groups,dc=example,dc=com\nobjectClass: posixGroup\ncn: miners\ngidNumber: 5000\nmemberUid: john\n\ndn: uid=john,ou=People,dc=example,dc=com\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: shadowAccount\nuid: john\nsn: Doe\ngivenName: John\ncn: John Doe\ndisplayName: John Doe\nuidNumber: 10000\ngidNumber: 5000\nuserPassword: {CRYPT}x\ngecos: John Doe\nloginShell: /bin/bash\nhomeDirectory: /home/john\nNote\nIt’s important that\nuid\nand\ngid\nvalues in your directory do not collide with local Linux users’ values as defined in\n/etc/passwd\nand\n/etc/group\n. You can use higher number ranges, such as starting at 5000 or even higher.\nAdd the content:\nldapadd -x -D cn=admin,dc=example,dc=com -W -f add_content.ldif\nThe command will ask for the admin password, and then show the entries as they are being added:\nEnter LDAP Password: ********\nadding new entry \"ou=People,dc=example,dc=com\"\n\nadding new entry \"ou=Groups,dc=example,dc=com\"\n\nadding new entry \"cn=miners,ou=Groups,dc=example,dc=com\"\n\nadding new entry \"uid=john,ou=People,dc=example,dc=com\"\nWe can check that the information has been correctly added with the\nldapsearch\nutility. For example, let’s search for the “john” entry, and request the\ncn\nand\ngidnumber\nattributes:\nldapsearch -x -LLL -b dc=example,dc=com '(uid=john)' cn gidNumber\nThe output shows the DNs that matched the search criteria, and the requested attributes:\ndn: uid=john,ou=People,dc=example,dc=com\ncn: John Doe\ngidNumber: 5000\nHere we used an LDAP “filter”:\n(uid=john)\n. LDAP filters are very flexible and can become complex. For example, to list the group names of which\njohn\nis a member, we could use the following command:\nldapsearch -x -LLL -b dc=example,dc=com '(&(objectClass=posixGroup)(memberUid=john))' cn gidNumber\nAnd the result tells us that “john” is a member of the “miners” group:\ndn: cn=miners,ou=Groups,dc=example,dc=com\ncn: miners\ngidNumber: 5000\nThat filter is a logical “AND” (signalled by the “\n&\n” character in the filter expression) between two attributes:\nobjectClass=posixGroup\nAND\nmemberUid=john\n. Filters are very important in LDAP and mastering their syntax is extremely helpful. They are used for simple queries like this, but can also select what content is to be replicated to a secondary server, or even in complex ACLs. The full specification is defined in\nRFC 4515\n.\nNotice we set the\nuserPassword\nfield for the “john” entry to the cryptic value\n{CRYPT}x\n. This essentially is an invalid password, because no hashing will produce just\nx\n. It’s a common pattern when adding a user entry without a default password. To change the password to something valid, you can now use\nldappasswd\n:\nldappasswd -x -D cn=admin,dc=example,dc=com -W -S uid=john,ou=people,dc=example,dc=com\nWe will be prompted for the new password twice, and at the end for the bind password corresponding to the bind DN specified via\n-D\n:\nNew password:\nRe-enter new password:\nEnter LDAP Password:\nTo verify the change, we can use\nldapwhoami\nwith simple bind authentication using john’s DN as the bind DN:\nldapwhoami -x -D uid=john,ou=people,dc=example,dc=com -W\nIf the new password worked, the output will show that we authenticated as the\nuid=john,ou=People,dc=example,dc=com\nDN:\nEnter LDAP Password:\ndn:uid=john,ou=People,dc=example,dc=com\nNote\nRemember that simple binds are insecure and you should\nadd TLS support\nto your server as soon as possible!\nChange the configuration\nThe\nslapd-config\nDIT can also be queried and modified. Here are some common operations.\nWhich “admin” DN to use?\n¶\nThroughout this guide so far, we have used two different authentication mechanisms to make changes to the directory. Which one is needed for what kind of change?\nEach directory tree suffix has its own specific administrative DN. This is the DN that can make changes to the tree and is not subject to ACLs. It is stored in the\nolcRootDN\nattribute under the\ncn=config\nconfiguration tree, and the corresponding password is in the\nolcRootPW\nattribute.\nBesides this specific administrator entry, ACLs can also grant such privileges to any other DN in the directory. All of this is setup by the\nslapd\npackage when it is installed. This results in the following DNs that can be used to make changes to each directory suffix:\nSuffix\nDN for making changes\nAuthentication mechanism\ncn=config\ncn=admin,cn=config\nabsent\ncn=config\ngidNumber=0+uidNumber=0,\ncn=peercred,cn=external,cn=auth\nSASL EXTERNAL as root via ldapi://\ndc=example,dc=com\ncn=admin,dc=example,dc=com\nSimple bind with password\nset during install or reconfigure\nChange the “admin” password\n¶\nThere is really only one administrative DN that has an associated password, and it’s the one created at install (or reconfigure) time. To locate it in the\ncn=config\nsuffix, run this command:\nsudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config '(olcSuffix=dc=example,dc=com)' olcSuffix olcRootDN olcRootPW\nThe output will be the configuration entry for the\ndc=example,dc=com\nsuffix, and show only the selected attributes in the response:\ndn: olcDatabase={1}mdb,cn=config\nolcSuffix: dc=example,dc=com\nolcRootDN: cn=admin,dc=example,dc=com\nolcRootPW: {SSHA}Y0UjBUUmf08TC25ePVc6waI/mfvPNktk\nSince the\nolcRootPW\npassword attribute we want to change is located under the\ncn=config\nsuffix, we will also have to use the SASL EXTERNAL authentication to modify it, according to the table shown earlier.\nNote\nEven though this\nolcRootDN\nis the administrative DN for the\ndc=example,dc=com\nsuffix, it is stored under the\ncn=config\nsuffix!\nTo change the password associated with the\nolcRootDN\nadministrative DN, we need to replace the value of the\nolcRootPW\nattribute. That value is not the literal password, but the hash of the password, using a specific hash algorithm.\nTo obtain the hash of a password, suitable to be used as the value of\nolcRootPW\n, run the\nslappasswd\ncommand and type the password you want, with a confirmation. The output will be the hash for that password, which we will need for the next step:\nNew password:\nRe-enter new password:\n{SSHA}VKrYMxlSKhONGRpC6rnASKNmXG2xHXFo\nNow prepare a\nchangerootpw.ldif\nfile with this content, which includes the hashed password from the output above:\ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nreplace: olcRootPW\nolcRootPW: {SSHA}VKrYMxlSKhONGRpC6rnASKNmXG2xHXFo\nFinally, run the\nldapmodify\ncommand on this file:\nldapmodify -Q -Y EXTERNAL -H ldapi:/// -f changerootpw.ldif\nIf successful, the output will show the entry that is being modified:\nmodifying\nentry\n\"olcDatabase=\n{1}\nmdb,cn=config\"\nAdd an index\n¶\nLike in other database types, having an index for attributes commonly used in searches can speed up such searches dramatically, specially on large trees. The default installation of OpenLDAP already creates several common indexes, but depending on your data and queries, other indexes might be helpful.\nUse\nldapmodify\nto add an “Index” to your\n{1}mdb,cn=config\ndatabase definition (for\ndc=example,dc=com\n). In this example, we will add an “equality” and a “substring” index to the\nmail\nattribute. Create a file called\nadd_index.ldif\n, and add the following contents:\ndn: olcDatabase={1}mdb,cn=config\nadd: olcDbIndex\nolcDbIndex: mail eq,sub\nThen issue the command:\nsudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f add_index.ldif\nThe output will show the modifications being done:\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\nYou can confirm the change with a search:\nldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config '(olcDatabase={1}mdb)' olcDbIndex\nAnd the result will include all instances of the\nolcDbIndex\nattribute:\ndn: olcDatabase={1}mdb,cn=config\nolcDbIndex: objectClass eq\nolcDbIndex: cn,uid eq\nolcDbIndex: uidNumber,gidNumber eq\nolcDbIndex: member,memberUid eq\nolcDbIndex: mail eq,sub\nSee also\nTo learn more about OpenLDAP indexes, check the upstream documentation at\nhttps://www.openldap.org/doc/admin26/tuning.html#Indexes\nAdd a schema\n¶\nSchemas can only be added to\ncn=config\nif they are in LDIF format. If not, they will first have to be converted. You can find unconverted schemas in addition to converted ones in the\n/etc/ldap/schema\ndirectory.\nNote\nIt is not trivial to remove a schema from the slapd-config database. Practice adding schemas on a test system.\nIn the following example we’ll add one of the pre-installed policy schemas in\n/etc/ldap/schema/\n. The pre-installed schemas exists in both converted (\n.ldif\n) and native (\n.schema\n) formats, so we don’t have to convert them and can use\nldapadd\ndirectly:\nsudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/corba.ldif\nThe output will confirm the new schema being added:\nadding\nnew\nentry\n\"cn=corba,cn=schema,cn=config\"\nIf the schema you want to add does not exist in LDIF format, a nice conversion tool that can be used is provided in the\nschema2ldif\npackage.\nLogging\n¶\nActivity logging for\nslapd\nis very useful when implementing an OpenLDAP-based solution – and it must be manually enabled after software installation. Otherwise, only rudimentary messages will appear in the logs. Logging, like any other such configuration, is enabled via the\nslapd-config\ndatabase.\nOpenLDAP comes with multiple logging levels, with each level containing the lower one (additive). A good level to try is\nstats\n. The\nslapd-config(5)\nmanual page has more to say on the different subsystems.\nExample logging with the stats level\n¶\nCreate the file\nlogging.ldif\nwith the following contents:\ndn: cn=config\nchangetype: modify\nreplace: olcLogLevel\nolcLogLevel: stats\nRun\nldapmodify\nto implement the change:\nsudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f logging.ldif\nDepending on how active your OpenLDAP server is, this will produce a significant amount of logging. It is recommended to revert back to a less verbose level once the need for this detailed logging isn’t there anymore.\nWhile in this verbose mode your host’s syslog engine (rsyslog) may have a hard time keeping up. If you see log message like this, it means some messages were dropped:\nrsyslogd-2177: imuxsock lost 228 messages from pid 2547 due to rate-limiting\nYou may consider a change to rsyslog’s configuration. In\n/etc/rsyslog.conf\n, add:\n# Disable rate limiting\n# (default is 200 messages in 5 seconds; below we make the 5 become 0)\n$SystemLogRateLimitInterval 0\nAnd then restart the rsyslog daemon:\nsudo systemctl restart syslog.service\nNext steps\n¶\nNow that you have successfully installed LDAP, you may want to\nset up users and groups\n, or find out more\nabout access control\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:25Z", "original_len_words": 2917}}
{"id": "f614e87a48", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/ldap-and-tls/", "title": "LDAP and Transport Layer Security (TLS) - Ubuntu Server documentation", "text": "LDAP and Transport Layer Security (TLS) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nLDAP and Transport Layer Security (TLS)\n¶\nWhen authenticating to an OpenLDAP server it is best to do so using an encrypted session. This can be accomplished using Transport Layer Security (TLS).\nHere, we will be our own Certificate Authority (CA) and then create and sign our LDAP server certificate as that CA. This guide will use the\ncerttool\nutility to complete these tasks. For simplicity, this is being done on the OpenLDAP server itself, but your real internal CA should be elsewhere.\nInstall the\ngnutls-bin\nand\nssl-cert\npackages:\nsudo\napt\ninstall\ngnutls-bin\nssl-cert\nCreate a private key for the Certificate Authority:\nsudo\ncerttool\n--generate-privkey\n--bits\n4096\n--outfile\n/etc/ssl/private/mycakey.pem\nCreate the template/file\n/etc/ssl/ca.info\nto define the CA:\ncn = Example Company\nca\ncert_signing_key\nexpiration_days = 3650\nCreate the self-signed CA certificate:\nsudo\ncerttool\n--generate-self-signed\n\\\n--load-privkey\n/etc/ssl/private/mycakey.pem\n\\\n--template\n/etc/ssl/ca.info\n\\\n--outfile\n/usr/local/share/ca-certificates/mycacert.crt\nNote\nYes, the\n--outfile\npath is correct. We are writing the CA certificate to\n/usr/local/share/ca-certificates\n. This is where\nupdate-ca-certificates\nwill pick up trusted local CAs from. To pick up CAs from\n/usr/share/ca-certificates\n, a call to\ndpkg-reconfigure\nca-certificates\nis necessary.\nRun\nupdate-ca-certificates\nto add the new CA certificate to the list of trusted CAs. Note the one added CA:\n$\nsudo\nupdate-ca-certificates\nUpdating\ncertificates\nin\n/etc/ssl/certs...\n1\nadded,\n0\nremoved\n;\ndone\n.\nRunning\nhooks\nin\n/etc/ca-certificates/update.d...\ndone\n.\nThis also creates a\n/etc/ssl/certs/mycacert.pem\nsymlink pointing to the real file in\n/usr/local/share/ca-certificates\n.\nMake a private key for the server:\nsudo\ncerttool\n--generate-privkey\n\\\n--bits\n2048\n\\\n--outfile\n/etc/ldap/ldap01_slapd_key.pem\nNote\nReplace\nldap01\nin the filename with your server’s\nhostname\n. Naming the certificate and key for the host and service that will be using them will help keep things clear.\nCreate the\n/etc/ssl/ldap01.info\ninfo file containing:\norganization = Example Company\ncn = ldap01.example.com\ntls_www_server\nencryption_key\nsigning_key\nexpiration_days = 365\nThe above certificate is good for 1 year, and it’s valid only for the\nldap01.example.com\nhostname. You can adjust this according to your needs.\nCreate the server’s certificate:\nsudo\ncerttool\n--generate-certificate\n\\\n--load-privkey\n/etc/ldap/ldap01_slapd_key.pem\n\\\n--load-ca-certificate\n/etc/ssl/certs/mycacert.pem\n\\\n--load-ca-privkey\n/etc/ssl/private/mycakey.pem\n\\\n--template\n/etc/ssl/ldap01.info\n\\\n--outfile\n/etc/ldap/ldap01_slapd_cert.pem\nAdjust permissions and ownership:\nsudo\nchgrp\nopenldap\n/etc/ldap/ldap01_slapd_key.pem\nsudo\nchmod\n0640\n/etc/ldap/ldap01_slapd_key.pem\nYour server is now ready to accept the new TLS configuration.\nCreate the file\ncertinfo.ldif\nwith the following contents (adjust paths and filenames accordingly):\ndn: cn=config\nadd: olcTLSCACertificateFile\nolcTLSCACertificateFile: /etc/ssl/certs/mycacert.pem\n-\nadd: olcTLSCertificateFile\nolcTLSCertificateFile: /etc/ldap/ldap01_slapd_cert.pem\n-\nadd: olcTLSCertificateKeyFile\nolcTLSCertificateKeyFile: /etc/ldap/ldap01_slapd_key.pem\nUse the\nldapmodify\ncommand to tell\nslapd\nabout our TLS work via the\nslapd-config\ndatabase:\nsudo\nldapmodify\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\ncertinfo.ldif\nIf you need access to\nLDAPS\n(LDAP over SSL), then you need to edit\n/etc/default/slapd\nand include\nldaps:///\nin\nSLAPD_SERVICES\nlike below:\nSLAPD_SERVICES=\"ldap:/// ldapi:/// ldaps:///\"\nAnd restart\nslapd\nwith:\nsudo\nsystemctl\nrestart\nslapd\nNote that\nStartTLS\nwill be available without the change above, and does NOT need a\nslapd\nrestart.\nTest\nStartTLS\n:\n$\nldapwhoami\n-x\n-ZZ\n-H\nldap://ldap01.example.com\nanonymous\nTest LDAPS:\n$\nldapwhoami\n-x\n-H\nldaps://ldap01.example.com\nanonymous\nCertificate for an OpenLDAP replica\nTo generate a certificate pair for an OpenLDAP replica (consumer), create a holding directory (which will be used for the eventual transfer) and run the following:\nmkdir\nldap02-ssl\ncd\nldap02-ssl\ncerttool\n--generate-privkey\n\\\n--bits\n2048\n\\\n--outfile\nldap02_slapd_key.pem\nCreate an info file,\nldap02.info\n, for the Consumer server, adjusting its values according to your requirements:\norganization\n=\nExample\nCompany\ncn\n=\nldap02.example.com\ntls_www_server\nencryption_key\nsigning_key\nexpiration_days\n=\n365\nCreate the Consumer’s certificate:\nsudo\ncerttool\n--generate-certificate\n\\\n--load-privkey\nldap02_slapd_key.pem\n\\\n--load-ca-certificate\n/etc/ssl/certs/mycacert.pem\n\\\n--load-ca-privkey\n/etc/ssl/private/mycakey.pem\n\\\n--template\nldap02.info\n\\\n--outfile\nldap02_slapd_cert.pem\nNote\nWe had to use\nsudo\nto get access to the CA’s private key. This means the generated certificate file is owned by root. You should change that ownership back to your regular user before copying these files over to the Consumer.\nGet a copy of the CA certificate:\ncp\n/etc/ssl/certs/mycacert.pem\n.\nWe’re done. Now transfer the\nldap02-ssl\ndirectory to the Consumer. Here we use\nscp\n(adjust accordingly):\ncd\n..\nscp\n-r\nldap02-ssl\nuser@consumer:\nOn the Consumer side, install the certificate files you just transferred:\nsudo\ncp\nldap02_slapd_cert.pem\nldap02_slapd_key.pem\n/etc/ldap\nsudo\nchgrp\nopenldap\n/etc/ldap/ldap02_slapd_key.pem\nsudo\nchmod\n0640\n/etc/ldap/ldap02_slapd_key.pem\nsudo\ncp\nmycacert.pem\n/usr/local/share/ca-certificates/mycacert.crt\nsudo\nupdate-ca-certificates\nCreate the file\ncertinfo.ldif\nwith the following contents (adjust accordingly regarding paths and filenames, if needed):\ndn: cn=config\nadd: olcTLSCACertificateFile\nolcTLSCACertificateFile: /etc/ssl/certs/mycacert.pem\n-\nadd: olcTLSCertificateFile\nolcTLSCertificateFile: /etc/ldap/ldap02_slapd_cert.pem\n-\nadd: olcTLSCertificateKeyFile\nolcTLSCertificateKeyFile: /etc/ldap/ldap02_slapd_key.pem\nConfigure the\nslapd-config\ndatabase:\nsudo\nldapmodify\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\ncertinfo.ldif\nLike before, if you want to enable LDAPS, edit\n/etc/default/slapd\nand add\nldaps:///\nto\nSLAPD_SERVICES\n, and then restart\nslapd\n.\nTest\nStartTLS\n:\n$\nldapwhoami\n-x\n-ZZ\n-H\nldap://ldap02.example.com\nanonymous\nTest LDAPS:\n$\nldapwhoami\n-x\n-H\nldaps://ldap02.example.com\nanonymous", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:25Z", "original_len_words": 810}}
{"id": "359a2d5c94", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/ldap-saslauthd-authentication-intro/", "title": "How to configure OpenLDAP with passthrough authentication - Ubuntu Server documentation", "text": "How to configure OpenLDAP with passthrough authentication - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to configure OpenLDAP with passthrough authentication\n¶\nBackground\n¶\nManaging large groups of users is great use case for OpenLDAP, and then allowing external applications to authenticate its users against this database\nallows the applications to offload user management to OpenLDAP. A sophisticated application can use a secure authentication method such as\nGeneric Security Services Application Programming Interface\n(GSSAPI)\nto attempt to bind to the OpenLDAP server and obtain the authentication but a less sophisticated application may only be able to performs a simple bind request to OpenLDAP. This simple bind request\nsupplies a username and password to OpenLDAP and then OpenLDAP checks the password against the hashed password it has stored internally and either accepts or rejects the bind.\nObviously there is a security concern sending plain passwords over a network so this technique must only be used over a TLS connection see\nsetting up Transport Layer Security (TLS)\nWhat is passthrough authentication\n¶\nA complication occurs when the passwords storage mechanism is outside of OpenLDAP. How can this simple bind process be used without storing the password in\nthe OpenLDAP database. One solution to this problem to use use the Simple Authentication and Security Layer (SASL) library to pass though this bind request to the mechanism where the password is\nactually stored. The application that does this is the saslauthd daemon. This daemon can be configured to use various other authentication systems and will then\npass back the authentication status to OpenLDAP which in turn passes it back to the calling application.\nA graphic showing this process is as follows:\nSaslauthd authentication providers\n¶\nAuthentication providers that saslauthd can use are:\ngetpwent  – use the getpwent() library function\nkerberos5 – use Kerberos 5\npam       – use PAM\nrimap     – use a remote IMAP server\nshadow    – use the local shadow password file\nsasldb    – use the local sasldb database file\nldap      – use LDAP (configuration is in /etc/saslauthd.conf)\nReference:\nhttps://www.openldap.org/doc/admin26/sasl.html", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:25Z", "original_len_words": 360}}
{"id": "63a2c27e80", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/ldap-saslauthd-kerberos/", "title": "How to configure OpenLDAP with passthrough SASL authentication using Kerberos - Ubuntu Server documentation", "text": "How to configure OpenLDAP with passthrough SASL authentication using Kerberos - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to configure OpenLDAP with passthrough SASL authentication using Kerberos\n¶\nBefore you begin\n¶\nIt is assumed you are starting with a working OpenLDAP server, with a hostname of\nldap-server.example.com\n. If not, follow this guide\nInstall and configure OpenLDAP\nto set it up. It is also assumed that the\nEXAMPLE.COM\nrealm is set up, and the Kerberos client tools (krb5-user) are installed on the ldap server. You will need to create an ubuntu principal. See\nHow to install a Kerberos server\n. You should also know how to create service principals. See\nHow to configure Kerberos service principals\n.\nAll the following configuration will be on\nldap-server.example.com\n.\nNote\n:\nThis process is not the same as using\nGeneric Security Services Application Programming Interface\n(GSSAPI) to log into the LDAP server.\nRather it is using\nsimple authentication\nwith the OpenLDAP server so this should be over a\nTransport Layer Security\n(TLS) connection.\nThe test user we will be using is\nubuntu@EXAMPLE.COM\nwhich must exist in the Kerberos database\nHow the passthrough authentication will work\n¶\nHere is a diagram showing how all the different pieces work together:\nWe will go over all these details next.\nPackage installation\n¶\nInstall\nsaslauthd\non the OpenLDAP server (\nldap-server.example.com\nin this document):\nsudo apt install sasl2-bin\nCheck the hostname\n¶\nGet the hostname from the server\nhostname -f\nWhich should give you the hostname of:\nldap-server.example.com\nAlso check the hostname and domain using a reverse lookup with your IP. For example, if the IP address is\n10.10.17.91\n:\nnslookup 10.10.17.91\nThe reply should look like this:\n91.17.10.10.in-addr.arpa        name = ldap-server.example.com.\nIf the result is the same as your host’s canonical name them all is well. If the domain is missing, the\nFully Qualified Domain Name\n(FQDN) can be entered in the\n/etc/hosts\nfile.\nsudo vi /etc/hosts\nAdd the FQDN before the short hostname. Using the same IP as in the previous example, we would have:\n10.10.17.91 ldap-server.example.com ldap-server\nCreate the saslauthd principal\n¶\nThe\nsaslauthd\ndaemon needs a Kerberos service principal in order to authenticate itself to the Kerberos server. Such principals are created with a random password, and the resulting key is stored in\n/etc/krb5.keytab\n. For more information about Kerberos service principals, please consult\nHow to configure Kerberos service principals\n.\nThe simplest way to create this principal, and extract the key safely, is to run the\nkadmin\ntool remotely, instead of on the Kerberos server. Since the key needs to be written to\n/etc/krb5.keytab\n, the tool needs to be run with root privileges. Additionally, since creating a new service principal, as well as extracting its key, are privileged operations, we need an\n/admin\ninstance of a principal in order to be allowed these actions. In this example, we will use\nubuntu/admin\n:\nsudo kadmin -p ubuntu/admin\nAn interactive session will look like below. Note the two commands we are issuing at the\nkadmin:\nprompt:\naddprinc\nand\nktadd\n:\nAuthenticating as principal ubuntu/admin with password.\nPassword for ubuntu/admin@EXAMPLE.COM\nkadmin: addprinc -randkey host/ldap-server.example.com\nNo policy specified for host/ldap-server.example.com@EXAMPLE.COM; defaulting to no policy\nPrincipal \"host/ldap-server.example.com@EXAMPLE.COM\" created.\nkadmin: ktadd host/ldap-server.example.com\nEntry for principal host/ldap-server.example.com with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab FILE:/etc/krb5.keytab.\nEntry for principal host/ldap-server.example.com with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab FILE:/etc/krb5.keytab.\nAlternatively, we can issue the commands directly:\nkadmin -p ubuntu/admin -q \"addprinc -randkey host/ldap-server.example.com\"\nNOTE\nsudo\nis not needed to remotely create a new principal.\nAnd:\nsudo kadmin -p ubuntu/admin -q \"ktadd host/ldap-server.example.com\"\nTo check that the service principal was added to\n/etc/krb5.keytab\n, run this command:\nsudo klist -k\nYou should see the following:\nKeytab name: FILE:/etc/krb5.keytab\nKVNO Principal\n---- --------------------------------------------------------------------------\n   2 host/ldap-server.example.com@EXAMPLE.COM\n   2 host/ldap-server.example.com@EXAMPLE.COM\nConfigure saslauthd\n¶\nWe now need to configure\nsaslauthd\nsuch that it uses Kerberos authentication. This is an option that is selected at startup time, via command-line options. The configuration file for such options is\n/etc/default/saslauthd\n. Only one change is needed in this file: update\nMECHANISMS\nto\nkerberos5\n:\nsudo vi /etc/default/saslauthd\nMake the following change:\n...\n# Which authentication mechanisms should saslauthd use? (default: pam)\n#\n# Available options in this Debian package:\n# getpwent  -- use the getpwent() library function\n# kerberos5 -- use Kerberos 5\n# pam       -- use PAM\n# rimap     -- use a remote IMAP server\n# shadow    -- use the local shadow password file\n# sasldb    -- use the local sasldb database file\n# ldap      -- use LDAP (configuration is in /etc/saslauthd.conf)\n#\n# Only one option may be used at a time. See the saslauthd man page\n# for more information.\n#\n# Example: MECHANISMS=\"pam\"\nMECHANISMS=\"kerberos5\"\n...\nIMPORTANT\n:\nFor Ubuntu version 22.04 and earlier “\nSTART=yes\n” must also be added to the default config file for\nsaslauthd\nto start.\nSave and exit the editor.\nEnable and start saslauthd\n¶\nContinue by enabling and starting the saslauthd service.\nsudo systemctl enable --now saslauthd\nTest saslauthd configuration\n¶\nThe\nsaslauthd\nservice can be tested with with the\ntestsaslauthd\ncommand. For example, with the correct Kerberos password for the\nubuntu\nprincipal:\ntestsaslauthd -u ubuntu -p ubuntusecret\n0: OK \"Success.\"\nAnd with the wrong Kerberos password:\ntestsaslauthd -u ubuntu -p ubuntusecretwrong\n0: NO \"authentication failed\"\nNOTE\n:\nIn Ubuntu 22.04 LTS and earlier, the\n/run/saslauthd\ndirectory is restricted to members of the\nsasl\ngroup, so the\ntestsaslauthd\ncommands above need to be run as root (via\nsudo\n) or as a user who is in the\nsasl\ngroup.\nConfigure OpenLDAP\n¶\nIn order for OpenLDAP to perform passthrough authentication using\nsaslauthd\n, we need to create the configuration file\n/etc/ldap/sasl2/slapd.conf\nwith the following content:\npwcheck_method: saslauthd\nThis will direct OpenLDAP to use\nsaslauthd\nas the password checking mechanism when performing passthrough authentication on behalf of a user.\nIn Ubuntu 22.04 LTS and earlier, the\nopenldap\nsystem user needs to be added to the\nsasl\ngroup, or else it will not have permission to contact the\nsaslauthd\nunix socket in\n/run/saslauthd/\n. To make this change, run:\nsudo\ngpasswd\n-\na\nopenldap\nsasl\nFinally, restart the OpenLDAP service:\nsudo\nsystemctl\nrestart\nslapd\n.\nservice\nChange the\nuserPassword\nattribute\n¶\nWhat triggers OpenLDAP to perform a passthrough authentication when processing a simple bind authentication request, is the special content of the\nuserPassword\nattribute. Normally, that attribute contains some form of password hash, which is used to authenticate the request. If, however, what it contains is in the format of\n{SASL}username@realm\n, then OpenLDAP will delegate the authentication to the SASL library, whose configuration is in that file we just created above.\nFor example, let’s examine the directory entry below:\ndn: uid=ubuntu,dc=example,dc=com\nuid: ubuntu\nobjectClass: account\nobjectClass: simpleSecurityObject\nuserPassword: {SSHA}S+WlmGneLDFeCwErKnY4mJngnVJMZAM5\nIf a simple bind is performed using a binddn of\nuid=ubuntu,dc=example,dc=com\n, the password will be checked against the hashed\nuserPassword\nvalue as normal. That is, no passthrough authentication will be done at all.\nHowever, if the\nuserPassword\nattribute is in this format:\nuserPassword: {SASL}ubuntu@EXAMPLE.COM\nThat will trigger the passthrough authentication, because the\nuserPassword\nattribute starts with the special prefix\n{SASL}\n. This will direct OpenLDAP to use\nsaslauthd\nfor the authentication, and use the name provided in the\nuserPassword\nattribute.\nIMPORTANT\nNote how the username present in the\nuserPassword\nattribute is independent of the binddn used in the simple bind! If the\nuserPassword\nattribute contained, say,\n{SASL}anotheruser@EXAMPLE.COM\n, OpenLDAP would ask\nsaslauthd\nto authenticate\nanotheruser@EXAMPLE.COM\n, and not the user from the binddn! Therefore, it’s important to use OpenLDAP ACLs to prevent users from changing the\nuserPassword\nattribute when using passthrough authentication!\nTo continue with this how-to, let’s create the\nuid=ubuntu\nentry in the directory, which will use passthrough authentication:\nldapadd -x -D cn=admin,dc=example,dc=com -W <<LDIF\ndn: uid=ubuntu,dc=example,dc=com\nuid: ubuntu\nobjectClass: account\nobjectClass: simpleSecurityObject\nuserPassword: {SASL}ubuntu@EXAMPLE.COM\nLDIF\nNOTE\nNote how we don’t need to add the posix attributes like user id, home directory, group, etc. All we really need is a directory entry that contains a\nuserPassword\nattribute.\nTest the authentication\n¶\nTo test that the simple bind is working, and using the Kerberos password for the\nubuntu\nuser, let’s use\nldapwhoami\n:\nldapwhoami -D uid=ubuntu,dc=example,dc=com -W -x\nEnter LDAP Password:\nA successful bind will look like\ndn:uid=ubuntu,dc=example,dc=com\nA failed bind will look like\nldap_bind: Invalid credentials (49)\nThese LDAP bind DNs can now be used with external applications that only support “simple” username and password authentication, and they will be authenticated against Kerberos behind the scenes.\nTroubleshooting\n¶\nSaslauthd can be run in debug mode for more verbose messages to aid in troubleshooting\nsudo systemctl stop saslauthd\nsudo /usr/sbin/saslauthd -a kerberos5 -d -m /var/run/saslauthd -n 1\nAlso the\n/var/log/auth.log\nfile can be checked for saslauthd log entries\nAdvanced options\n¶\nSaslauthd can be configured in the\n/etc/saslauthd.conf\nfile. The settings depend on the authorization mechanism configured in\n/etc/default/saslauthd\n, which in this case is\nkerberos5\n.\nSome options are:\nkrb5_keytab\n: Override the default keytab file location of\n/etc/krb5.keytab\n.\nkrb5_verify_principal\n: Change the name of the Kerberos service principal used by\nsaslauthd\n.\nFor example, with this content in\n/etc/saslauthd.conf\n:\nkrb5_keytab: /etc/saslauthd.keytab\nkrb5_verify_principal: saslauthd\nWe have changed the keytab file to\n/etc/saslauthd.keytab\n, and the service principal that\nsaslauthd\nwill use to authenticate itself with the Kerberos server becomes\nsaslauthd/ldap-server.example.com@EXAMPLE.COM\n(following this how-to). Note how the domain and realm names remain the same in this service principal, and only the actual principal name is affected (changed from the default value of\nhost\nto\nsaslauthd\n).\nReferences\n¶\nOpenLDAP Passthrough Authentication\nCyrus SASL Password Verification\nCyrus SASL Slapd Configuration File\nKerberos Client Hostname Requirements", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:26Z", "original_len_words": 1623}}
{"id": "f1ac10b3c8", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/replication/", "title": "OpenLDAP replication - Ubuntu Server documentation", "text": "OpenLDAP replication - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nOpenLDAP replication\n¶\nThe LDAP service becomes increasingly important as more networked systems begin to depend on it. In such an environment, it is standard practice to build redundancy (\nhigh availability\n) into LDAP to prevent disruption should the LDAP server become unresponsive. This is done through\nLDAP replication\n.\nReplication is achieved via the Sync replication engine,\nsyncrepl\n. This allows changes to be synchronized using a\nConsumer - Provider\nmodel. A detailed description of this replication mechanism can be found in the\nOpenLDAP administrator’s guide\nand in its defining\nRFC 4533\n.\nThere are two ways to use this replication:\nStandard replication\n: Changed entries are sent to the consumer in their entirety. For example, if the\nuserPassword\nattribute of the\nuid=john,ou=people,dc=example,dc=com\nentry changed, then the whole entry is sent to the consumer.\nDelta replication\n: Only the actual change is sent, instead of the whole entry.\nThe delta replication sends less data over the network, but is more complex to set up. We will show both in this guide.\nImportant\nYou\nmust\nhave Transport Layer Security (TLS) enabled already before proceeding with this guide. Please consult the\nLDAP with TLS guide\nfor details of how to set this up.\nProvider configuration - replication user\n¶\nBoth replication strategies will need a replication user, as well as updates to the\nACL\ns and limits regarding this user. To create the replication user, save the following contents to a file called\nreplicator.ldif\n:\ndn: cn=replicator,dc=example,dc=com\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: replicator\ndescription: Replication user\nuserPassword: {CRYPT}x\nThen add it with\nldapadd\n:\n$\nldapadd\n-x\n-ZZ\n-H\nldap://ldap01.example.com\n-D\ncn\n=\nadmin,dc\n=\nexample,dc\n=\ncom\n-W\n-f\nreplicator.ldif\nEnter\nLDAP\nPassword:\nadding\nnew\nentry\n\"cn=replicator,dc=example,dc=com\"\nNow set a password for it with\nldappasswd\n:\n$\nldappasswd\n-x\n-ZZ\n-H\nldap://ldap01.example.com\n-D\ncn\n=\nadmin,dc\n=\nexample,dc\n=\ncom\n-W\n-S\ncn\n=\nreplicator,dc\n=\nexample,dc\n=\ncom\nNew\npassword:\nRe-enter\nnew\npassword:\nEnter\nLDAP\nPassword:\nNote\nPlease adjust the server URI in the\n-H\nparameter if needed to match your deployment.\nThe next step is to give this replication user the correct privileges, i.e.:\nRead access to the content that we want replicated\nNo search limits on this content\nFor that we need to update the ACLs on the provider. Since ordering matters, first check what the existing ACLs look like on the\ndc=example,dc=com\ntree:\n$\nsudo\nldapsearch\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-LLL\n-b\ncn\n=\nconfig\n'(olcSuffix=dc=example,dc=com)'\nolcAccess\ndn:\nolcDatabase\n={\n1\n}\nmdb,cn\n=\nconfig\nolcAccess:\n{\n0\n}\nto\nattrs\n=\nuserPassword\nby\nself\nwrite\nby\nanonymous\nauth\nby\n*\nnone\nolcAccess:\n{\n1\n}\nto\nattrs\n=\nshadowLastChange\nby\nself\nwrite\nby\n*\nread\nolcAccess:\n{\n2\n}\nto\n*\nby\n*\nread\nWhat we need is to insert a new rule before the first one, and also adjust the limits for the replicator user. Prepare the\nreplicator-acl-limits.ldif\nfile with this content:\ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nadd: olcAccess\nolcAccess: {0}to *\n  by dn.exact=\"cn=replicator,dc=example,dc=com\" read\n  by * break\n-\nadd: olcLimits\nolcLimits: dn.exact=\"cn=replicator,dc=example,dc=com\"\n  time.soft=unlimited time.hard=unlimited\n  size.soft=unlimited size.hard=unlimited\nAnd add it to the server:\n$\nsudo\nldapmodify\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\nreplicator-acl-limits.ldif\nmodifying\nentry\n\"olcDatabase={1}mdb,cn=config\"\nProvider configuration - standard replication\n¶\nThe remaining configuration for the provider using standard replication is to add the\nsyncprov\noverlay on top of the\ndc=example,dc=com\ndatabase.\nCreate a file called\nprovider_simple_sync.ldif\nwith this content:\n# Add indexes to the frontend db.\ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nadd: olcDbIndex\nolcDbIndex: entryCSN eq\n-\nadd: olcDbIndex\nolcDbIndex: entryUUID eq\n\n#Load the syncprov module.\ndn: cn=module{0},cn=config\nchangetype: modify\nadd: olcModuleLoad\nolcModuleLoad: syncprov\n\n# syncrepl Provider for primary db\ndn: olcOverlay=syncprov,olcDatabase={1}mdb,cn=config\nchangetype: add\nobjectClass: olcOverlayConfig\nobjectClass: olcSyncProvConfig\nolcOverlay: syncprov\nolcSpCheckpoint: 100 10\nolcSpSessionLog: 100\nWarning\nThe LDIF above has some parameters that you should review before deploying in production on your directory. In particular –\nolcSpCheckpoint\nand\nolcSpSessionLog\n.\nPlease see the\nslapo-syncprov(5)\nmanual page. In general,\nolcSpSessionLog\nshould be equal to (or preferably larger than) the number of entries in your directory. Also see\nITS #8125\nfor details on an existing bug.\nAdd the new content:\nsudo\nldapadd\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\nprovider_simple_sync.ldif\nThe Provider is now configured.\nConsumer configuration - standard replication\n¶\nInstall the software by going through\nthe installation steps\n. Make sure schemas and the database suffix are the same, and\nenable TLS\n.\nCreate an LDIF file with the following contents and name it\nconsumer_simple_sync.ldif\n:\ndn: cn=module{0},cn=config\nchangetype: modify\nadd: olcModuleLoad\nolcModuleLoad: syncprov\n\ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nadd: olcDbIndex\nolcDbIndex: entryUUID eq\n-\nadd: olcSyncrepl\nolcSyncrepl: rid=0\n  provider=ldap://ldap01.example.com\n  bindmethod=simple\n  binddn=\"cn=replicator,dc=example,dc=com\" credentials=<secret>\n  searchbase=\"dc=example,dc=com\"\n  schemachecking=on\n  type=refreshAndPersist retry=\"60 +\"\n  starttls=critical tls_reqcert=demand\n-\nadd: olcUpdateRef\nolcUpdateRef: ldap://ldap01.example.com\nEnsure the following attributes have the correct values:\nprovider\n: Provider server’s\nhostname\n–\nldap01.example.com\nin this example – or IP address. It must match what is presented in the provider’s SSL certificate.\nbinddn\n: The bind\nDN\nfor the replicator user.\ncredentials\n: The password you selected for the replicator user.\nsearchbase\n: The database suffix you’re using, i.e., content that is to be replicated.\nolcUpdateRef\n: Provider server’s hostname or IP address, given to clients if they try to write to this consumer.\nrid\n: Replica ID, a unique 3-digit ID that identifies the replica. Each consumer should have at least one\nrid\n.\nNote\nA successful encrypted connection via\nSTART_TLS\nis being enforced in this configuration, to avoid sending the credentials in the clear across the network. See\nLDAP with TLS\nfor details on how to set up OpenLDAP with trusted SSL certificates.\nAdd the new configuration:\nsudo\nldapadd\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\nconsumer_simple_sync.ldif\nNow you’re done! The\ndc=example,dc=com\ntree should now be synchronising.\nProvider configuration - delta replication\n¶\nThe remaining provider configuration for delta replication is:\nCreate a new database called\naccesslog\nAdd the\nsyncprov\noverlay on top of the\naccesslog\nand\ndc=example,dc=com\ndatabases\nAdd the\naccesslog\noverlay on top of the\ndc=example,dc=com\ndatabase\nAdd\nsyncprov\nand\naccesslog\noverlays and DBs\n¶\nCreate an LDIF file with the following contents and name it\nprovider_sync.ldif\n:\n# Add indexes to the frontend db.\ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nadd: olcDbIndex\nolcDbIndex: entryCSN eq\n-\nadd: olcDbIndex\nolcDbIndex: entryUUID eq\n    \n#Load the syncprov and accesslog modules.\ndn: cn=module{0},cn=config\nchangetype: modify\nadd: olcModuleLoad\nolcModuleLoad: syncprov\n-\nadd: olcModuleLoad\nolcModuleLoad: accesslog\n    \n# Accesslog database definitions\ndn: olcDatabase={2}mdb,cn=config\nobjectClass: olcDatabaseConfig\nobjectClass: olcMdbConfig\nolcDatabase: {2}mdb\nolcDbDirectory: /var/lib/ldap/accesslog\nolcSuffix: cn=accesslog\nolcRootDN: cn=admin,dc=example,dc=com\nolcDbIndex: default eq\nolcDbIndex: entryCSN,objectClass,reqEnd,reqResult,reqStart\nolcAccess: {0}to * by dn.exact=\"cn=replicator,dc=example,dc=com\" read by * break\nolcLimits: dn.exact=\"cn=replicator,dc=example,dc=com\"\n  time.soft=unlimited time.hard=unlimited\n  size.soft=unlimited size.hard=unlimited\n    \n# Accesslog db syncprov.\ndn: olcOverlay=syncprov,olcDatabase={2}mdb,cn=config\nchangetype: add\nobjectClass: olcOverlayConfig\nobjectClass: olcSyncProvConfig\nolcOverlay: syncprov\nolcSpNoPresent: TRUE\nolcSpReloadHint: TRUE\n    \n# syncrepl Provider for primary db\ndn: olcOverlay=syncprov,olcDatabase={1}mdb,cn=config\nchangetype: add\nobjectClass: olcOverlayConfig\nobjectClass: olcSyncProvConfig\nolcOverlay: syncprov\nolcSpCheckpoint: 100 10\nolcSpSessionLog: 100\n    \n# accesslog overlay definitions for primary db\ndn: olcOverlay=accesslog,olcDatabase={1}mdb,cn=config\nobjectClass: olcOverlayConfig\nobjectClass: olcAccessLogConfig\nolcOverlay: accesslog\nolcAccessLogDB: cn=accesslog\nolcAccessLogOps: writes\nolcAccessLogSuccess: TRUE\n# scan the accesslog DB every day, and purge entries older than 7 days\nolcAccessLogPurge: 07+00:00 01+00:00\nWarning\nThe LDIF above has some parameters that you should review before deploying in production on your directory. In particular –\nolcSpCheckpoint\n,\nolcSpSessionLog\n.\nPlease see the\nslapo-syncprov(5)\nmanpage. In general,\nolcSpSessionLog\nshould be equal to (or preferably larger than) the number of entries in your directory. Also see\nITS #8125\nfor details on an existing bug.\nFor\nolcAccessLogPurge\n, please check the\nslapo-accesslog(5)\nmanpage.\nCreate a directory:\nsudo\n-u\nopenldap\nmkdir\n/var/lib/ldap/accesslog\nAdd the new content:\nsudo\nldapadd\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\nprovider_sync.ldif\nThe Provider is now configured.\nConsumer configuration\n¶\nInstall the software by going through\nthe installation steps\n. Make sure schemas and the database suffix are the same, and\nenable TLS\n.\nCreate an LDIF file with the following contents and name it\nconsumer_sync.ldif\n:\ndn: cn=module{0},cn=config\nchangetype: modify\nadd: olcModuleLoad\nolcModuleLoad: syncprov\n    \ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nadd: olcDbIndex\nolcDbIndex: entryUUID eq\n-\nadd: olcSyncrepl\nolcSyncrepl: rid=0\n  provider=ldap://ldap01.example.com\n  bindmethod=simple\n  binddn=\"cn=replicator,dc=example,dc=com\" credentials=<secret>\n  searchbase=\"dc=example,dc=com\"\n  logbase=\"cn=accesslog\"\n  logfilter=\"(&(objectClass=auditWriteObject)(reqResult=0))\"\n  schemachecking=on\n  type=refreshAndPersist retry=\"60 +\"\n  syncdata=accesslog\n  starttls=critical tls_reqcert=demand\n-\nadd: olcUpdateRef\nolcUpdateRef: ldap://ldap01.example.com\nEnsure the following attributes have the correct values:\nprovider\n: Provider server’s hostname –\nldap01.example.com\nin this example – or IP address. It must match what is presented in the provider’s SSL certificate.\nbinddn\n: The bind DN for the replicator user.\ncredentials\n: The password you selected for the replicator user.\nsearchbase\n: The database suffix you’re using, i.e., content that is to be replicated.\nolcUpdateRef\n: Provider server’s hostname or IP address, given to clients if they try to write to this consumer.\nrid\n: Replica ID, a unique 3-digit ID that identifies the replica. Each consumer should have at least one\nrid\n.\nNote\nNote that a successful encrypted connection via\nSTART_TLS\nis being enforced in this configuration, to avoid sending the credentials in the clear across the network. See\nLDAP with TLS\nfor details on how to set up OpenLDAP with trusted SSL certificates.\nAdd the new configuration:\nsudo\nldapadd\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\nconsumer_sync.ldif\nYou’re done! The\ndc=example,dc=com\ntree should now be synchronising.\nTesting\n¶\nOnce replication starts, you can monitor it by running:\n$\nldapsearch\n-z1\n-LLL\n-x\n-s\nbase\n-b\ndc\n=\nexample,dc\n=\ncom\ncontextCSN\ndn:\ndc\n=\nexample,dc\n=\ncom\ncontextCSN:\n20200423222317\n.722667Z#000000#000#000000\nOn both the provider and the consumer. Once the\ncontextCSN\nvalue for both match, both trees are in sync. Every time a change is done in the provider, this value will change and so should the one in the consumer(s).\nIf your connection is slow and/or your LDAP database large, it might take a while for the consumer’s\ncontextCSN\nmatch the provider’s. But, you will know it is progressing since the consumer’s\ncontextCSN\nwill be steadily increasing.\nIf the consumer’s\ncontextCSN\nis missing or does not match the provider, you should stop and figure out the issue before continuing. Try checking the\nslapd\nentries in\n/var/log/syslog\nin the provider to see if the consumer’s authentication requests were successful, or that its requests to retrieve data return no errors. In particular, verify that you can connect to the provider from the consumer as the replicator BindDN using\nSTART_TLS\n:\nldapwhoami\n-x\n-ZZ\n-H\nldap://ldap01.example.com\n-D\ncn\n=\nreplicator,dc\n=\nexample,dc\n=\ncom\n-W\nFor our example, you should now see the\njohn\nuser in the replicated tree:\n$\nldapsearch\n-x\n-LLL\n-H\nldap://ldap02.example.com\n-b\ndc\n=\nexample,dc\n=\ncom\n'(uid=john)'\nuid\ndn:\nuid\n=\njohn,ou\n=\nPeople,dc\n=\nexample,dc\n=\ncom\nuid:\njohn\nReferences\n¶\nReplication types, OpenLDAP Administrator’s Guide\nLDAP Sync Replication - OpenLDAP Administrator’s Guide\nRFC 4533\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:26Z", "original_len_words": 1797}}
{"id": "0d140fdc03", "source_url": "https://documentation.ubuntu.com/server/how-to/openldap/users-and-groups/", "title": "How to set up LDAP users and groups - Ubuntu Server documentation", "text": "How to set up LDAP users and groups - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to set up LDAP users and groups\n¶\nOnce you\nhave a working LDAP server\n, you will need to install libraries on the client that know how and when to contact it. On Ubuntu, this was traditionally done by installing the\nlibnss-ldap\npackage, but nowadays you should use the\nSystem Security Services Daemon (SSSD)\n. To find out how to use LDAP with SSSD, refer to\nour SSSD and LDAP\nguide.\nUser and group management -\nldapscripts\n¶\nA common use case for an LDAP server is to store UNIX user and group information in the directory. There are many tools out there, and big deployments will usually develop their own. However, as a quick and easy way to get started with storing user and group information in OpenLDAP, you can use the\nldapscripts\npackage.\nInstall ldapscripts\n¶\nYou can install\nldapscripts\nby running the following command:\nsudo\napt\ninstall\nldapscripts\nThen edit the file\n/etc/ldapscripts/ldapscripts.conf\nto arrive at something similar to the following:\nSERVER=ldap://ldap01.example.com\nLDAPBINOPTS=\"-ZZ\"\nBINDDN='cn=admin,dc=example,dc=com'\nBINDPWDFILE=\"/etc/ldapscripts/ldapscripts.passwd\"\nSUFFIX='dc=example,dc=com'\nGSUFFIX='ou=Groups'\nUSUFFIX='ou=People'\nMSUFFIX='ou=Computers'\nNote\nAdjust\nSERVER\nand related\nSUFFIX\noptions to suit your directory structure.\nHere, we are forcing use of\nSTART_TLS\n(\n-ZZ\nparameter). Refer to\nLDAP with TLS\nto learn how to set up the server with TLS support.\nStore the\ncn=admin\npassword in the\n/etc/ldapscripts/ldapscripts.passwd\nfile and make sure it’s only readable by the\nroot\nlocal user:\necho\n-n\n'password'\n|\nsudo\ntee\n/etc/ldapscripts/ldapscripts.passwd\nsudo\nchmod\n400\n/etc/ldapscripts/ldapscripts.passwd\nNote\n:\nThe password file must contain exactly and only the password characters, no end-of-line or anything else. The\necho\ncommand above with the\n-n\nparameter achieves that by suppressing the\nEOL\ncharacter\n\\n\n. And in order to prevent the password from appearing in the shell history, the\necho\ncommand line is prefixed by a space.\nThe scripts are now ready to help manage your directory.\nManage users and groups with ldapscripts\n¶\nHere are some brief examples you can use to manage users and groups using\nldapscripts\n.\nCreate a new user\n¶\nsudo\nldapaddgroup\ngeorge\nsudo\nldapadduser\ngeorge\ngeorge\nThis will create a group and user with name “george” and set the user’s primary group (\ngid\n) to “george” as well.\nChange a user’s password\n¶\n$\nsudo\nldapsetpasswd\ngeorge\n\nChanging\npassword\nfor\nuser\nuid\n=\ngeorge,ou\n=\nPeople,dc\n=\nexample,dc\n=\ncom\nNew\nPassword:\nRetype\nNew\nPassword:\nSuccessfully\nset\npassword\nfor\nuser\nuid\n=\ngeorge,ou\n=\nPeople,dc\n=\nexample,dc\n=\ncom\nDelete a user\n¶\nsudo\nldapdeleteuser\ngeorge\nNote that this won’t delete the user’s primary group, but will remove the user from supplementary ones.\nAdd a group\n¶\nsudo\nldapaddgroup\nqa\nDelete a group\n¶\nsudo\nldapdeletegroup\nqa\nAdd a user to a group\n¶\nsudo\nldapaddusertogroup\ngeorge\nqa\nYou should now see a\nmemberUid\nattribute for the\nqa\ngroup with a value of\ngeorge\n.\nRemove a user from a group\n¶\nsudo\nldapdeleteuserfromgroup\ngeorge\nqa\nThe\nmemberUid\nattribute should now be removed from the\nqa\ngroup.\nManage user attributes with\nldapmodifyuser\n¶\nThe\nldapmodifyuser\nscript allows you to add, remove, or replace a user’s attributes. The script uses the same syntax as the\nldapmodify\nutility. For example:\nsudo\nldapmodifyuser\ngeorge\n# About to modify the following entry :\ndn:\nuid\n=\ngeorge,ou\n=\nPeople,dc\n=\nexample,dc\n=\ncom\nobjectClass:\naccount\nobjectClass:\nposixAccount\ncn:\ngeorge\nuid:\ngeorge\nuidNumber:\n10001\ngidNumber:\n10001\nhomeDirectory:\n/home/george\nloginShell:\n/bin/bash\ngecos:\ngeorge\ndescription:\nUser\naccount\nuserPassword::\ne1NTSEF9eXFsTFcyWlhwWkF1eGUybVdFWHZKRzJVMjFTSG9vcHk\n=\n# Enter your modifications here, end with CTRL-D.\ndn:\nuid\n=\ngeorge,ou\n=\nPeople,dc\n=\nexample,dc\n=\ncom\nreplace:\ngecos\ngecos:\nGeorge\nCarlin\nThe user’s\ngecos\nshould now be “George Carlin”.\nldapscripts\ntemplates\n¶\nA nice feature of\nldapscripts\nis the template system. Templates allow you to customize the attributes of user, group, and machine objects. For example, to enable the\nuser\ntemplate, edit\n/etc/ldapscripts/ldapscripts.conf\nby changing:\nUTEMPLATE=\"/etc/ldapscripts/ldapadduser.template\"\nThere are sample templates in the\n/usr/share/doc/ldapscripts/examples\ndirectory. Copy or rename the\nldapadduser.template.sample\nfile to\n/etc/ldapscripts/ldapadduser.template\n:\nsudo\ncp\n/usr/share/doc/ldapscripts/examples/ldapadduser.template.sample\n\\\n/etc/ldapscripts/ldapadduser.template\nEdit the new template to add the desired attributes. The following will create new users with an\nobjectClass\nof\ninetOrgPerson\n:\ndn: uid=<user>,<usuffix>,<suffix>\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\ncn: <user>\nsn: <ask>\nuid: <user>\nuidNumber: <uid>\ngidNumber: <gid>\nhomeDirectory: <home>\nloginShell: <shell>\ngecos: <user>\ndescription: User account\ntitle: Employee\nNotice the\n<ask>\noption used for the\nsn\nattribute. This will make\nldapadduser\nprompt you for its value.\nThere are utilities in the package that were not covered here. This command will output a list of them:\ndpkg\n-L\nldapscripts\n|\ngrep\n/usr/sbin\nNext steps\n¶\nNow that you know how to set up and modify users and groups, it’s a good idea to secure your LDAP communication by\nsetting up Transport Layer Security (TLS)\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:26Z", "original_len_words": 824}}
{"id": "37147f6535", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/", "title": "Samba - Ubuntu Server documentation", "text": "Samba - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSamba\n¶\nA Samba server can be deployed as a full Active Directory Domain Controller (Samba AD/DC), providing authentication to domain users – whether Linux or Windows.\nSet up a Samba AD Domain Controller\nJoin an Active Directory domain\nSet up sharing services\n¶\nSamba can be configured as a file server or print server, to share files and printers with Windows clients.\nSet up a file server\nSet up a print server\nAccess controls\n¶\nShare access controls\nCreate AppArmor profile\nMount CIFS shares permanently\nLegacy options\n¶\nThese options are now deprecated, but still available.\nNT4 domain controller\nOpenLDAP backend\nSee also\n¶\nExplanation:\nIntroduction to Samba", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:26Z", "original_len_words": 140}}
{"id": "4c2dae513a", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/apparmor-profile/", "title": "Create a Samba AppArmor profile - Ubuntu Server documentation", "text": "Create a Samba AppArmor profile - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nCreate a Samba AppArmor profile\n¶\nUbuntu comes with the AppArmor security module, which provides mandatory access controls. The default AppArmor profile for Samba may need to be adapted to your configuration. More details on using AppArmor can be found\nin this guide\n.\nThere are default AppArmor profiles for\n/usr/sbin/smbd\nand\n/usr/sbin/nmbd\n, the Samba daemon binaries, as part of the\napparmor-profiles\npackage.\nInstall\napparmor-profiles\n¶\nTo install the package, enter the following command from a terminal prompt:\nsudo\napt\ninstall\napparmor-profiles\napparmor-utils\nNote\nThis package contains profiles for several other binaries.\nAppArmor profile modes\n¶\nBy default, the profiles for\nsmbd\nand\nnmbd\nare set to ‘complain’ mode. In this mode, Samba can work without modifying the profile, and only logs errors or violations. There is no need to add exceptions for the shares, as the\nsmbd\nservice unit takes care of doing that automatically via a helper script.\nThis is what an\nALLOWED\nmessage looks like. It means that, were the profile not in\ncomplain\nmode, this action would have been denied instead (formatted into multiple lines here for better visibility):\nJun 30 14:41:09 ubuntu kernel: [  621.478989] audit: \ntype=1400 audit(1656600069.123:418):\napparmor=\"ALLOWED\" operation=\"exec\" profile=\"smbd\"\nname=\"/usr/lib/x86_64-linux-gnu/samba/samba-bgqd\" pid=4122 comm=\"smbd\"\nrequested_mask=\"x\" denied_mask=\"x\" fsuid=0 ouid=0\ntarget=\"smbd//null-/usr/lib/x86_64-linux-gnu/samba/samba-bgqd\"\nThe alternative to ‘complain’ mode is ‘enforce’ mode, where any operations that violate policy are blocked. To place the profile into\nenforce\nmode and reload it, run:\nsudo\naa-enforce\n/usr/sbin/smbd\nsudo\napparmor_parser\n-r\n-W\n-T\n/etc/apparmor.d/usr.sbin.smbd\nIt’s advisable to monitor\n/var/log/syslog\nfor\naudit\nentries that contain AppArmor\nDENIED\nmessages, or\n/var/log/audit/audit.log\nif you are running the\nauditd\ndaemon. Actions blocked by AppArmor may surface as odd or unrelated errors in the application.\nFurther reading\n¶\nFor more information on how to use AppArmor, including details of the profile modes,\nthe Debian AppArmor guide\nmay be helpful.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:27Z", "original_len_words": 336}}
{"id": "7018e3a7c3", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/file-server/", "title": "Set up Samba as a file server - Ubuntu Server documentation", "text": "Set up Samba as a file server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSet up Samba as a file server\n¶\nOne of the most common ways to network Ubuntu and Windows computers is to configure Samba as a\nfile server\n. It can be set up to share files with Windows clients, as we’ll see in this section.\nThe server will be configured to share files with any client on the network without prompting for a password. If your environment requires stricter Access Controls see\nShare Access Control\n.\nWarning\nIf you use\nSamba and authd\nat the same time, you must specify user and group mapping. Otherwise, you will encounter permission issues due to mismatched user and group identifiers.\nIf you\nare\nusing Samba with authd, you should follow the instructions in the\nsteps for the server\nguide in the authd documentation instead.\nInstall Samba\n¶\nThe first step is to install the\nsamba\npackage. From a terminal prompt enter:\nsudo\napt\ninstall\nsamba\nThat’s all there is to it; you are now ready to configure Samba to share files.\nConfigure Samba as a file server\n¶\nThe main Samba configuration file is located in\n/etc/samba/smb.conf\n. The default configuration file contains a significant number of comments, which document various configuration directives.\nNote\nNot all available options are included in the default configuration file. See the\nsmb.conf\nman page\nor the\nSamba HOWTO Collection\nfor more details.\nFirst, edit the\nworkgroup\nparameter in the\n[global]\nsection of\n/etc/samba/smb.conf\nand change it to better match your environment:\nworkgroup = EXAMPLE\nCreate a new section at the bottom of the file, or uncomment one of the examples, for the directory you want to share:\n[share]\n    comment = Ubuntu File Server Share\n    path = /srv/samba/share\n    browsable = yes\n    guest ok = yes\n    read only = no\n    create mask = 0755\ncomment\nA short description of the share. Adjust to fit your needs.\npath\nThe path to the directory you want to share.\nNote\nThis example uses\n/srv/samba/sharename\nbecause, according to the\nFilesystem Hierarchy Standard (FHS)\n,\n/srv\nis where site-specific data should be served. Technically, Samba shares can be placed anywhere on the\nfilesystem\nas long as the permissions are correct, but adhering to standards is recommended.\nbrowsable\nEnables Windows clients to browse the shared directory using Windows Explorer.\nguest\nok\nAllows clients to connect to the share without supplying a password.\nread\nonly\ndetermines if the share is read only or if write privileges are granted. Write privileges are allowed only when the value is\nno\n, as is seen in this example. If the value is\nyes\n, then access to the share is read only.\ncreate\nmask\nDetermines the permissions that new files will have when created.\nCreate the directory\n¶\nNow that Samba is configured, the directory needs to be created and the permissions changed. From a terminal, run the following commands:\nsudo\nmkdir\n-p\n/srv/samba/share\nsudo\nchown\nnobody:nogroup\n/srv/samba/share/\nThe\n-p\nswitch tells\nmkdir\nto create the entire directory tree if it doesn’t already exist.\nEnable the new configuration\n¶\nFinally, restart the Samba services to enable the new configuration by running the following command:\nsudo\nsystemctl\nrestart\nsmbd.service\nnmbd.service\nWarning\nOnce again, the above configuration gives full access to any client on the local network. For a more secure configuration see\nShare Access Control\n.\nFrom a Windows client you should now be able to browse to the Ubuntu file server and see the shared directory. If your client doesn’t show your share automatically, try to access your server by its IP address, e.g.\n\\\\192.168.1.1\n, in a Windows Explorer window. To check that everything is working try creating a directory from Windows.\nTo create additional shares simply create new\n[sharename]\nsections in\n/etc/samba/smb.conf\n, and restart Samba. Just make sure that the directory you want to share actually exists and the permissions are correct.\nThe file share named\n[share]\nand the path\n/srv/samba/share\nused in this example can be adjusted to fit your environment. It is a good idea to name a share after a directory on the file system. Another example would be a share name of\n[qa]\nwith a path of\n/srv/samba/qa\n.\nFurther reading\n¶\nFor in-depth Samba configurations see the\nSamba how-to collection\nThe guide is also available\nin printed format\n.\nO’Reilly’s\nUsing Samba\nis another good reference.\nThe\nUbuntu Wiki Samba\npage.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:27Z", "original_len_words": 758}}
{"id": "9a6f493a05", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/member-server-in-an-ad-domain/", "title": "Member server in an Active Directory domain - Ubuntu Server documentation", "text": "Member server in an Active Directory domain - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nMember server in an Active Directory domain\n¶\nA Samba server needs to join the Active Directory (AD) domain before it can serve files and printers to Active Directory users. This is different from\nNetwork User Authentication with SSSD\n, where we integrate the AD users and groups into the local Ubuntu system as if they were local.\nFor Samba to authenticate these users via Server Message Block (SMB) authentication protocols, we need both for the remote users to be “seen”, and for Samba itself to be aware of the domain. In this scenario, Samba is called a Member Server or Domain Member.\nSee also\nSamba itself has the necessary tooling to join an Active Directory domain. It requires a sequence of manual steps and configuration file editing, which is\nthoroughly documented on the Samba wiki\n. It’s useful to read that documentation to get an idea of the steps necessary, and the decisions you will need to make.\nUse\nrealmd\nto join the Active Directory domain\n¶\nFor this guide, though, we are going to use the\nrealmd\npackage and instruct it to use the Samba tooling for joining the AD domain. This package will make certain decisions for us which will work for most cases, but more complex setups involving multiple or very large domains might require additional tweaking.\nInstall\nrealmd\n¶\nFirst, let’s install the necessary packages:\nsudo\napt\ninstall\nrealmd\nsamba\nIn order to have the joined machine registered in the AD\nDNS\n, it needs to have an\nFQDN\nset. You might have that already, if running the\nhostname\n-f\ncommand returns a full\nhostname\nwith domain. If it doesn’t, then set the hostname as follows:\nsudo\nhostnamectl\nhostname\n<yourfqdn>\nFor this guide, we will be using\nj1.internal.example.fake\n, and the AD domain will be\ninternal.example.fake\n.\nVerify the AD server\n¶\nNext, we need to verify that the AD server is both reachable and known by running the following command:\nsudo\nrealm\ndiscover\ninternal.example.fake\nThis should provide an output like this, given our setup:\ninternal.example.fake\n  type: kerberos\n  realm-name: INTERNAL.EXAMPLE.FAKE\n  domain-name: internal.example.fake\n  configured: no\n  server-software: active-directory\n  client-software: sssd\n  required-package: sssd-tools\n  required-package: sssd\n  required-package: libnss-sss\n  required-package: libpam-sss\n  required-package: adcli\n  required-package: samba-common-bin\nrealm\nis suggesting a set of packages for the discovered domain, but we will override that and select the Samba tooling for this join, because we want Samba to become a Member Server.\nJoin the AD domain\n¶\nLet’s join the domain in verbose mode so we can see all the steps:\nsudo\nrealm\njoin\n-v\n--membership-software\n=\nsamba\n--client-software\n=\nwinbind\ninternal.example.fake\nThis should produce the following output for us:\n* Resolving: _ldap._tcp.internal.example.fake\n * Performing LDAP DSE lookup on: 10.0.16.5\n * Successfully discovered: internal.example.fake\nPassword for Administrator:\n * Unconditionally checking packages\n * Resolving required packages\n * Installing necessary packages: libnss-winbind samba-common-bin libpam-winbind winbind\n * LANG=C LOGNAME=root /usr/bin/net --configfile /var/cache/realmd/realmd-smb-conf.A53NO1 -U Administrator --use-kerberos=required ads join internal.example.fake\nPassword for [INTEXAMPLE\\Administrator]:\nUsing short domain name -- INTEXAMPLE\nJoined 'J1' to dns domain 'internal.example.fake'\n * LANG=C LOGNAME=root /usr/bin/net --configfile /var/cache/realmd/realmd-smb-conf.A53NO1 -U Administrator ads keytab create\nPassword for [INTEXAMPLE\\Administrator]:\n * /usr/sbin/update-rc.d winbind enable\n * /usr/sbin/service winbind restart\n * Successfully enrolled machine in realm\nNote\nThis command also installed the\nlibpam-winbind\npackage,\nwhich allows AD users to authenticate to other services on this system via PAM, like SSH or console logins\n. For example, if your SSH server allows password authentication (\nPasswordAuthentication\nyes\nin\n/etc/ssh/sshd_config\n), then the domain users will be allowed to login remotely on this system via SSH.\nIf you don’t expect or need AD users to log into this system (unless it’s via Samba or Windows), then it’s safe and probably best to remove the\nlibpam-winbind\npackage.\nUntil\nbug #1980246\nis fixed, one extra step is needed:\nConfigure\n/etc/nsswitch.conf\nby adding the word\nwinbind\nto the\npasswd\nand\ngroup\nlines as shown below:\npasswd:         files systemd winbind\ngroup:          files systemd winbind\nNow you will be able to query users from the AD domain. Winbind adds the short domain name as a prefix to domain users and groups:\n$ getent passwd INTEXAMPLE\\\\Administrator\nINTEXAMPLE\\administrator:*:2000500:2000513::/home/administrator@INTEXAMPLE:/bin/bash\nYou can find out the short domain name in the\nrealm\noutput shown earlier, or inspect the\nworkgroup\nparameter of\n/etc/samba/smb.conf\n.\nCommon installation options\n¶\nWhen domain users and groups are brought to the Linux world, a bit of translation needs to happen, and sometimes new values need to be created. For example, there is no concept of a “login shell” for AD users, but it exists in Linux.\nThe following are some common\n/etc/samba/smb.conf\noptions you are likely to want to tweak in your installation. The\nsmb.conf(5)\nmanual page explains the\n%\nvariable substitutions and other details:\nhome directory\ntemplate\nhomedir\n=\n/home/%U@%D\n(Another popular choice is\n/home/%D/%U\n)\nlogin shell\ntemplate\nshell\n=\n/bin/bash\nwinbind\nseparator\n=\n\\\nThis is the\n\\\ncharacter between the short domain name and the user or group name that we saw in the\ngetent\npasswd\noutput above.\nwinbind\nuse\ndefault\ndomain\nIf this is set to\nyes\n, then the domain name will not be part of the users and groups. Setting this to\nyes\nmakes the system more friendly towards Linux users, as they won’t have to remember to include the domain name every time a user or group is referenced. However, if multiple domains are involved, such as in an AD forest or other form of domain trust relationship, then leave this setting at\nno\n(default).\nTo have the home directory created automatically the first time a user logs in to the system, and if you haven’t removed\nlibpam-winbind\n, then enable the\npam_mkhomedir\nmodule via this command:\nsudo\npam-auth-update\n--enable\nmkhomedir\nNote that this won’t apply to logins via Samba: this only creates the home directory for system logins like those via\nssh\nor the console.\nExport shares\n¶\nShares can be exported as usual. Since this is now a Member Server, there is no need to deal with user and group management. All of this is integrated with the Active Directory server we joined.\nFor example, let’s create a simple\n[storage]\nshare. Add this to the\n/etc/samba/smb.conf\nfile:\n[storage]\n    path = /storage\n    comment = Storage share\n    writable = yes\n    guest ok = no\nThen create the\n/storage\ndirectory. Let’s also make it\n1777\nso all users can use it, and then ask samba to reload its configuration:\nsudo\nmkdir\n-m\n1777\n/storage\nsudo\nsmbcontrol\nsmbd\nreload-config\nWith this, users from the AD domain will be able to access this share. For example, if there is a user\nubuntu\nthe following command would access the share from another system, using the domain credentials:\n$ smbclient //j1.internal.example.fake/storage -U INTEXAMPLE\\\\ubuntu\nEnter INTEXAMPLE\\ubuntu's password:\nTry \"help\" to get a list of possible commands.\nsmb: \\>\nAnd\nsmbstatus\non the member server will show the connected user:\n$ sudo smbstatus\n\nSamba version 4.15.5-Ubuntu\nPID     Username     Group        Machine                                   Protocol Version  Encryption           Signing\n----------------------------------------------------------------------------------------------------------------------------------------\n3631    INTEXAMPLE\\ubuntu INTEXAMPLE\\domain users 10.0.16.1 (ipv4:10.0.16.1:39534)          SMB3_11           -                    partial(AES-128-CMAC)\n\nService      pid     Machine       Connected at                     Encryption   Signing\n---------------------------------------------------------------------------------------------\nstorage      3631    10.0.16.1     Wed Jun 29 17:42:54 2022 UTC     -            -\n\nNo locked files\nYou can also restrict access to the share as usual. Just keep in mind the syntax for the domain users. For example, to restrict access to the\n[storage]\nshare we just created to\nonly\nmembers of the\nLTS\nReleases\ndomain group, add the\nvalid\nusers\nparameter like below:\n[storage]\n    path = /storage\n    comment = Storage share\n    writable = yes\n    guest ok = no\n    valid users = \"@INTEXAMPLE\\ LTS Releases\"\nChoose an\nidmap\nbackend\n¶\nrealm\nmade some choices for us when we joined the domain. A very important one is the\nidmap\nbackend, and it might need changing for more complex setups.\nUser and group identifiers on the AD side are not directly usable as identifiers on the Linux site. A\nmapping\nneeds to be performed.\nWinbind supports several\nidmap\nbackends, and each one has its own manual page. The three main ones are:\nidmap_ad(8)\nidmap_autorid(8)\nidmap_rid(8)\nChoosing the correct backend for each deployment type needs careful planing. Upstream has some guidelines at\nChoosing an\nidmap\nbackend\n, and each man page has more details and recommendations.\nThe\nrealm\ntool selects (by default) the\nrid\nbackend. This backend uses an algorithm to calculate the Unix user and group IDs from the respective RID value on the AD side. You might need to review the\nidmap\nconfig\nsettings in\n/etc/samba/smb.conf\nand make sure they can accommodate the number of users and groups that exist in the domain, and that the range does not overlap with users from other sources.\nFor example, these settings:\nidmap config * : range = 10000-999999\nidmap config intexample : backend = rid\nidmap config intexample : range = 2000000-2999999\nidmap config * : backend = tdb\nWill reserve the\n2,000,000\nthrough\n2,999,999\nrange for user and group ID allocations on the Linux side for the\nintexample\ndomain. The default backend (\n*\n, which acts as a “globbing” catch-all rule) is used for the\nBUILTIN\nuser and groups, and other domains (if they exist). It’s important that these ranges do not overlap.\nThe\nAdministrator\nuser we inspected before with\ngetent\npasswd\ncan give us a glimpse of how these ranges are used (output format changed for clarity):\n$\nid\nINTEXAMPLE\n\\\\\nAdministrator\nuid\n=\n2000500\n(\nINTEXAMPLE\n\\a\ndministrator\n)\ngid\n=\n2000513\n(\nINTEXAMPLE\n\\d\nomain\nusers\n)\ngroups\n=\n2000513\n(\nINTEXAMPLE\n\\d\nomain\nusers\n)\n,\n2000500\n(\nINTEXAMPLE\n\\a\ndministrator\n)\n,\n2000572\n(\nINTEXAMPLE\n\\d\nenied\nrodc\npassword\nreplication\ngroup\n)\n,\n2000519\n(\nINTEXAMPLE\n\\e\nnterprise\nadmins\n)\n,\n2000518\n(\nINTEXAMPLE\n\\s\nchema\nadmins\n)\n,\n2000520\n(\nINTEXAMPLE\n\\g\nroup\npolicy\ncreator\nowners\n)\n,\n2000512\n(\nINTEXAMPLE\n\\d\nomain\nadmins\n)\n,\n10001\n(\nBUILTIN\n\\u\nsers\n)\n,\n10000\n(\nBUILTIN\n\\a\ndministrators\n)\nFurther reading\n¶\nThe Samba Wiki", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:27Z", "original_len_words": 1679}}
{"id": "5f2d29032a", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/mount-cifs-shares-permanently/", "title": "How to mount CIFS shares permanently - Ubuntu Server documentation", "text": "How to mount CIFS shares permanently - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to mount CIFS shares permanently\n¶\nCommon Internet File System (CIFS) shares are a file-sharing protocol used (mainly) in Windows for accessing files and resources (such as printers) over a network.\nPermanently mounting CIFS shares involves configuring your system to automatically connect to these shared resources when the system boots, which is useful when network users need consistent and regular access to them.\nIn this guide, we will show you how to permanently mount and access CIFS shares. The shares can be hosted on a Windows computer/server, or on a Linux/UNIX server running Samba. If you want to know how to host shares, see\nIntroduction to Samba\n.\nPrerequisites\n¶\nIn order to use this guide, you will need to ensure that your network connections have been configured properly. Throughout this guide, we will use the following naming conventions:\nThe local (Ubuntu) username is\nubuntuusername\nThe share username on the Windows computer is\nmsusername\nThe share password on the Windows computer is\nmspassword\nThe Windows computer’s name is\nservername\n(this can be either an IP address or an assigned name)\nThe name of the share is\nsharename\nThe shares are to be mounted in\n/media/windowsshare\nInstall CIFS\n¶\nTo install CIFS, run the following command:\nsudo\napt-get\ninstall\ncifs-utils\nMount unprotected (guest) network folders\n¶\nFirst, let’s create the mount directory. You will need a separate directory for each mount:\nsudo\nmkdir\n/media/windowsshare\nThen edit your\n/etc/fstab\nfile (with root privileges) to add this line:\n//servername/sharename /media/windowsshare cifs guest,uid=1000 0 0\nWhere:\nservername\nis the server\nhostname\nor IP address,\nguest\nindicates you don’t need a password to access the share,\nuid=1000\nmakes the Linux user (specified by the ID) the owner of the mounted share, allowing them to rename files, and\nIf there is any space in the server path, you need to replace it by\n\\040\n, for example:\n//servername/My\\040Documents\nAfter you add the entry to\n/etc/fstab\n, type:\nsudo\nmount\n/media/windowsshare\nMount password-protected network folders\n¶\nTo auto-mount a password-protected share, you can edit\n/etc/fstab\n(with root privileges), and add this line:\n//servername/sharename /media/windowsshare cifs username=msusername,password=mspassword 0 0\nThis is not a good idea however:\n/etc/fstab\nis readable by everyone – and so is your Windows password within it. The way around this is to use a credentials file. This is a file that contains just the username and password.\nCreate a credentials file\n¶\nUsing a text editor, create a file for your remote server’s logon credential:\ngedit\n~/.smbcredentials\nEnter your Windows username and password in the file:\nusername=msusername\n\npassword=mspassword\nSave the file and exit the editor.\nChange the permissions of the file to prevent unwanted access to your credentials:\nchmod\n600\n~/.smbcredentials\nThen edit your\n/etc/fstab\nfile (with root privileges) to add this line (replacing the insecure line in the example above, if you added it):\n//servername/sharename /media/windowsshare cifs credentials=/home/ubuntuusername/.smbcredentials 0 0\nSave the file and exit the editor.\nFinally, test mounting the share by running:\nsudo\nmount\n/media/windowsshare\nIf there are no errors, you should test how it works after a reboot. Your remote share should mount automatically. However, if the remote server goes offline, the boot process could present errors because it won’t be possible to mount the share.\nChanging the share ownership\n¶\nIf you need to change the owner of a share, you’ll need to add a\nUID\n(short for ‘User ID’) or\nGID\n(short for ‘Group ID’) parameter to the share’s mount options:\n//servername/sharename /media/windowsshare cifs uid=ubuntuusername,credentials=/home/ubuntuusername/.smbcredentials 0 0\nMount network folders with authd\n¶\nIf you use Samba and authd at the same time, you must specify user and group mapping. Otherwise, you will encounter permission issues due to mismatched user and group identifiers.\nIf you\nare\nusing Samba with authd, follow the instructions in the\nsteps for the client\nguide in the authd documentation.\nMount password-protected shares using\nlibpam-mount\n¶\nIn addition to the initial assumptions, we’re assuming here that your username and password are the same on both the Ubuntu machine and the network drive.\nInstall\nlibpam-mount\n¶\nsudo\napt-get\ninstall\nlibpam-mount\nEdit\n/etc/security/pam_mount.conf.xml\nusing your preferred text editor.\nsudo\ngedit\n/etc/security/pam_mount.conf.xml\nFirst, we’re moving the user-specific config parts to a file which users can actually edit themselves.\nRemove the commenting tags\n(<!--\nand\n-->)\nsurrounding the section called\n<luserconf\nname=\".pam_mount.conf.xml\"\n/>\n. We also need to enable some extra mount options to be used. For that, edit the “\n<mntoptions\nallow=...\n” tag and add\nuid,gid,dir_mode,credentials\nto it.\nSave the file when done. With this in place, users can create their own\n~/.pam_mount.conf.xml\n.\ngedit\n~/.pam_mount.conf.xml\nAdd the following:\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n\n<pam_mount>\n\n<volume options=\"uid=%(USER),gid=100,dir_mode=0700,credentials=/home/ubuntuusername/.smbcredentials,nosuid,nodev\" user=\"*\" mountpoint=\"/media/windowsshare\" path=\"sharename\" server=\"servername\" fstype=\"cifs\" />\n\n</pam_mount>\nTroubleshooting\n¶\nLogin errors\n¶\nIf you get the error “mount error(13) permission denied”, then the server denied your access. Here are the first things to check:\nAre you using a valid username and password? Does that account really have access to this folder?\nDo you have blank space in your credentials file? It should be\npassword=mspassword\n, not\npassword\n=\nmspassword\n.\nDo you need a domain? For example, if you are told that your username is\nSALES\\sally\n, then actually your username is\nsally\nand your domain is\nSALES\n. You can add a\ndomain=SALES\nline to the\n~/.credentials\nfile.\nThe security and version settings are interrelated. SMB1 is insecure and no longer supported. At first, try to not specify either security or version: do not specify\nsec=\nor\nvers=\n. If you still have authentication errors then you may need to specify either\nsec=\nor\nvers=\nor both. You can try the options listed at the\nmount.cifs(8)\nmanual page.\nMount after login instead of boot\n¶\nIf for some reason, such as networking problems, the automatic mounting during boot doesn’t work, you can add the\nnoauto\nparameter to your CIFS\nfstab\nentry and then have the share mounted at login.\nIn\n/etc/fstab\n:\n//servername/sharename /media/windowsshare cifs noauto,credentials=/home/ubuntuusername/.smbpasswd 0 0\nYou can now manually mount the share after you log in. If you would like the share to be automatically mounted after each login, please see the section above about\nlibpam-mount\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:27Z", "original_len_words": 1058}}
{"id": "e34ac70834", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/nt4-domain-controller-legacy/", "title": "NT4 domain controller (legacy) - Ubuntu Server documentation", "text": "NT4 domain controller (legacy) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nNT4 domain controller (legacy)\n¶\nNote\nThis section is flagged as\nlegacy\nbecause nowadays, Samba can be deployed in full Active Directory domain controller mode, and the old-style NT4 Primary Domain Controller is deprecated.\nA Samba server can be configured to appear as a Windows NT4-style domain controller. A major advantage of this configuration is the ability to centralise user and machine credentials. Samba can also use multiple backends to store the user information.\nPrimary domain controller\n¶\nIn this section, we’ll install and configure Samba as a Primary Domain Controller (PDC) using the default\nsmbpasswd\nbackend.\nInstall Samba\n¶\nFirst, we’ll install Samba, and\nlibpam-winbind\n(to sync the user accounts), by entering the following in a terminal prompt:\nsudo\napt\ninstall\nsamba\nlibpam-winbind\nConfigure Samba\n¶\nNext, we’ll configure Samba by editing\n/etc/samba/smb.conf\n. The\nsecurity\nmode should be set to\nuser\n, and the\nworkgroup\nshould relate to your organization:\nworkgroup = EXAMPLE\n...\nsecurity = user\nIn the commented “Domains” section, add or uncomment the following (the last line has been split to fit the format of this document):\ndomain logons = yes\nlogon path = \\\\%N\\%U\\profile\nlogon drive = H:\nlogon home = \\\\%N\\%U\nlogon script = logon.cmd\nadd machine script = sudo /usr/sbin/useradd -N -g machines -c Machine -d\n      /var/lib/samba -s /bin/false %u\nNote\nIf you wish to not use\nRoaming Profiles\nleave the\nlogon\nhome\nand\nlogon\npath\noptions commented out.\ndomain\nlogons\nProvides the\nnetlogon\nservice, causing Samba to act as a domain controller.\nlogon\npath\nPlaces the user’s Windows profile into their home directory. It is also possible to configure a\n[profiles]\nshare placing all profiles under a single directory.\nlogon\ndrive\nSpecifies the home directory local path.\nlogon\nhome\nSpecifies the home directory location.\nlogon\nscript\nDetermines the script to be run locally once a user has logged in. The script needs to be placed in the\n[netlogon]\nshare.\nadd\nmachine\nscript\nA script that will automatically create the\nMachine Trust Account\nneeded for a workstation to join the domain.\nIn this example the\nmachines\ngroup will need to be created using the\naddgroup\nutility (see\nSecurity - Users: Adding and Deleting Users\nfor details).\nMapping shares\n¶\nUncomment the\n[homes]\nshare to allow the\nlogon\nhome\nto be mapped:\n[homes]\n   comment = Home Directories\n   browseable = no\n   read only = no\n   create mask = 0700\n   directory mask = 0700\n   valid users = %S\nWhen configured as a domain controller, a\n[netlogon]\nshare needs to be configured. To enable the share, uncomment:\n[netlogon]\n   comment = Network Logon Service\n   path = /srv/samba/netlogon\n   guest ok = yes\n   read only = yes\n   share modes = no\nNote\nThe original\nnetlogon\nshare path is\n/home/samba/netlogon\n, but according to the\nFilesystem Hierarchy Standard (FHS)\n,\n/srv is the correct location\nfor site-specific data provided by the system.\nNow create the\nnetlogon\ndirectory, and an empty (for now)\nlogon.cmd\nscript file:\nsudo\nmkdir\n-p\n/srv/samba/netlogon\nsudo\ntouch\n/srv/samba/netlogon/logon.cmd\nYou can enter any normal Windows logon script commands in\nlogon.cmd\nto customize the client’s environment.\nRestart Samba to enable the new domain controller, using the following command:\nsudo\nsystemctl\nrestart\nsmbd.service\nnmbd.service\nFinal setup tasks\n¶\nLastly, there are a few additional commands needed to set up the appropriate rights.\nSince\nroot\nis disabled by default, a system group needs to be mapped to the Windows\nDomain Admins\ngroup in order to join a workstation to the domain. Using the\nnet\nutility, from a terminal enter:\nsudo\nnet\ngroupmap\nadd\nntgroup\n=\n\"Domain Admins\"\nunixgroup\n=\nsysadmin\nrid\n=\n512\ntype\n=\nd\nYou should change\nsysadmin\nto whichever group you prefer. Also, the user joining the domain needs to be a member of the\nsysadmin\ngroup, as well as a member of the system\nadmin\ngroup. The\nadmin\ngroup allows\nsudo\nuse.\nIf the user does not have Samba credentials yet, you can add them with the\nsmbpasswd\nutility. Change the\nsysadmin\nusername appropriately:\nsudo\nsmbpasswd\n-a\nsysadmin\nAlso, rights need to be explicitly provided to the\nDomain Admins\ngroup to allow the\nadd machine script\n(and other admin functions) to work. This is achieved by executing:\nnet\nrpc\nrights\ngrant\n-U\nsysadmin\n\"EXAMPLE\\Domain Admins\"\nSeMachineAccountPrivilege\n\\\nSePrintOperatorPrivilege\nSeAddUsersPrivilege\nSeDiskOperatorPrivilege\n\\\nSeRemoteShutdownPrivilege\nYou should now be able to join Windows clients to the Domain in the same manner as joining them to an NT4 domain running on a Windows server.\nBackup domain controller\n¶\nWith a Primary Domain Controller (PDC) on the network it is best to have a Backup Domain Controller (BDC) as well. This will allow clients to authenticate in case the PDC becomes unavailable.\nWhen configuring Samba as a BDC you need a way to sync account information with the PDC. There are multiple ways of accomplishing this; secure copy protocol (SCP),\nrsync\n, or by using LDAP as the\npassdb\nbackend.\nUsing LDAP is the most robust way to sync account information, because both domain controllers can use the same information in real time. However, setting up an LDAP server may be overly complicated for a small number of user and computer accounts. See\nSamba - OpenLDAP Backend\nfor details.\nFirst, install\nsamba\nand\nlibpam-winbind\n. From a terminal enter:\nsudo\napt\ninstall\nsamba\nlibpam-winbind\nNow, edit\n/etc/samba/smb.conf\nand uncomment the following in the\n[global]\n:\nworkgroup = EXAMPLE\n...\nsecurity = user\nIn the commented\nDomains\nuncomment or add:\ndomain logons = yes\ndomain master = no\nMake sure a user has rights to read the files in\n/var/lib/samba\n. For example, to allow users in the\nadmin\ngroup to SCP the files, enter:\nsudo\nchgrp\n-R\nadmin\n/var/lib/samba\nNext, sync the user accounts, using SCP to copy the\n/var/lib/samba\ndirectory from the PDC:\nsudo\nscp\n-r\nusername@pdc:/var/lib/samba\n/var/lib\nYou can replace\nusername\nwith a valid username and\npdc\nwith the\nhostname\nor IP address of your actual PDC.\nFinally, restart samba:\nsudo\nsystemctl\nrestart\nsmbd.service\nnmbd.service\nYou can test that your Backup Domain Controller is working by first stopping the Samba daemon on the PDC – then try to log in to a Windows client joined to the domain.\nAnother thing to keep in mind is if you have configured the\nlogon\nhome\noption as a directory on the PDC, and the PDC becomes unavailable, access to the user’s\nHome\ndrive will also be unavailable. For this reason it is best to configure the\nlogon\nhome\nto reside on a separate file server from the PDC and BDC.\nFurther reading\n¶\nFor in depth Samba configurations see the\nSamba HOWTO Collection\n.\nThe guide is also available\nin printed format\n.\nO’Reilly’s\nUsing Samba\nis also a good reference.\nChapter 4\nof the Samba HOWTO Collection explains setting up a Primary Domain Controller.\nChapter 5\nof the Samba HOWTO Collection explains setting up a Backup Domain Controller.\nThe\nUbuntu Wiki Samba\npage.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:28Z", "original_len_words": 1176}}
{"id": "73084c2fb4", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/openldap-backend-legacy/", "title": "OpenLDAP backend (legacy) - Ubuntu Server documentation", "text": "OpenLDAP backend (legacy) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nOpenLDAP backend (legacy)\n¶\nNote\nThis section is flagged as\nlegacy\nbecause nowadays, Samba 4 is best integrated with its own LDAP server in Active Directory mode. Integrating Samba with LDAP as described here covers the NT4 mode, which has been deprecated for many years.\nThis section covers the integration of Samba with LDAP. The Samba server’s role will be that of a “standalone” server and the LDAP directory will provide the authentication layer in addition to containing the user, group, and machine account information that Samba requires in order to function (in any of its 3 possible roles). The pre-requisite is an OpenLDAP server configured with a directory that can accept authentication requests. See\nInstall LDAP\nand\nLDAP with Transport Layer Security\nfor details on fulfilling this requirement. Once those steps are completed, you will need to decide what specifically you want Samba to do for you and then configure it accordingly.\nThis guide will assume that the LDAP and Samba services are running on the same server and therefore use SASL EXTERNAL authentication whenever changing something under\ncn=config\n. If that is not your scenario, you will have to run those LDAP commands on the LDAP server.\nInstall the software\n¶\nThere are two packages needed when integrating Samba with LDAP:\nsamba\nand\nsmbldap-tools\n.\nStrictly speaking, the\nsmbldap-tools\npackage isn’t needed, but unless you have some other way to manage the various Samba entities (users, groups, computers) in an LDAP context then you should install it.\nInstall these packages now:\nsudo\napt\ninstall\nsamba\nsmbldap-tools\nConfigure LDAP\n¶\nWe will now configure the LDAP server so that it can accommodate Samba data. We will perform three tasks in this section:\nImport a schema\nIndex some entries\nAdd objects\nSamba schema\n¶\nIn order for OpenLDAP to be used as a backend for Samba, the DIT will need to use attributes that can properly describe Samba data. Such attributes can be obtained by introducing a Samba LDAP schema. Let’s do this now.\nThe schema is found in the now-installed samba package and is already in the LDIF format. We can import it with one simple command:\nsudo\nldapadd\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\n/usr/share/doc/samba/examples/LDAP/samba.ldif\nTo query and view this new schema:\nsudo\nldapsearch\n-Q\n-LLL\n-Y\nEXTERNAL\n-H\nldapi:///\n-b\ncn\n=\nschema,cn\n=\nconfig\n'cn=*samba*'\nSamba indices\n¶\nNow that\nslapd\nknows about the Samba attributes, we can set up some indices based on them. Indexing entries is a way to improve performance when a client performs a filtered search on the DIT.\nCreate the file\nsamba_indices.ldif\nwith the following contents:\ndn: olcDatabase={1}mdb,cn=config\nchangetype: modify\nreplace: olcDbIndex\nolcDbIndex: objectClass eq\nolcDbIndex: uidNumber,gidNumber eq\nolcDbIndex: loginShell eq\nolcDbIndex: uid,cn eq,sub\nolcDbIndex: memberUid eq,sub\nolcDbIndex: member,uniqueMember eq\nolcDbIndex: sambaSID eq\nolcDbIndex: sambaPrimaryGroupSID eq\nolcDbIndex: sambaGroupType eq\nolcDbIndex: sambaSIDList eq\nolcDbIndex: sambaDomainName eq\nolcDbIndex: default sub,eq\nUsing the\nldapmodify\nutility load the new indices:\nsudo\nldapmodify\n-Q\n-Y\nEXTERNAL\n-H\nldapi:///\n-f\nsamba_indices.ldif\nIf all went well you should see the new indices when using\nldapsearch\n:\nsudo\nldapsearch\n-Q\n-LLL\n-Y\nEXTERNAL\n-H\n\\\nldapi:///\n-b\ncn\n=\nconfig\nolcDatabase\n={\n1\n}\nmdb\nolcDbIndex\nAdding Samba LDAP objects\n¶\nNext, configure the\nsmbldap-tools\npackage to match your environment. The package comes with a configuration helper script called\nsmbldap-config\n. Before running it, though, you should decide on two important configuration settings in\n/etc/samba/smb.conf\n:\nnetbios name\nHow this server will be known. The default value is derived from the server’s\nhostname\n, but truncated at 15 characters.\nworkgroup\nThe workgroup name for this server, or, if you later decide to make it a domain controller, this will be the domain.\nIt’s important to make these choices now because\nsmbldap-config\nwill use them to generate the config that will be later stored in the LDAP directory. If you run\nsmbldap-config\nnow and later change these values in\n/etc/samba/smb.conf\nthere will be an inconsistency.\nOnce you are happy with\nnetbios\nname\nand\nworkgroup\n, proceed to generate the\nsmbldap-tools\nconfiguration by running the configuration script which will ask you some questions:\nsudo\nsmbldap-config\nSome of the more important ones:\nworkgroup name\nHas to match what you will configure in\n/etc/samba/smb.conf\nlater on.\nldap suffix\nHas to match the LDAP suffix you chose when you configured the LDAP server.\nother ldap suffixes\nThey are all relative to\nldap\nsuffix\nabove. For example, for\nldap\nuser\nsuffix\nyou should use\nou=People\n, and for computer/machines, use\nou=Computers\n.\nldap master bind dn\nand\nbind password\nUse the Root\nDN\ncredentials.\nThe\nsmbldap-populate\nscript will then add the LDAP objects required for Samba. It will ask you for a password for the “domain root” user, which is also the “root” user stored in LDAP:\nsudo\nsmbldap-populate\n-g\n10000\n-u\n10000\n-r\n10000\nThe\n-g\n,\n-u\nand\n-r\nparameters tell\nsmbldap-tools\nwhere to start the numeric\nuid\nand\ngid\nallocation for the LDAP users. You should pick a range start that does not overlap with your local\n/etc/passwd\nusers.\nYou can create a LDIF file containing the new Samba objects by executing\nsudo\nsmbldap-populate\n-e\nsamba.ldif\n. This allows you to look over the changes making sure everything is correct. If it is, rerun the script without the\n'-e'\nswitch. Alternatively, you can take the LDIF file and import its data as per usual.\nYour LDAP directory now has the necessary information to authenticate Samba users.\nSamba configuration\n¶\nTo configure Samba to use LDAP, edit its configuration file\n/etc/samba/smb.conf\ncommenting out the default\npassdb\nbackend\nparameter and adding some LDAP-related ones. Make sure to use the same values you used when running\nsmbldap-populate\n:\n#  passdb backend = tdbsam\nworkgroup = EXAMPLE\n    \n# LDAP Settings\npassdb backend = ldapsam:ldap://ldap01.example.com\nldap suffix = dc=example,dc=com\nldap user suffix = ou=People\nldap group suffix = ou=Groups\nldap machine suffix = ou=Computers\nldap idmap suffix = ou=Idmap\nldap admin dn = cn=admin,dc=example,dc=com\nldap ssl = start tls\nldap passwd sync = yes\nChange the values to match your environment.\nNote\nThe\nsmb.conf\nas shipped by the package is quite long and has many configuration examples. An easy way to visualize it without any comments is to run\ntestparm\n-s\n.\nNow inform Samba about the Root DN user’s password (the one set during the installation of the\nslapd\npackage):\nsudo\nsmbpasswd\n-W\nAs a final step to have your LDAP users be able to connect to Samba and authenticate, we need these users to also show up in the system as “Unix” users. Use SSSD for that as detailed in\nNetwork User Authentication with SSSD\n.\nInstall\nsssd-ldap\n:\nsudo\napt\ninstall\nsssd-ldap\nConfigure\n/etc/sssd/sssd.conf\n:\n[\nsssd\n]\nconfig_file_version\n=\n2\ndomains\n=\nexample.com\n[\ndomain/example.com\n]\nid_provider\n=\nldap\nauth_provider\n=\nldap\nldap_uri\n=\nldap://ldap01.example.com\ncache_credentials\n=\nTrue\nldap_search_base\n=\ndc\n=\nexample,dc\n=\ncom\nAdjust permissions and start the service:\nsudo\nchmod\n0600\n/etc/sssd/sssd.conf\nsudo\nchown\nroot:root\n/etc/sssd/sssd.conf\nsudo\nsystemctl\nstart\nsssd\nRestart the Samba services:\nsudo\nsystemctl\nrestart\nsmbd.service\nnmbd.service\nTo quickly test the setup, see if\ngetent\ncan list the Samba groups:\n$\ngetent\ngroup\nReplicators\nReplicators:*:552:\nNote\nThe names are case sensitive!\nIf you have existing LDAP users that you want to include in your new LDAP-backed Samba they will, of course, also need to be given some of the extra Samba specific attributes. The\nsmbpasswd\nutility can do this for you:\nsudo\nsmbpasswd\n-a\nusername\nYou will be prompted to enter a password. It will be considered as the new password for that user. Making it the same as before is reasonable. Note that this command cannot be used to create a new user from scratch in LDAP (unless you are using\nldapsam:trusted\nand\nldapsam:editposix\n, which are not covered in this guide).\nTo manage user, group, and machine accounts use the utilities provided by the\nsmbldap-tools\npackage. Here are some examples:\nTo add a new user with a home directory:\nsudo\nsmbldap-useradd\n-a\n-P\n-m\nusername\nThe\n-a\noption adds the Samba attributes, and the\n-P\noption calls the\nsmbldap-passwd\nutility after the user is created allowing you to enter a password for the user. Finally,\n-m\ncreates a local home directory. Test with the\ngetent\ncommand:\ngetent\npasswd\nusername\nTo remove a user:\nsudo\nsmbldap-userdel\nusername\nIn the above command, use the\n-r\noption to remove the user’s home directory.\nTo add a group:\nsudo\nsmbldap-groupadd\n-a\ngroupname\nAs for\nsmbldap-useradd\n, the\n-a\nadds the Samba attributes.\nTo make an existing user a member of a group:\nsudo\nsmbldap-groupmod\n-m\nusername\ngroupname\nThe\n-m\noption can add more than one user at a time by listing them in comma-separated format.\nTo remove a user from a group:\nsudo\nsmbldap-groupmod\n-x\nusername\ngroupname\nTo add a Samba machine account:\nsudo\nsmbldap-useradd\n-t\n0\n-w\nusername\nReplace\nusername\nwith the name of the workstation. The\n-t\n0\noption creates the machine account without a delay, while the\n-w\noption specifies the user as a machine account.\nResources\n¶\nUpstream documentation collection\nUpstream samba wiki", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:28Z", "original_len_words": 1540}}
{"id": "75c6e3eafc", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/print-server/", "title": "Set up Samba as a print server - Ubuntu Server documentation", "text": "Set up Samba as a print server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSet up Samba as a print server\n¶\nAnother common way to network Ubuntu and Windows computers is to configure Samba as a\nprint server\n. This will allow it to share printers installed on an Ubuntu server, whether locally or over the network.\nJust as we did in\nusing Samba as a file server\n, this section will configure Samba to allow any client on the local network to use the installed printers without prompting for a username and password.\nIf your environment requires stricter Access Controls see\nShare Access Control\n.\nInstall and configure CUPS\n¶\nBefore installing and configuring Samba as a print server, it is best to already have a working CUPS installation. See\nour guide on CUPS\nfor details.\nInstall Samba\n¶\nTo install the\nsamba\npackage, run the following command in your terminal:\nsudo\napt\ninstall\nsamba\nConfigure Samba\n¶\nAfter installing\nsamba\n, edit\n/etc/samba/smb.conf\n. Change the\nworkgroup\nattribute to what is appropriate for your network:\nworkgroup = EXAMPLE\nIn the\n[printers]\nsection, change the\nguest ok\noption to ‘yes’:\nbrowsable = yes\nguest ok = yes\nAfter editing\nsmb.conf\n, restart Samba:\nsudo\nsystemctl\nrestart\nsmbd.service\nnmbd.service\nThe default Samba configuration will automatically share any printers installed. Now all you need to do is install the printer locally on your Windows clients.\nFurther reading\n¶\nFor in-depth Samba configurations see the\nSamba HOWTO Collection\n.\nThe guide is also available in\nprinted format\n.\nO’Reilly’s\nUsing Samba\nis another good reference.\nAlso, see the\nCUPS Website\nfor more information on configuring CUPS.\nThe\nUbuntu Wiki Samba\npage.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:28Z", "original_len_words": 303}}
{"id": "09add709cb", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/provision-samba-ad-controller/", "title": "Provisioning a Samba Active Directory Domain Controller - Ubuntu Server documentation", "text": "Provisioning a Samba Active Directory Domain Controller - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nProvisioning a Samba Active Directory Domain Controller\n¶\nA Samba Active Directory Domain Controller (also known as just Samba AD/DC) is a server running Samba services that can provide authentication to domain users and computers, linux or Windows. It should be dedicated to authentication and authorization services, and not provide file or print services: that should be the role of member servers joined to the domain.\nSee also\nFor more information on why the Samba AD/DC server should not be used to provide file and print services, please refer to this\nlist of reasons and caveats in the Samba Wiki\n.\nThis guide will show how to bootstrap a Samba AD/DC server and verify it’s functioning properly.\nInstallation\n¶\nThis command will install the packages necessary for bootstrapping and testing the Samba AD/DC services:\nsudo apt install samba-ad-dc krb5-user bind9-dnsutils\nNote that the installation of\nkrb5-user\nmight prompt some questions. It’s fine to answer with just the default values (just hit ENTER) at this stage, including when it asks about what the Kerberos realm and servers are.\nNext, we should make sure that the normal Samba services\nsmbd\n,\nnmbd\n, and\nwindind\n, are disabled:\nsudo systemctl disable --now smbd nmbd winbind\nsudo systemctl mask smbd nmbd winbind\nAnd enable the Samba AD/DC service, but without starting it yet:\nsudo systemctl unmask samba-ad-dc\nsudo systemctl enable samba-ad-dc\nNote\nA Samba AD/DC deployment represents a collection of services connected to each other, and needs its own specific systemd service unit.\nThe Samba AD/DC provisioning tool will want to create a new Samba configuration file, dedicated to the AD/DC role, but it will refrain from replacing an existing one. We have to therefore move it away before continuing:\nsudo mv /etc/samba/smb.conf /etc/samba/smb.conf.orig\nProvisioning\n¶\nWith the packages installed, the Samba AD/DC service can be provisioned. For this how-to, we will use the following values:\nDomain:\nEXAMPLE\nRealm:\nEXAMPLE.INTERNAL\nAdministrator password:\nPassw0rd\n(pick your own)\nTo perform the provisioning, run this command:\nsudo samba-tool domain provision \\\n    --domain EXAMPLE \\\n    --realm=EXAMPLE.INTERNAL \\\n    --adminpass=Passw0rd \\\n    --server-role=dc \\\n    --use-rfc2307 \\\n    --dns-backend=SAMBA_INTERNAL\nIf you omit the\n--adminpass\noption, a random password will be chosen and be included in the provisioning output. Be sure to save it!\nWarning\nProviding passwords in the command line is generally unsafe. Other users on the system who can see the process listing can spot the password, and it will also be saved in the shell history, unless the command starts with a blank space.\nThe command will take a few seconds to run, and will output a lot of information. In the end, it should be like this (long lines truncated for better readability):\n(...)\nINFO ... #498: Server Role:     active directory domain controller\nINFO ... #499: Hostname:        ad\nINFO ... #500: NetBIOS Domain:  EXAMPLE\nINFO ... #501: DNS Domain:      example.internal\nINFO ... #502: DOMAIN SID:      S-1-5-21-2373640847-2123283686-338028823\nIf you didn’t use the\n--adminpass\noption, the administrator password will be part of the output above in a line like this:\nINFO ... #497: Admin password:  sbruR-Py>];k=KDn1H58PB#\nPost-installation steps\n¶\nThe AD/DC services are not running yet. Some post-installation steps are necessary before the services can be started.\nFirst, adjust\ndns\nforwarder\nin\n/etc/samba/smb.conf\nto point at your\nDNS\nserver. It will be used for all queries that are not local to the Active Directory domain we just deployed (\nEXAMPLE.INTERNAL\n). The provisioning script simply copied the server IP from\n/etc/resolv.conf\nto this parameter, but if we leave it like that, it will point back to itself:\n[global]\n    dns forwarder = 127.0.0.53\nIf unsure, it’s best to use the current DNS server this system is already using. That can be seen with the\nresolvectl\nstatus\ncommand. Look for the\nCurrent\nDNS\nServer\nline and note the IP address:\nGlobal\n         Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n  resolv.conf mode: stub\n\nLink 2 (enp5s0)\n    Current Scopes: DNS\n         Protocols: +DefaultRoute -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\nCurrent DNS Server: 10.10.17.1\n       DNS Servers: 10.10.17.1\n        DNS Domain: lxd\nIn the above example, the DNS server is\n10.10.17.1\n, so that should be used in\n/etc/samba/smb.conf\n’s\ndns\nforwarder\n:\n[global]\n    dns forwarder = 10.10.17.1\nNext, we need to be sure this system will be using the Samba DNS server for its queries, and for that we need to adjust\n/etc/resolv.conf\n. That file will be a symlink, so instead of just rewriting its contents, first we have to remove it:\nsudo unlink /etc/resolv.conf\nNote: this will make sudo issue complaints about DNS from this point on, until the Samba DNS service is up and running.\nAnd now recreate the file\n/etc/resolv.conf\nwith this content:\nnameserver 127.0.0.1\nsearch example.internal\nStop and disable\nsystemd-resolved\nas the resolver will now be using the Samba DNS server directly:\nsudo systemctl disable --now systemd-resolved\nFinally, we need to update\n/etc/krb5.conf\nwith the content generated by the Samba provisioning script. Since this system is dedicated to being a Samba AD/DC server, we can just copy the generated file over:\nsudo cp -f /var/lib/samba/private/krb5.conf /etc/krb5.conf\nIf there are other Kerberos realms involved, you should manually merge the two files.\nWe are now ready to start the Samba AD/DC services:\nsudo systemctl start samba-ad-dc\nAnd this concludes the installation. The next section will show how to perform some basic checks.\nVerification\n¶\nHere are some verification steps that can be run to check that the provisioning was done correctly and the service is ready.\nKerberos authentication\n¶\nA Kerberos ticket for the\nAdministrator\nprincipal can be obtained with the\nkinit\ncommand. Note you don’t have to be\nroot\nto run this command. It’s perfectly fine to get a ticket for a different principal than your own user, even a privileged one:\nkinit Administrator\nThe command will ask for a password. The password is the one supplied to the\nsamba-tool\ncommand earlier, when the domain was provisioned, or the randomly chosen one if the\n--adminpass\noption was not used.\nPassword for Administrator@EXAMPLE.INTERNAL:\nSee also\nIf you are not familiar with Kerberos, please see our\nIntroduction to Kerberos\n.\nTo verify the ticket was obtained, use\nklist\n, which should have output similar to the following:\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: Administrator@EXAMPLE.INTERNAL\n\nValid starting     Expires            Service principal\n07/24/24 13:52:33  07/24/24 23:52:33  krbtgt/EXAMPLE.INTERNAL@EXAMPLE.INTERNAL\n        renew until 07/25/24 13:52:02\nDNS tests\n¶\nUsing the Kerberos ticket from the step above, we can check the DNS server that Samba is running.\nIf everything is correct, the\nhostname\ncommand should be able to return both the short\nhostname\n, and the fully qualified hostname.\nFor the short hostname, use the command:\nhostname\nFor the fully qualified hostname, run this instead:\nhostname -f\nFor example,\nhostname\n-f\nwould return something like\nad.example.internal\n, while\nhostname\nreturns only\nad\n.\nServer Information\n¶\nWith that information at hand and verified, we can perform further checks. Let’s get information about the DNS service provided by this domain controller:\nsamba-tool dns serverinfo $(hostname -f)\nThis command will produce a long output, truncated below:\ndwVersion                   : 0xece0205\nfBootMethod                 : DNS_BOOT_METHOD_DIRECTORY\nfAdminConfigured            : FALSE\nfAllowUpdate                : TRUE\nfDsAvailable                : TRUE\npszServerName               : AD.example.internal\npszDsContainer              : CN=MicrosoftDNS,DC=DomainDnsZones,DC=example,DC=internal\n(...)\nEven though it doesn’t look like it, the\nsamba-tool\ndns\nserverinfo\ncommand used Kerberos authentication. The\nklist\ncommand output now shows another ticket that was obtained automatically:\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: Administrator@EXAMPLE.INTERNAL\n\nValid starting     Expires            Service principal\n07/24/24 14:29:55  07/25/24 00:29:55  krbtgt/EXAMPLE.INTERNAL@EXAMPLE.INTERNAL\n        renew until 07/25/24 14:29:53\n07/24/24 14:29:59  07/25/24 00:29:55  host/ad.example.internal@EXAMPLE.INTERNAL\n        renew until 07/25/24 14:29:53\nA ticket for the\nhost/ad.example.internal@EXAMPLE.INTERNAL\nprincipal is now also part of the ticket cache.\nDNS records\n¶\nThe DNS server backing the Samba Active Directory deployment also needs to have some specific DNS records in its zones, which are needed for service discoverability. Let’s check if they were added correctly with this simple script:\nfor srv in _ldap._tcp _kerberos._tcp _kerberos._udp _kpasswd._udp; do\n    echo -n \"${srv}.example.internal: \"\n    dig @localhost +short -t SRV ${srv}.example.internal\ndone\nThe output should have no errors, and show the DNS records for each query:\n_ldap._tcp.example.internal: 0 100 389 ad.example.internal.\n_kerberos._tcp.example.internal: 0 100 88 ad.example.internal.\n_kerberos._udp.example.internal: 0 100 88 ad.example.internal.\n_kpasswd._udp.example.internal: 0 100 464 ad.example.internal.\nAnd, of course, our own hostname must be in DNS as well:\ndig @localhost +short -t A $(hostname -f)\nThe above command should return your own IP address.\nUser creation test\n¶\nUsers (and groups, and other entities as well) can be created with the\nsamba-tool\ncommand. It can be used remotely, to manage users on a remote Samba AD server, or locally on the same server it is managing.\nWhen run\nlocally as root\n, no further authentication is required:\nsamba-tool user add noble\nThe command above will prompt for the desired password for the\nnoble\nuser, and if it satisfies the password complexity criteria, the user will be created. As with the\nAdministrator\nSamba user,\nkinit\nnoble\ncan be used to test that the\nnoble\nuser can be authenticated.\nNote\nsamba-tool\ncreates\nSamba\nusers, not local Linux users! These Samba users only exist for domain joined computers and other SMB connections/shares.\nThe default password policy is quite severe, requiring complex passwords. To display the current policy, run:\nsudo samba-tool domain passwordsettings show\nWhich will show the default password policy for the domain:\nPassword information for domain 'DC=example,DC=internal'\n\nPassword complexity: on\nStore plaintext passwords: off\nPassword history length: 24\nMinimum password length: 7\nMinimum password age (days): 1\nMaximum password age (days): 42\nAccount lockout duration (mins): 30\nAccount lockout threshold (attempts): 0\nReset account lockout after (mins): 30\nEach one of these can be changed, including the password complexity. For example, to set the minimum password length to 12:\nsudo samba-tool domain passwordsettings set --min-pwd-length=12\nTo see all the options, run:\nsamba-tool domain passwordsettings set --help\nNext steps\n¶\nThis Samba AD/DC server can be treated as an Active Directory server for Window and Linux systems. Typically next steps would be to create users and groups, and join member servers and workstations to this domain. Workstation users would login using the domain credentials, and member servers are used to provide file and print services.\nReferences\n¶\nActive Directory Domain Services Overview\nsamba-tool(8)\nmanual page\nActive Directory integration:\nChoosing an integration method\nJoining a Member Server\nJoining Workstations (without Samba services)", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:28Z", "original_len_words": 1728}}
{"id": "1591ee695b", "source_url": "https://documentation.ubuntu.com/server/how-to/samba/share-access-controls/", "title": "Share access controls - Ubuntu Server documentation", "text": "Share access controls - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nShare access controls\n¶\nThere are several options available to control access for each individual shared directory. Using the\n[share]\nexample, this section will cover some common options.\nSince most of these options will deal with user authentication, we first need to briefly address how that is done in Samba.\nAuthenticating Samba users\n¶\nSamba cannot authenticate existing Linux users using the its native protocols. This is just not compatible with the way Linux passwords are stored in the system (in\n/etc/shadow\n, for example). All local Linux users that the system may have are not automatically available as Samba users. To have a local Linux user available as a Samba user, they need to be created in the Samba credentials database. That means we will have two user databases: the Linux one, and the Samba one.\nSee also\nHow to create and manage Linux users is covered in\nUsers and groups management\n.\nTo add an existing Linux user to the Samba user database, the command\nsmbpasswd\nis used. For example, here we are adding an existing Linux user called\nmelissa\nto the Samba user database:\nsudo smbpasswd -a melissa\nThe command will prompt for a password twice, for confirmation, and create the Samba user.\nNote\nAs this is a separate user database, the password selected for the Samba user does not need to be the same as the Linux password for that user. In fact, most Samba servers setup this way will have the Linux users setup without a valid password: these users only exist so that the corresponding Samba users can be created.\nIf this user does not exist in Linux, the\nsmbpasswd\ncommand will fail. Samba users must first exist as Linux users.\nTo later change the password of an existing Samba user, the same command is used. For example:\nsudo smbpasswd melissa\nSamba also has the concept of Samba groups, but since there is no authentication going on, there is no need to create a separate group database just for Samba groups. We can use the normal Linux group, as long as the group members (the users) exist both in the Linux and Samba user database.\nGroups\n¶\nGroups\ndefine a collection of users who have a common level of access to particular network resources. This provides granularity in controlling access to such resources. For example, let’s consider a group called “qa” is defined to contain the users\nFreda\n,\nDanika\n, and\nRob\n,  and then a group called “support” is created containing the users\nDanika\n,\nJeremy\n, and\nVincent\n. Any network resources configured to allow access by the “qa” group will be available to Freda, Danika, and Rob, but not Jeremy or Vincent. Danika can access resources available to both groups since she belongs to both the “qa” and “support” groups. All other users only have access to resources explicitly allowed to the group they are part of.\nWhen mentioning groups in the Samba configuration file,\n/etc/samba/smb.conf\n, the recognized syntax is to preface the group name with an “@” symbol. For example, if you wished to use a group named\nsysadmin\nin a certain section of the\n/etc/samba/smb.conf\n, you would do so by entering the group name as\n@sysadmin\n. If a group name has a space in it, use double quotes, like\n\"@LTS\nReleases\"\n.\nRead and write permissions\n¶\nRead and write permissions define the explicit rights a computer or user has to a particular share. Such permissions may be defined by editing the\n/etc/samba/smb.conf\nfile and specifying the explicit permissions inside a share.\nFor example, if you have defined a Samba share called\nshare\nand wish to give read-only permissions to the group of users known as “qa”, but wanted to allow writing to the share by the group called “sysadmin”\nand\nthe user named “vincent”, then you could edit the\n/etc/samba/smb.conf\nfile and add the following entries under the\n[share]\nentry:\nread list = @qa\nwrite list = @sysadmin, vincent\nAnother possible Samba permission is to declare\nadministrative\npermissions to a particular shared resource. Users having administrative permissions may read, write, or modify any information contained in the resource the user has been given explicit administrative permissions to.\nFor example, if you wanted to give the user\nMelissa\nadministrative permissions to the\nshare\nexample, you would edit the\n/etc/samba/smb.conf\nfile, and add the following line under the\n[share]\nentry:\nadmin users = melissa\nNote\nRemember that the users listed in\nsmb.conf\nfor these access controls need to exist both as Linux users, and Samba users.\nAfter editing\n/etc/samba/smb.conf\n, reload Samba for the changes to take effect by running the following command:\nsudo\nsmbcontrol\nsmbd\nreload-config\nFilesystem permissions\n¶\nNow that Samba has been configured to limit which groups have access to the shared directory, the\nfilesystem\npermissions need to be checked.\nTraditional Linux file permissions do not map well to Windows NT Access Control Lists (\nACL\ns). Fortunately POSIX ACLs are available on Ubuntu servers, which provides more fine-grained control. For example, to enable ACLs on\n/srv\nin an EXT3 filesystem, edit\n/etc/fstab\nand add the\nacl\noption:\nUUID=66bcdd2e-8861-4fb0-b7e4-e61c569fe17d /srv  ext3    noatime,relatime,acl 0       1\nThen remount the partition:\nsudo\nmount\n-v\n-o\nremount\n/srv\nNote\nThis example assumes\n/srv\nis on a separate partition. If\n/srv\n, or wherever you have configured your share path, is part of the\n/\npartition then a reboot may be required.\nTo match the Samba configuration above, the “sysadmin” group will be given read, write, and execute permissions to\n/srv/samba/share\n, the “qa” group will be given read and execute permissions, and the files will be owned by the username “Melissa”. Enter the following in a terminal:\nsudo\nchown\n-R\nmelissa\n/srv/samba/share/\nsudo\nchgrp\n-R\nsysadmin\n/srv/samba/share/\nsudo\nsetfacl\n-R\n-m\ng:qa:rx\n/srv/samba/share/\nNote\nThe\nsetfacl\ncommand above gives\nexecute\npermissions to all files in the\n/srv/samba/share\ndirectory, which you may or may not want.\nNow from a Windows client you should notice the new file permissions are implemented. See the\nacl(5)\nand\nsetfacl(1)\nmanual pages for more information on POSIX ACLs.\nFurther reading\n¶\nFor in-depth Samba configurations see the\nSamba HOWTO Collection\n.\nThe guide is also available in\nprinted format\n.\nO’Reilly’s\nUsing Samba\nis also a good reference.\nChapter 18\nof the Samba HOWTO Collection is devoted to security.\nFor more information on Samba and ACLs see the\nSamba ACLs page\n.\nThe\nUbuntu Wiki Samba\npage.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:29Z", "original_len_words": 1107}}
{"id": "0ece89b903", "source_url": "https://documentation.ubuntu.com/server/how-to/security/", "title": "Security - Ubuntu Server documentation", "text": "Security - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nSecurity\n¶\nWhile a fresh Ubuntu installation is usually safe for immediate use, there are some additional steps you can take to introduce a layered approach to your system’s security. If you are new to Ubuntu, you may want to refer to our\nIntroduction to security\nfirst for a general overview.\nGeneral configuration\n¶\nUsers and groups management\nfor setting up user accounts, permissions and password policies\nFirewalls\nare recommended for network security\nAppArmor\nlimits permissions and access for the software running on your system\nConsole security\nfor an additional physical security barrier\nAuthentication\n¶\nThese tools are particularly useful for more advanced or complex setups.\nKerberos\nis a network authentication protocol providing identity verification for distributed systems\nNetwork user authentication with SSSD\nhandles authentication, user/group information and authorisation from disparate network sources\nSmart card authentication\nprovides a physical authentication method\nCryptography\n¶\nThe Secure Shell (SSH) cryptographic protocol that provides secure channels on an unsecured network. In Ubuntu, OpenSSH is the most commonly used implementation of SSH. It provides a suite of utilities for encrypting data transfers and can also be used for remote login and authentication.\nOpenSSH\nOpenSSH server\n2FA with TOTP/HOTP\n2FA with U2F/FIDO\nInstall a root CA certificate\nVirtual Private Network (VPN)\n¶\nVPNs are commonly used to provide encrypted, secure access to a network. Two of the most popular choices in Ubuntu are OpenVPN and WireGuard VPN.\nOpenVPN\nis a well-established option that supports many platforms besides Linux\nWireGuard VPN\nis a modern and performant option that removes a lot of the complexity from configuring a VPN\nSee also\n¶\nExplanation:\nIntroduction to security\nExplanation:\nSecurity topics", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:29Z", "original_len_words": 303}}
{"id": "4268d9de49", "source_url": "https://documentation.ubuntu.com/server/how-to/software/", "title": "Managing software - Ubuntu Server documentation", "text": "Managing software - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nManaging software\n¶\nPackage management\nshows you how to manage the software on your system using APT, dpkg, and other package managers\nUpdates\n¶\nAutomatic updates\nshows you how to configure (or turn off) automatic updates\nUpgrade your release\nshows you how to upgrade from one Ubuntu release to the next one\nSnapshot service\nshows you how to use the Ubuntu Snapshot Service to update packages to time-specific archive states.\nTroubleshooting\n¶\nReporting bugs\nshows you how to report a bug using the Apport utility\nKernel crash dump\nshows how to use the kernel crash dump utility\nSee also\n¶\nExplanation:\nManaging software", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:29Z", "original_len_words": 135}}
{"id": "518ede3019", "source_url": "https://documentation.ubuntu.com/server/how-to/software/automatic-updates/", "title": "Automatic updates - Ubuntu Server documentation", "text": "Automatic updates - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nAutomatic updates\n¶\nUbuntu will apply security updates automatically, without user interaction. This is done via the\nunattended-upgrades\npackage, which is installed by default.\nBut as the name suggests, it can apply other types of updates, and with interesting options alongside. For example:\nIt can reboot the system, if an update warrants it.\nIt can apply other types of updates, not just security.\nIt can block certain updates from ever being applied automatically.\nAnd more. Let’s explore some of these options.\nImportant\nJust adding another package repository to an Ubuntu system WILL NOT make\nunattended-upgrades\nconsider it for updates! This is explained in\nwhere to pick updates from\nlater in this document.\nConfiguration layout\n¶\nIf for some reason the package is not present, it can be installed with the following command:\nsudo\napt\ninstall\nunattended-upgrades\nImportant files and directories:\n/etc/apt/apt.conf.d/50unattended-upgrades\n: This file contains the options that control the behavior of the tool, such as if a reboot should be scheduled or not, or which packages are blocked from being upgraded.\n/etc/apt/apt.conf.d/20auto-upgrades\n: This file is used to control whether the unattended upgrade should be enabled or not, and how often it should run.\n/var/log/unattended-upgrades\n: This directory contains detailed logs of each unattended upgrade run.\nRight after installation, automatic installation of security updates will be enabled, including\nExpanded Security Maintenance (ESM)\nif that is available on the system. By default,\nunattended-upgrades\nruns once per day.\nEnabling and disabling unattended upgrades\n¶\nUnattended upgrades performs the equivalent of\napt\nupdate\nand\napt\nupgrade\n(see\nUpgrading packages\nfor details on these commands. First, it refreshes the package lists, to become aware of the new state of the package repositories. Then it checks which upgrades are available and applies them.\nThese two steps are controlled via the\nUpdate-Package-Lists\nand\nUnattended-Upgrade\noptions in\n/etc/apt/apt.conf.d/20auto-upgrades\n:\nAPT::Periodic::Update-Package-Lists \"1\";\nAPT::Periodic::Unattended-Upgrade \"1\";\nEach option accepts a time-based value, representing the number of days. A value of\n0\ndisables the action. The default value,\n1\n, executes the action daily. A value of\n2\nexecutes it every two days, and so forth.\nTherefore, to disable unattended upgrades, set these options to zero:\nAPT::Periodic::Update-Package-Lists \"0\";\nAPT::Periodic::Unattended-Upgrade \"0\";\nSystemd timer units,\napt-daily.timer\nand\napt-daily-upgrade.timer\n, trigger these actions at a scheduled time with a random delay. These timers activate services that execute the\n/usr/lib/apt/apt.systemd.daily\nscript.\nHowever, it may happen that if the machine is off at the time the timer unit elapses, the timer may be triggered immediately at the next startup (still subject to the\nRandomizedDelaySec\nvalue). As a result,\nunattended-upgrades\nmay often run on system startup and thereby cause immediate activity and prevent other package operations from taking place at that time. For example, if another package has to be installed, it would have to wait until the upgrades are completed.\nIn many cases this is beneficial, but in some cases it might be counter-productive; examples are administrators with many shut-down machines or VM images that are only started for some quick action, which is delayed or even blocked by the unattended upgrades. To change this behavior, we can change/override the configuration of both APT’s timer units\napt-daily-upgrade.timer\nand\napt-daily.timer\n. To do so, use\nsystemctl\nedit\n<timer_unit>\nand override the\nPersistent\nattribute setting it to\nfalse\n:\n[Timer]\nPersistent\n=\nfalse\nWith this change, the timer will trigger the service only on the next scheduled time. In other words, it won’t catch up to the run it missed while the system was off. See the explanation for the\nPersistent\noption in\nsystemd.timer(5)\nmanpage for more details.\nWhere to pick updates from\n¶\nIn\n/etc/apt/apt.conf.d/50unattended-upgrades\n, the\nAllowed-Origins\nsection specifies which repositories will be used to gather updates from. See the\nUbuntu Packaging Guide\nfor additional information about each official repository that Ubuntu uses.\nThis is the default:\nUnattended-Upgrade::Allowed-Origins {\n    \"${distro_id}:${distro_codename}\";\n    \"${distro_id}:${distro_codename}-security\";\n    // Extended Security Maintenance; doesn't necessarily exist for\n    // every release and this system may not have it installed, but if\n    // available, the policy for updates is such that unattended-upgrades\n    // should also install from here by default.\n    \"${distro_id}ESMApps:${distro_codename}-apps-security\";\n    \"${distro_id}ESM:${distro_codename}-infra-security\";\n//  \"${distro_id}:${distro_codename}-updates\";\n//  \"${distro_id}:${distro_codename}-proposed\";\n//  \"${distro_id}:${distro_codename}-backports\";\n};\nNote\nThe double “//” indicates a comment, so whatever follows “//” will not be evaluated.\nIf you want to also allow non-security updates to be applied automatically, then uncomment the line about\n-updates\n, like so:\nUnattended-Upgrade::Allowed-Origins {\n    \"${distro_id}:${distro_codename}\";\n    \"${distro_id}:${distro_codename}-security\";\n    // Extended Security Maintenance; doesn't necessarily exist for\n    // every release and this system may not have it installed, but if\n    // available, the policy for updates is such that unattended-upgrades\n    // should also install from here by default.\n    \"${distro_id}ESMApps:${distro_codename}-apps-security\";\n    \"${distro_id}ESM:${distro_codename}-infra-security\";\n    \"${distro_id}:${distro_codename}-updates\";\n//  \"${distro_id}:${distro_codename}-proposed\";\n//  \"${distro_id}:${distro_codename}-backports\";\n};\nThe\nOrigin\nfield is a standard field used in package repositories. By default,\nunattended-upgrades\nwill ship with only official Ubuntu repositories configured, which is the configuration shown above. To have the system apply upgrades automatically from other repositories, its\nOrigin\nneeds to be added to this configuration option.\nAutomatic upgrades from a PPA\n¶\nA very popular package repository type is a\nLaunchpad PPA\n. PPAs are normally referred to using the format\nppa:\\<user\\>/\\<name\\>\n. For example, the PPA at\nhttps://launchpad.net/~canonical-server/+archive/ubuntu/server-backports\nis also referred to as\nppa:canonical-server/server-backports\n.\nTo use a PPA in the\nAllowed-Origins\nconfiguration, we need its\nOrigin\nfield. For PPAs, it is in the format\nLP-PPA-<user>-<name>\n. Adding it to the\nAllowed-Origins\nconfiguration would result in the following (continuing from the example above):\nUnattended-Upgrade::Allowed-Origins {\n    \"${distro_id}:${distro_codename}\";\n    \"${distro_id}:${distro_codename}-security\";\n    // Extended Security Maintenance; doesn't necessarily exist for\n    // every release and this system may not have it installed, but if\n    // available, the policy for updates is such that unattended-upgrades\n    // should also install from here by default.\n    \"${distro_id}ESMApps:${distro_codename}-apps-security\";\n    \"${distro_id}ESM:${distro_codename}-infra-security\";\n    \"${distro_id}:${distro_codename}-updates\";\n//  \"${distro_id}:${distro_codename}-proposed\";\n//  \"${distro_id}:${distro_codename}-backports\";\n    \"LP-PPA-canonical-server-server-backports:${distro_codename}\";\n};\nDue to the hyphens acting as both separators and part of the name, the complete configuration can become visually confusing, making it difficult to immediately distinguish between the username and PPA name. But that’s ok, because it’s the whole text that matters.\nNow when the tool runs, that PPA will be considered for upgrades and is listed in\nAllowed origins\n:\n2025-03-13 22:44:29,802 INFO Starting unattended upgrades script\n2025-03-13 22:44:29,803 INFO Allowed origins are: o=Ubuntu,a=noble, o=Ubuntu,a=noble-security, o=UbuntuESMApps,a=noble-apps-security, o=UbuntuESM,a=noble-infra-security, o=LP-PPA-canonical-server-server-backports,a=noble\n2025-03-13 22:44:29,803 INFO Initial blacklist:\n2025-03-13 22:44:29,803 INFO Initial whitelist (not strict):\n2025-03-13 22:44:33,029 INFO Option --dry-run given, *not* performing real actions\n2025-03-13 22:44:33,029 INFO Packages that will be upgraded: ibverbs-providers libibverbs1 rdma-core\n2025-03-13 22:44:33,029 INFO Writing dpkg log to /var/log/unattended-upgrades/unattended-upgrades-dpkg.log\n2025-03-13 22:44:34,421 INFO All upgrades installed\n2025-03-13 22:44:34,855 INFO The list of kept packages can't be calculated in dry-run mode.\nThe correct\nOrigin\nvalue to use is available in the repository’s\nInRelease\n(or, for older formats, the\nRelease\nfile), which can be found at the URL of the repository, or locally on the system after an\napt\nupdate\ncommand was run. Locally these files are in the\n/var/lib/apt/lists/\ndirectory. For example, for the PPA case, we have:\n/var/lib/apt/lists/ppa.launchpadcontent.net_canonical-server_server-backports_ubuntu_dists_noble_InRelease\nWhich has contents:\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nOrigin: LP-PPA-canonical-server-server-backports\nLabel: Server Team Backports\nSuite: noble\nVersion: 24.04\nCodename: noble\nDate: Tue, 03 Dec 2024  6:00:43 UTC\nArchitectures: amd64 arm64 armhf i386 ppc64el riscv64 s390x\nComponents: main\nDescription: Ubuntu Noble 24.04\n(...)\nAnd there we can see the\nOrigin\n.\nHow to block certain packages\n¶\nSpecific packages can also be excluded from an update. This is controlled via the\nUnattended-Upgrade::Package-Blacklist\nconfiguration option in\n/etc/apt/apt.conf.d/50unattended-upgrades\n, which contains a list of\nPython Regular Expressions\n. Each line of this list is checked against the available package updates, and if there is a match, that package is not upgraded.\nNote\nKeep in mind that blocking a package might prevent other updates from being installed if they depend on the blocked package!\nFor example, this will block all packages that start with\nlinux-\nfrom being automatically upgraded:\nUnattended-Upgrade::Package-Blacklist {\n    \"linux-\";\n}\nA more specific configuration like the one below will block only the\nlibc6\nand\nlibc-bin\npackages from being automatically upgraded:\nUnattended-Upgrade::Package-Blacklist {\n    \"libc6$\";\n    \"libc-bin$\";\n}\nHere, the use of the\n$\ncharacter marks the end of the package name (in regular expression terms, it’s the end of the line, i.e., the end of the match).\nNote\nThe regular expressions used here behave as if the “\n^\n” character is present at the start, i.e., the\nlibc6$\nexpression will match\nlibc6\n, but will NOT match\nglibc6\nfor example.\nOf course, this being a regular expression means we could also write the above like this:\nUnattended-Upgrade::Package-Blacklist {\n    \"libc(6|-bin)$\";\n}\nJust be careful to not overuse the power of regular expressions: readability is key.\nNotifications\n¶\nBesides logging,\nunattended-upgrades\ncan also send out reports via email. There are two options that control this behavior in\n/etc/apt/apt.conf.d/50unattended-upgrades\n:\nUnattended-Upgrade::Mail\n\"user@example.com\";\n: If set to an email address, this option will trigger an email to this address containing an activity report. When this value is empty, or not set, (which is the default), no report is sent.\nUnattended-Upgrade::MailReport\n\"on-change\";\n: This option controls when a report is sent:\nalways\n: Always send a report, regardless if upgrades were applied or not.\nonly-on-error\n: Only send a report if there was an error.\non-change\n: Only send a report if upgrades were applied. This is the default value.\nNote\nSending out emails like this requires the separate configuration of a package like\nssmtp(8)\nor another minimalistic mail client that is capable of sending messages to a mail server.\nNotification examples\n¶\nHere are some email examples (lines wrapped for better legibility).\nNo changes applied, no errors\n¶\nThis would only be sent if\nUnattended-Upgrade::MailReport\nis set to\nalways\n:\nSubject:\nunattended-upgrades\nresult\nfor\n<hostname>:\nSUCCESS\n\nUnattended upgrade result: No packages found that can be upgraded\n unattended and no pending auto-removals\n\nUnattended-upgrades log:\nStarting unattended upgrades script\nAllowed origins are: o=Ubuntu,a=noble, o=Ubuntu,a=noble-security,\n o=UbuntuESMApps,a=noble-apps-security,\n o=UbuntuESM,a=noble-infra-security, o=Ubuntu,a=noble,\n o=Ubuntu,a=noble-security, o=UbuntuESMApps,a=noble-apps-security,\n o=UbuntuESM,a=noble-infra-security\nInitial blacklist:\nInitial whitelist (not strict):\nNo packages found that can be upgraded unattended and no pending auto-removals\nUpgrades applied, no errors\n¶\nThis is the default email report, when\nUnattended-Upgrade::MailReport\nis set to\non-change\n:\nSubject:\nunattended-upgrades\nresult\nfor\nnuc1:\nSUCCESS\n\nUnattended upgrade result: All upgrades installed\n\nPackages that were upgraded:\n linux-firmware\n\nPackage installation log:\nLog started: 2025-03-13  06:19:10\nPreparing to unpack\n .../linux-firmware_20240318.git3b128b60-0ubuntu2.10_amd64.deb ...\nUnpacking linux-firmware (20240318.git3b128b60-0ubuntu2.10) over\n (20240318.git3b128b60-0ubuntu2.9) ...\nSetting up linux-firmware (20240318.git3b128b60-0ubuntu2.10) ...\nProcessing triggers for initramfs-tools (0.142ubuntu25.5) ...\nupdate-initramfs: Generating /boot/initrd.img-6.8.0-55-generic\n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nLog ended: 2025-03-13  06:19:26\n\n\n\nUnattended-upgrades log:\nStarting unattended upgrades script\nAllowed origins are: o=Ubuntu,a=noble, o=Ubuntu,a=noble-security,\n o=UbuntuESMApps,a=noble-apps-security,\n o=UbuntuESM,a=noble-infra-security, o=Ubuntu,a=noble,\n o=Ubuntu,a=noble-security, o=UbuntuESMApps,a=noble-apps-security,\n o=UbuntuESM,a=noble-infra-security\nInitial blacklist:\nInitial whitelist (not strict):\nPackages that will be upgraded: linux-firmware\nWriting dpkg log to /var/log/unattended-upgrades/unattended-upgrades-dpkg.log\nAll upgrades installed\nReboots\n¶\nSometimes a system needs to be rebooted to fully apply an update. Such updates can use a mechanism in Ubuntu to let the system know that a reboot is recommended.\nunattended-upgrades\ncan benefit from this mechanism and optionally reboot the system automatically when needed.\nReboots can be very disruptive, especially if the system fails to come back. There are some configuration options where this behavior can be adjusted:\nUnattended-Upgrade::Automatic-Reboot\n\"false\";\n: If this option is set to\ntrue\n, the system will be rebooted\nwithout confirmation\nat the end of an upgrade run if a reboot was requested. The default value is\nfalse\n.\nUnattended-Upgrade::Automatic-Reboot-WithUsers\n\"true\";\n: Automatically reboot even if there are users currently logged in when\nUnattended-Upgrade::Automatic-Reboot\n(the option above) is set to\ntrue\n. The default value is\ntrue\n.\nUnattended-Upgrade::Automatic-Reboot-Time\n\"now\";\n: If automatic reboot is enabled and needed, reboot at the specific time instead of immediately. The time value is passed as-is to the\nshutdown(8)\ncommand. It can be the text “now” (which is the default), or in the format “hh:mm” (hours:minutes), or an offset in minutes specified like “+m”. Note that if using “hh:mm”, it will be in the local system’s timezone.\nNote\nFor more information about this time specification for the reboot, and other options like cancelling a scheduled reboot, see the\nshutdown(8)\nmanpage.\nBelow are the logs of an\nunattended-upgrades\nrun that started at 20:43. The tool installed the available upgrades and detected that a reboot was requested, which was scheduled using the configured\nAutomatic-Reboot-Time\n(20:45 in this example):\n2025-03-13 20:43:25,923 INFO Starting unattended upgrades script\n2025-03-13 20:43:25,924 INFO Allowed origins are: o=Ubuntu,a=noble, o=Ubuntu,a=noble-security, o=UbuntuESMApps,a=noble-apps-security, o=UbuntuESM,a=noble-infra-security\n2025-03-13 20:43:25,924 INFO Initial blacklist:\n2025-03-13 20:43:25,924 INFO Initial whitelist (not strict):\n2025-03-13 20:43:29,082 INFO Packages that will be upgraded: libc6 python3-jinja2\n2025-03-13 20:43:29,082 INFO Writing dpkg log to /var/log/unattended-upgrades/unattended-upgrades-dpkg.log\n2025-03-13 20:43:39,532 INFO All upgrades installed\n2025-03-13 20:43:40,201 WARNING Found /var/run/reboot-required, rebooting\n2025-03-13 20:43:40,207 WARNING Shutdown msg: b\"Reboot scheduled for Thu 2025-03-13 20:45:00 UTC, use 'shutdown -c' to cancel.\"\nWhen to consider disabling automatic updates\n¶\nWhile automatic security updates are enabled in Ubuntu by default, in some situations it might make sense to disable this feature, or carefully limit its reach.\nHere are some considerations.\nSystems which just get recreated\n¶\nSome systems are designed to be redeployed from a new base image rather than receive updates. This is common in cloud and container-based applications, where outdated instances are destroyed and replaced with newer ones. These systems are typically very lean, focused solely on running specific applications, and so may lack self-update tools.\nKeep in mind that the security exposure is still there: it’s only the update mechanism that is different, and comes in the form of a new deployment. The update still has to happen somewhere, it’s just not at runtime. Until that new deployment is done, outdated software might still be running.\nManual steps required\n¶\nWhile Ubuntu updates rarely require manual steps to complete an upgrade (at most a reboot can be required), it could be plausible that other applications require some manual steps after or before an update is applied. If that is the case, and if such steps cannot be safely automated, then maybe\nunattended-upgrades\nshould be disabled on such systems.\nDo consider block-listing such packages instead, if they are known to trigger such manual steps. In that case, the system can still benefit from all the other upgrades that might become available.\nToo much of a risk\n¶\nEven with all the care in the world, applying updates to a running system comes with risk. Ubuntu believes that risk to be less than the risk of NOT applying a security update, which is why\nunattended-upgrades\nwill apply security updates by default. But for some specific systems, the risk vs benefit equation might favor staying put and not applying an update unless specifically requested.\nAlways keep in mind, however, that specific packages can be blocked from receiving updates. For example, if a particular system runs a critical application that could break if certain libraries on the system are updated, then perhaps an acceptable compromise is to block these library packages from receiving upgrades, instead of disabling the whole feature.\nAs a middle-ground solution, you can configure\nunattended-upgrades\nto postpone impending updates to a later time. Read how to configure this feature in the\nPostponable updates\nsection.\nFleet management\n¶\nThe\nunattended-upgrades\nfeature is helpful, does its job, and even sends out reports. But it’s not intended to be a replacement for fleet management software. If a large number of Ubuntu systems needs to be kept updated, other solutions are better suited for the job. Such large deployments usually come with much stricter and wider requirements, like:\nCompliance reports: How many systems are up-do-date, how many are still behind, for how long has a system been exposed to a known vulnerability, etc.\nMaintenance windows: Different systems might require different maintenance windows. Some can be updated anytime, others only on weekends, etc.\nCanary rollouts: The ability to rollout updates to an initial group of systems, and over time increase the number of systems that will receive the update.\nAn example of such a Fleet Management software for Ubuntu systems is\nLandscape\n.\nPostponable updates\n¶\nBy default, system updates are applied automatically in the background without any user interaction.\nStarting with Ubuntu 25.04, a system administrator can allow users to postpone these automatic updates for a limited number of days by setting the\nUnattended-Update::Postpone-For-Days\noption.\nWhen this option is set,\nunattended-upgrade\nwill run according to the cadence set by the administrator and check for updates. If there are updates available it will notify active users and prompt them to choose if they want to upgrade immediately, or postpone them. For example, if\nUnattended-Upgrade::Postpone-For-Days\n\"3\"\nis set, then the user can postpone upgrades for up to three days. After that, the next time\nunattended-updates\nruns the user will not be prompted and the upgrades will be applied to the system.\nTo enable the feature, edit the\n/etc/apt/apt.conf.d/50unattended-upgrades\nfile and set the number of days that a user is allowed to postpone the automatic updates for. To postpone for up to\n3\ndays:\nUnattended-Upgrade::Postpone-For-Days \"3\";\nTo disable the feature, set the number of days to\n0\n.\nPrompt duration\n¶\nThe\nUnattended-Upgrade::Postpone-Wait-Time\nconfiguration option controls the amount of time (in seconds) that a user has available to send a postpone request after being prompted. If no postpone request is received within the specified time, the updates will start being applied as normal.\nWho can postpone\n¶\nThe system administrator can restrict access to the postpone request by defining Polkit rules for the\ncom.ubuntu.UnattendedUpgrade.Pending.Postpone\naction. By default, access is granted to users of an active session. See the\npolkit documentation\nfor how to set up authorization rules.\nNotifications in different environments\n¶\nThe prompting functionality is implemented graphically on Ubuntu Desktop by the\nupdate-notifier\nprogram. The user is shown a notification with the option to postpone the updates. Then, while updates are being applied an icon is visible in the system tray area informing the user so they know when it is safe to resume critical activities that may be affected by the updates.\nOn other environments, such as Ubuntu Server, you can implement your own prompting client by listening for the\nAboutToStart\nsignal on the system bus and send a call to the\nPostpone()\nmethod. Read the\n/usr/share/dbus-1/interfaces/com.ubuntu.UnattendedUpgrade.Pending.xml\ninterface specification for more details.\nTesting and troubleshooting\n¶\nIt’s possible to test some configuration changes to\nunattended-upgrade\nwithout having to wait for the next time it would run. The\nunattended-upgrade\ntool has a\nmanual page\nthat explains all its command-line options. Here are the most useful ones for testing and troubleshooting:\n-v\n: Show a more verbose output.\n--dry-run\n:  Just simulate what would happen, without actually making any changes.\nFor example, let’s say we want to check if the PPA origin was included correctly in the\nAllowed-Origins\nconfiguration, and if an update that we know is available would be considered.\nAfter we add\n\"LP-PPA-canonical-server-server-backports:${distro_codename}\";\nto\nAllowed-Origins\nin\n/etc/apt/apt.conf.d/50unattended-upgrades\n, we can run the tool in\nverbose\nand\ndry-run\nmodes to check what would happen:\nsudo\nunattended-upgrade\n-v\n--dry-run\nWhich produces the following output, in this example scenario:\nStarting unattended upgrades script\nAllowed origins are: o=Ubuntu,a=noble, o=Ubuntu,a=noble-security, o=UbuntuESMApps,a=noble-apps-security, o=UbuntuESM,a=noble-infra-security, o=LP-PPA-canonical-server-server-backports,a=noble\nInitial blacklist:\nInitial whitelist (not strict):\nOption --dry-run given, *not* performing real actions\nPackages that will be upgraded: rdma-core\nWriting dpkg log to /var/log/unattended-upgrades/unattended-upgrades-dpkg.log\n/usr/bin/unattended-upgrade:567: DeprecationWarning: This process (pid=1213) is multi-threaded, use of fork() may lead to deadlocks in the child.\n  pid = os.fork()\n/usr/bin/dpkg --status-fd 10 --no-triggers --unpack --auto-deconfigure /var/cache/apt/archives/rdma-core_52.0-2ubuntu1~backport24.04.202410192216~ubuntu24.04.1_amd64.deb\n/usr/bin/dpkg --status-fd 10 --configure --pending\nAll upgrades installed\nThe list of kept packages can't be calculated in dry-run mode.\nOf note, we see:\nAllowed\norigins\ninclude\no=LP-PPA-canonical-server-server-backports,a=noble\n, which is the PPA we included.\nThe\nrdma-core\npackage would be updated.\nLet’s check this\nrdma-core\npackage with the command\napt-cache\npolicy\nrdma-core\n:\nrdma-core:\n  Installed: 50.0-2build2\n  Candidate: 52.0-2ubuntu1~backport24.04.202410192216~ubuntu24.04.1\n  Version table:\n     52.0-2ubuntu1~backport24.04.202410192216~ubuntu24.04.1 500\n        500 https://ppa.launchpadcontent.net/canonical-server/server-backports/ubuntu noble/main amd64 Packages\n *** 50.0-2build2 500\n        500 http://br.archive.ubuntu.com/ubuntu noble/main amd64 Packages\n        100 /var/lib/dpkg/status\nAnd indeed, there is an update available from that PPA, and the next time\nunattended-upgrade\nruns on its own, it will apply that update. In fact, if the\n--dry-run\noption is removed from the command-line we just ran, the update will be installed.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:30Z", "original_len_words": 3389}}
{"id": "015050465e", "source_url": "https://documentation.ubuntu.com/server/how-to/software/kernel-crash-dump/", "title": "Kernel crash dump - Ubuntu Server documentation", "text": "Kernel crash dump - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nKernel crash dump\n¶\nA ‘kernel crash dump’ refers to a portion of the contents of volatile memory (RAM) that is copied to disk whenever the execution of the kernel is disrupted. The following events can cause a kernel disruption:\nKernel panic\nNon-maskable interrupts (NMI)\nMachine check exceptions (MCE)\nHardware failure\nManual intervention\nFor some of these events (kernel panic, NMI) the kernel will react automatically and trigger the crash dump mechanism through\nkexec\n. In other situations a manual intervention is required in order to capture the memory. Whenever one of the above events occurs, it is important to find out the root cause in order to prevent it from happening again. The cause can be determined by inspecting the copied memory contents.\nKernel crash dump mechanism\n¶\nWhen a kernel panic occurs, the kernel relies on the\nkexec\nmechanism to quickly reboot a new instance of the kernel in a pre-reserved section of memory that had been allocated when the system booted (see below). This permits the existing memory area to remain untouched in order to safely copy its contents to storage.\nKDump enabled by default\n¶\nStarting in Oracular Oriole (24.10) the kernel crash dump facility will be enabled by default during standard Ubuntu Desktop or Ubuntu Server installations on systems that meet the following requirements:\nthe system has at least 4 CPU threads\nthe system has at least 6GB of RAM, and less than 2TB of RAM\nthe free space available in\n/var\nis more than 5 times the amount of RAM and swap space\nand the CPU architecture is\namd64 or s390x, or\narm64 and UEFI is used\nOn machines with it enabled (either by default or by manual installation), it can be disabled via the command:\nsudo\napt\nremove\nkdump-tools\nOn machines that do not meet these requirements and on pre-24.10 releases, the kernel crash dump facility can be enabled manually by following the installation instructions that follow.\nInstallation\n¶\nThe kernel crash dump utility is installed with the following command:\nsudo\napt\ninstall\nkdump-tools\nNote\nStarting with 16.04, the kernel crash dump mechanism is enabled by default.\nDuring the installation, you will be prompted with the following dialog:\n|------------------------| Configuring kdump-tools |------------------------|\n |                                                                           |\n |                                                                           |\n | If you choose this option, the kdump-tools mechanism will be enabled.  A  |\n | reboot is still required in order to enable the crashkernel kernel        |\n | parameter.                                                                |\n |                                                                           |\n | Should kdump-tools be enabled be default?                                 |\n |                                                                           |\n |                    <Yes>                       <No>                       |\n |                                                                           |\n |---------------------------------------------------------------------------|\n‘Yes’ should be selected to enable\nkdump-tools\n.  If you want to revisit your choice, you can use the\ndpkg-reconfigure\nkdump-tools\ncommand and answer ‘Yes’ or ‘No’.\nAs well, you will also need to edit\n/etc/default/kdump-tools\nto enable\nkdump\nby including the following line:\nUSE_KDUMP=1\nIf you’re disabling\nkdump-tools\n, either set USE_KDUMP=0 or remove the line from the file.\nIf a reboot has not been done since installation of the\nlinux-crashdump\npackage, a reboot will be required in order to activate the\ncrashkernel=\nboot\nparameter. Upon reboot,\nkdump-tools\nwill be enabled and active.\nIf you enable\nkdump-tools\nafter a reboot, you will only need to issue the\nkdump-config\nload\ncommand to activate the\nkdump\nmechanism.\nYou can view the current status of\nkdump\nvia the command\nkdump-config\nshow\n.  This will display something like this:\nDUMP_MODE:        kdump\nUSE_KDUMP:        1\nKDUMP_SYSCTL:     kernel.panic_on_oops=1\nKDUMP_COREDIR:    /var/crash\ncrashkernel addr: \n   /var/lib/kdump/vmlinuz\nkdump initrd: \n   /var/lib/kdump/initrd.img\ncurrent state:    ready to kdump\nkexec command:\n  /sbin/kexec -p --command-line=\"...\" --initrd=...\nThis tells us that we will find core dumps in\n/var/crash\n.\nConfiguration\n¶\nIn addition to local dump, it is now possible to use the remote dump functionality to send the kernel crash dump to a remote server, using either the SSH or NFS protocols.\nLocal kernel crash dumps\n¶\nLocal dumps are configured automatically and will remain in use unless a remote protocol is chosen. Many configuration options exist and are thoroughly documented in the\n/etc/default/kdump-tools\nfile.\nRemote kernel crash dumps using the SSH protocol\n¶\nTo enable remote dumps using the SSH protocol, the\n/etc/default/kdump-tools\nmust be modified in the following manner:\n# ---------------------------------------------------------------------------\n# Remote dump facilities:\n# SSH - username and hostname of the remote server that will receive the dump\n#       and dmesg files.\n# SSH_KEY - Full path of the ssh private key to be used to login to the remote\n#           server. use kdump-config propagate to send the public key to the\n#           remote server\n# HOSTTAG - Select if hostname of IP address will be used as a prefix to the\n#           timestamped directory when sending files to the remote server.\n#           'ip' is the default.\nSSH=\"ubuntu@kdump-netcrash\"\nThe only mandatory variable to define is SSH. It must contain the username and\nhostname\nof the remote server using the format\n{username}@{remote\nserver}\n.\nSSH_KEY\nmay be used to provide an existing private key to be used. Otherwise, the\nkdump-config\npropagate\ncommand will create a new keypair. The\nHOSTTAG\nvariable may be used to use the hostname of the system as a prefix to the remote directory to be created instead of the IP address.\nThe following example shows how\nkdump-config\npropagate\nis used to create and propagate a new keypair to the remote server:\nsudo\nkdump-config\npropagate\nWhich produces an output like this:\nNeed to generate a new ssh key...\nThe authenticity of host 'kdump-netcrash (192.168.1.74)' can't be established.\nECDSA key fingerprint is SHA256:iMp+5Y28qhbd+tevFCWrEXykDd4dI3yN4OVlu3CBBQ4.\nAre you sure you want to continue connecting (yes/no)? yes\nubuntu@kdump-netcrash's password: \npropagated ssh key /root/.ssh/kdump_id_rsa to server ubuntu@kdump-netcrash\nThe password of the account used on the remote server will be required in order to successfully send the public key to the server.\nThe\nkdump-config\nshow\ncommand can be used to confirm that\nkdump\nis correctly configured to use the SSH protocol:\nkdump-config\nshow\nWhose output appears like this:\nDUMP_MODE:        kdump\nUSE_KDUMP:        1\nKDUMP_SYSCTL:     kernel.panic_on_oops=1\nKDUMP_COREDIR:    /var/crash\ncrashkernel addr: 0x2c000000\n   /var/lib/kdump/vmlinuz: symbolic link to /boot/vmlinuz-4.4.0-10-generic\nkdump initrd: \n   /var/lib/kdump/initrd.img: symbolic link to /var/lib/kdump/initrd.img-4.4.0-10-generic\nSSH:              ubuntu@kdump-netcrash\nSSH_KEY:          /root/.ssh/kdump_id_rsa\nHOSTTAG:          ip\ncurrent state:    ready to kdump\nRemote kernel crash dumps using the NFS protocol\n¶\nTo enable remote dumps using the NFS protocol, the\n/etc/default/kdump-tools\nmust be modified in the following manner:\n# NFS -     Hostname and mount point of the NFS server configured to receive\n#           the crash dump. The syntax must be {HOSTNAME}:{MOUNTPOINT} \n#           (e.g. remote:/var/crash)\n#\nNFS=\"kdump-netcrash:/var/crash\"\nAs with the SSH protocol, the\nHOSTTAG\nvariable can be used to replace the IP address by the hostname as the prefix of the remote directory.\nThe\nkdump-config\nshow\ncommand can be used to confirm that\nkdump\nis correctly configured to use the NFS protocol :\nkdump-config\nshow\nWhich produces an output like this:\nDUMP_MODE:        kdump\nUSE_KDUMP:        1\nKDUMP_SYSCTL:     kernel.panic_on_oops=1\nKDUMP_COREDIR:    /var/crash\ncrashkernel addr: 0x2c000000\n   /var/lib/kdump/vmlinuz: symbolic link to /boot/vmlinuz-4.4.0-10-generic\nkdump initrd: \n   /var/lib/kdump/initrd.img: symbolic link to /var/lib/kdump/initrd.img-4.4.0-10-generic\nNFS:              kdump-netcrash:/var/crash\nHOSTTAG:          hostname\ncurrent state:    ready to kdump\nVerification\n¶\nTo confirm that the kernel dump mechanism is enabled, there are a few things to verify. First, confirm that the\ncrashkernel\nboot parameter is present (note that the following line has been split into two to fit the format of this document):\ncat\n/proc/cmdline\nBOOT_IMAGE\n=\n/vmlinuz-3.2.0-17-server\nroot\n=\n/dev/mapper/PreciseS-root\nro\ncrashkernel\n=\n384M-2G:64M,2G-:128M\nThe\ncrashkernel\nparameter has the following syntax:\ncrashkernel=<range1>:<size1>[,<range2>:<size2>,...][@offset]\n    range=start-[end] 'start' is inclusive and 'end' is exclusive.\nSo for the\ncrashkernel\nparameter found in\n/proc/cmdline\nwe would have :\ncrashkernel\n=\n384M-2G:64M,2G-:128M\nThe above value means:\nif the RAM is smaller than 384M, then don’t reserve anything (this is the “rescue” case)\nif the RAM size is between 386M and 2G (exclusive), then reserve 64M\nif the RAM size is larger than 2G, then reserve 128M\nSecond, verify that the kernel has reserved the requested memory area for the\nkdump\nkernel by running:\ndmesg\n|\ngrep\n-i\ncrash\nWhich produces the following output in this case:\n...\n[\n0\n.000000\n]\nReserving\n64MB\nof\nmemory\nat\n800MB\nfor\ncrashkernel\n(\nSystem\nRAM:\n1023MB\n)\nFinally, as seen previously, the\nkdump-config\nshow\ncommand displays the current status of the\nkdump-tools\nconfiguration :\nkdump-config\nshow\nWhich produces:\nDUMP_MODE:        kdump\nUSE_KDUMP:        1\nKDUMP_SYSCTL:     kernel.panic_on_oops=1\nKDUMP_COREDIR:    /var/crash\ncrashkernel addr: 0x2c000000\n   /var/lib/kdump/vmlinuz: symbolic link to /boot/vmlinuz-4.4.0-10-generic\nkdump initrd: \n      /var/lib/kdump/initrd.img: symbolic link to /var/lib/kdump/initrd.img-4.4.0-10-generic\ncurrent state:    ready to kdump\n\nkexec command:\n      /sbin/kexec -p --command-line=\"BOOT_IMAGE=/vmlinuz-4.4.0-10-generic root=/dev/mapper/VividS--vg-root ro debug break=init console=ttyS0,115200 irqpoll maxcpus=1 nousb systemd.unit=kdump-tools.service\" --initrd=/var/lib/kdump/initrd.img /var/lib/kdump/vmlinuz\nTesting the crash dump mechanism\n¶\nWarning\nTesting the crash dump mechanism\nwill cause a system reboot\n. In certain situations, this can cause data loss if the system is under heavy load. If you want to test the mechanism, make sure that the system is idle or under very light load.\nVerify that the\nSysRQ\nmechanism is enabled by looking at the value of the\n/proc/sys/kernel/sysrq\nkernel parameter:\ncat\n/proc/sys/kernel/sysrq\nIf a value of\n0\nis returned, the dump and then reboot feature is disabled. A value greater than\n1\nindicates that a sub-set of\nsysrq\nfeatures is enabled. See\n/etc/sysctl.d/10-magic-sysrq.conf\nfor a detailed description of the options and their default values. Enable dump then reboot testing with the following command:\nsudo\nsysctl\n-w\nkernel.sysrq\n=\n1\nOnce this is done, you must become root, as just using\nsudo\nwill not be sufficient. As the\nroot\nuser, you will have to issue the command\necho\nc\n>\n/proc/sysrq-trigger\n. If you are using a network connection, you will lose contact with the system. This is why it is better to do the test while being connected to the system console. This has the advantage of making the kernel dump process visible.\nA typical test output should look like the following :\nsudo -s\n[sudo] password for ubuntu: \n# echo c > /proc/sysrq-trigger\n[   31.659002] SysRq : Trigger a crash\n[   31.659749] BUG: unable to handle kernel NULL pointer dereference at           (null)\n[   31.662668] IP: [<ffffffff8139f166>] sysrq_handle_crash+0x16/0x20\n[   31.662668] PGD 3bfb9067 PUD 368a7067 PMD 0 \n[   31.662668] Oops: 0002 [#1] SMP \n[   31.662668] CPU 1 \n....\nThe rest of the output is truncated, but you should see the system rebooting and somewhere in the log, you will see the following line :\nBegin: Saving vmcore from kernel crash ...\nOnce completed, the system will reboot to its normal operational mode. You will then find the kernel crash dump file, and related subdirectories, in the\n/var/crash\ndirectory by running, e.g.\nls\n/var/crash\n, which produces the following:\n201809240744\nkexec_cmd\nlinux-image-4.15.0-34-generic-201809240744.crash\nIf the dump does not work due to an ‘out of memory’ (OOM) error, then try increasing the amount of reserved memory by editing\n/etc/default/grub.d/kdump-tools.cfg\n. For example, to reserve 512 megabytes:\nGRUB_CMDLINE_LINUX_DEFAULT=\"$GRUB_CMDLINE_LINUX_DEFAULT crashkernel=384M-:512M\"\nYou can then run\nsudo\nupdate-grub\n, reboot afterwards, and then test again.\nResources\n¶\nKernel crash dump is a vast topic that requires good knowledge of the Linux kernel. You can find more information on the topic here:\nKdump kernel documentation\n.\nAnalyzing Linux Kernel Crash\n(Based on Fedora, it still gives a good walkthrough of kernel dump analysis)", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:30Z", "original_len_words": 1858}}
{"id": "6d0c8ed6eb", "source_url": "https://documentation.ubuntu.com/server/how-to/software/package-management/", "title": "Install and manage packages - Ubuntu Server documentation", "text": "Install and manage packages - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nInstall and manage packages\n¶\nThe recommended way to install Debian packages (“deb” files) is using the\nAdvanced Packaging Tool (APT), which can be used on the command line using the\napt\nutility.\nThe commands contained within\napt\nprovide the means to install new software\npackages, upgrade existing software packages, update the package list index,\nand even upgrade the entire Ubuntu system.\nUpdate the package index\n¶\nThe APT package index is a database of available packages from the\nrepositories defined in the\n/etc/apt/sources.list\nfile and in the\n/etc/apt/sources.list.d\ndirectory. To update the local package index with\nthe latest changes made in the repositories, and thereby access the most\nup-to-date version of the package you’re interested in, type the following:\nsudo\napt\nupdate\nInstall a package\n¶\nAs an example, to install the\nnmap\nnetwork scanner, run the following command:\nsudo\napt\ninstall\nnmap\nTip\nYou can install or remove multiple packages at once by separating them with\nspaces.\nRemove a package\n¶\nTo remove the package installed in the previous example, run the following:\nsudo\napt\nremove\nnmap\nAdding the\n--purge\noption to\napt\nremove\nwill remove the package\nconfiguration files as well. This may or may not be what you want, so use it\nwith caution.\nNote\nWhile\napt\nis a command-line tool, it is intended to be used interactively,\nand not to be called from non-interactive scripts. The\napt-get\ncommand should\nbe used in scripts (perhaps with the\n--quiet\nflag). For basic commands the\nsyntax of the two tools is identical.\nUpgrading packages\n¶\nInstalled packages on your computer may periodically have upgrades available from the package repositories (e.g., security updates). To upgrade your system, first update your package index and then perform the upgrade – as follows:\nsudo\napt\nupdate\nsudo\napt\nupgrade\nFor details on how to upgrade to a new Ubuntu release, see our\nguide on upgrading releases\n. For further information about using APT, read the comprehensive\nAPT User’s Guide\n, or type\napt\nhelp\n.\nAptitude\n¶\nLaunching Aptitude with no command-line options will give you a menu-driven, text-based\nfrontend\nto the APT system. Many of the common package management functions, such as installation, removal, and upgrade, can be performed in Aptitude with single-key commands, which are typically lowercase letters.\nAptitude is best suited for use in a non-graphical terminal environment to ensure the command keys work properly. You can start the menu-driven interface of Aptitude as a regular user by typing the following command at a terminal prompt:\nsudo\naptitude\nWhen Aptitude starts, you will see a menu bar at the top of the screen and two panes below the menu bar. The top pane contains package categories, such as “New Packages” and “Not Installed Packages”. The bottom pane contains information related to the packages and package categories.\nUsing Aptitude for package management is relatively straightforward thanks to its user interface. The following are examples of common package management functions as performed in Aptitude:\nInstalling packages\n¶\nTo install a package, locate it in the “Not Installed Packages” category by using the keyboard arrow keys and the\nEnter\nkey.\nHighlight the desired package, then press the\n+\nkey. The package entry should turn\ngreen\n, which indicates it has been marked for installation. Now press\ng\nto be presented with a summary of package actions. Press\ng\nagain, and the package will be downloaded and installed. When finished, press\nEnter\nto return to the menu.\nRemove Packages\n¶\nTo remove a package, locate it in the “Installed Packages” category by using the keyboard arrow keys and the\nEnter\nkey.\nHighlight the package you want to remove, then press the\n-\nkey. The package entry should turn\npink\n, indicating it has been marked for removal. Now press\ng\nto be presented with a summary of package actions. Press\ng\nagain, and the package will be removed. When finished, press\nEnter\nto return to the menu.\nUpdating the package index\n¶\nTo update the package index, press the\nu\nkey.\nUpgrade packages\n¶\nTo upgrade packages, first update the package index as detailed above, and then press the\nU\nkey to mark all packages with available updates. Now press\ng\n, which will present you with a summary of package actions. Press\ng\nagain to begin the download and installation. When finished, press\nEnter\nto return to the menu.\nThe first column of information displayed in the package list (in the top pane) lists the current state of the package (when viewing packages). It uses the following key to describe the package state:\ni\n: Installed package\nc\n: Package not installed, but package configuration remains on the system\np\n: Purged from system\nv\n: Virtual package\nB\n: Broken package\nu\n: Unpacked files, but package not yet configured\nC\n: Half-configured - configuration failed and requires fix\nH\n: Half-installed - removal failed and requires a fix\nTo exit Aptitude, simply press the\nq\nkey and confirm you want to exit. Many other functions are available from the Aptitude menu by pressing the\nF10\nkey.\nCommand-line Aptitude\n¶\nYou can also use Aptitude as a command-line tool, similar to\napt\n. To install the\nnmap\npackage with all necessary dependencies (as in the\napt\nexample), you would use the following command:\nsudo\naptitude\ninstall\nnmap\nTo remove the same package, you would use the command:\nsudo\naptitude\nremove\nnmap\nConsult the\nAptitude manpages\nfor full details of Aptitude’s command-line options.\ndpkg\n¶\ndpkg\nis a package manager for Debian-based systems. It can install, remove, and build packages, but unlike other package management systems, it cannot automatically download and install packages – or their dependencies.\nAPT and Aptitude are newer, and layer additional features on top of\ndpkg\n. This section covers using\ndpkg\nto manage locally installed packages.\nList packages\n¶\nTo list\nall\npackages in the system’s package database (both installed and uninstalled) run the following command from a terminal prompt:\ndpkg\n-l\nDepending on the number of packages on your system, this can generate a large amount of output. Pipe the output through\ngrep\nto see if a specific package is installed:\ndpkg\n-l\n|\ngrep\napache2\nReplace\napache2\nwith any package name, part of a package name, or a regular expression.\nList files\n¶\nTo list the files installed by a package, in this case the\nufw\npackage, enter:\ndpkg\n-L\nufw\nIf you are unsure which package installed a file,\ndpkg\n-S\nmay be able to tell you. For example:\ndpkg\n-S\n/etc/host.conf\nbase-files:\n/etc/host.conf\nThe output shows that the\n/etc/host.conf\nbelongs to the base-files package.\nNote\nMany files are automatically generated during the package install process, and even though they are on the\nfilesystem\n,\ndpkg\n-S\nmay not know which package they belong to.\nInstalling a deb file\n¶\nYou can install a local\n.deb\nfile by entering:\nsudo\ndpkg\n-i\nzip_3.0-4_amd64.deb\nChange\nzip_3.0-4_amd64.deb\nto the actual file name of the local\n.deb\nfile you wish to install.\nUninstalling packages\n¶\nYou can uninstall a package by running:\nsudo\ndpkg\n-r\nzip\nCaution\nUninstalling packages using\ndpkg\n, is\nNOT\nrecommended in most cases. It is better to use a package manager that handles dependencies to ensure that the system is left in a consistent state. For example, using\ndpkg\n-r\nzip\nwill remove the\nzip\npackage, but any packages that depend on it will still be installed and may no longer function correctly as a result.\nFor more\ndpkg\noptions see the\ndpkg(1)\nmanpage:\nman\ndpkg\n.\nAPT configuration\n¶\nConfiguration of the APT system repositories is stored in the\n/etc/apt/sources.list\nfile and the\n/etc/apt/sources.list.d\ndirectory. An example of this file is referenced here, along with information on adding or removing repository references from the file.\nYou can edit the file to enable and disable repositories. For example, to disable the requirement to insert the Ubuntu CD-ROM whenever package operations occur, simply comment out the appropriate line for the CD-ROM, which appears at the top of the file:\n# no more prompting for CD-ROM please\n# deb cdrom:[DISTRO-APT-CD-NAME - Release i386 (20111013.1)]/ DISTRO-SHORT-CODENAME main restricted\nAutomatic updates\n¶\nIt’s possible to setup an Ubuntu system with Automatic Updates, such that certain types of upgrades are applied automatically. In fact, the default for Ubuntu Server is to automatically apply security updates. Please see the\nAutomatic updates\nsection for details.\nExtra repositories\n¶\nIn addition to the officially-supported package repositories available for Ubuntu, there are also community-maintained repositories which add thousands more packages for potential installation. Two of the most popular are the\nUniverse\nand\nMultiverse\nrepositories. These repositories are not officially supported by Ubuntu, but because they are maintained by the community they generally provide packages which are safe for use with your Ubuntu computer.\nFor more information, see our guide on\nusing third-party repositories\n.\nWarning\nBe advised that packages in Universe and Multiverse are not officially supported and do not receive security patches, except through Ubuntu Pro’s\nExpanded Security Maintenance\n. A subscription to\nUbuntu Pro\nis free for personal use on up to five machines.\nPackages in the\nmultiverse\nrepository often have licensing issues that prevent them from being distributed with a free operating system, and they may be illegal in your locality.\nMany other package sources are available – sometimes even offering only one package, as in the case of packages provided by the developer of a single application. You should always be cautious when using non-standard package sources/repos, however. Research the packages and their origins carefully before performing any installation, as some packages could render your system unstable or non-functional in some respects.\nBy default, the\nuniverse\nand\nmultiverse\nrepositories are enabled. If you would like to disable them, edit\n/etc/apt/sources.list\nand comment out the following lines:\ndeb http://archive.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME universe multiverse\ndeb-src http://archive.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME universe multiverse\n    \ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME universe\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME universe\ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates universe\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates universe\n    \ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME multiverse\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME multiverse\ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates multiverse\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates multiverse\n    \ndeb http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security universe\ndeb-src http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security universe\ndeb http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security multiverse\ndeb-src http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security multiverse\nlogging\n¶\nActions of the\napt\ncommand, such as installation and removal of packages,\nare logged in the\n/var/log/dpkg.log\nlog file.\nFurther reading\n¶\nMost of the material covered in this chapter is available in the respective man pages, many of which are available online.\nThe\nInstalling Software\nUbuntu wiki page has more information.\nThe\nAPT User’s Guide\ncontains useful information regarding APT usage.\nFor more information about systemd timer units (and systemd in general), visit the\nsystemd(1)\nmanual page and\nsystemd.timer(5)\nmanual page\nSee the\nAptitude user’s manual\nfor more Aptitude options.\nThe\nAdding Repositories HOWTO (Ubuntu Wiki)\npage contains more details on adding repositories.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:30Z", "original_len_words": 1827}}
{"id": "43a24805d1", "source_url": "https://documentation.ubuntu.com/server/how-to/software/report-a-bug/", "title": "How to report a bug in Ubuntu Server - Ubuntu Server documentation", "text": "How to report a bug in Ubuntu Server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to report a bug in Ubuntu Server\n¶\nThe Ubuntu project, including Ubuntu Server,\nuses Launchpad\nas its bug tracker. To file a bug, you will first need to\ncreate a Launchpad account\n.\nReport bugs with\napport-cli\n¶\nThe preferred way to report a bug is with the\napport-cli\ncommand. This command collects information from the machine on which it is run and publishes it to the bug report on Launchpad.\nGetting this information to Launchpad can be a challenge if the system is not running a desktop environment with a browser (a common scenario with servers) or if it does not have Internet access. The steps to take in these situations are described below.\nNote\nThe commands\napport-cli\nand\nubuntu-bug\nshould give the same results on a command-line interface (CLI) server. The latter is actually a symlink to\napport-bug\n, which is intelligent enough to know whether a desktop environment is in use, and will choose\napport-cli\nif not. Since server systems tend to be CLI-only,\napport-cli\nwas chosen from the outset in this guide.\nBug reports in Ubuntu need to be filed against a specific software package, so the name of the package (source package or program name/path) affected by the bug needs to be supplied to\napport-cli\n:\napport-cli\nPACKAGENAME\nOnce\napport-cli\nhas finished gathering information you will be asked what to do with it. For instance, to report a bug against vim using\napport-cli\nvim\nproduces output like this:\n*** Collecting problem information\n    \nThe collected information can be sent to the developers to improve the\napplication. This might take a few minutes.\n...\n    \n*** Send problem report to the developers?\n    \nAfter the problem report has been sent, please fill out the form in the\nautomatically opened web browser.\n   \nWhat would you like to do? Your options are:\n  S: Send report (2.8 KB)\n  V: View report\n  K: Keep report file for sending later or copying to somewhere else\n  I: Cancel and ignore future crashes of this program version\n  C: Cancel\nPlease choose (S/V/K/I/C):\nThe first three options are described below.\nS: Send report\n¶\nSubmits the collected information to Launchpad as part of the process of filing a new bug report. You will be given the opportunity to describe the bug in your own words.\n*** Uploading problem information\n    \nThe collected information is being sent to the bug tracking system.\nThis might take a few minutes.\n94%\n    \n*** To continue, you must visit the following URL:\n    \n  https://bugs.launchpad.net/ubuntu/+source/vim/+filebug/09b2495a-e2ab-11e3-879b-68b5996a96c8?\n    \nYou can launch a browser now, or copy this URL into a browser on another computer.\n    \n    \nChoices:\n  1: Launch a browser now\n  C: Cancel\nPlease choose (1/C):  1\nThe browser that will be used when choosing ‘1’ will be the one known on the system as\nwww-browser\nvia the\nDebian alternatives system\n. Examples of text-based browsers to install include links,\nELinks\n, lynx, and w3m. You can also manually point an existing browser at the given URL.\nV: View\n¶\nThis displays the collected information on the screen for review. This can be a lot of information! Press\nEnter\nto scroll through the screens. Press\nq\nto quit and return to the choice menu.\nK: Keep\n¶\nThis writes the collected information to disk. The resulting file can be later used to file the bug report, typically after transferring it to another Ubuntu system.\nWhat would you like to do? Your options are:\n  S: Send report (2.8 KB)\n  V: View report\n  K: Keep report file for sending later or copying to somewhere else\n  I: Cancel and ignore future crashes of this program version\n  C: Cancel\nPlease choose (S/V/K/I/C): k\nProblem report file: /tmp/apport.vim.1pg92p02.apport\nTo report the bug, get the file onto an Internet-enabled Ubuntu system and apply\napport-cli\nto it. This will cause the menu to appear immediately (since the information is already collected). You should then press\ns\nto send:\napport-cli\napport.vim.1pg92p02.apport\nTo directly save a report to disk (without menus) you can run:\napport-cli\nvim\n--save\napport.vim.test.apport\nReport names should end in\n.apport\n.\nNote\nIf this Internet-enabled system is non-Ubuntu/Debian,\napport-cli\nis not available so the bug will need to be created manually. An\napport\nreport is also not to be included as an attachment to a bug either so it is completely useless in this scenario.\nReporting application crashes\n¶\nThe software package that provides the\napport-cli\nutility,\napport\n, can be configured to automatically capture the state of a crashed application. This is enabled by default in\n/etc/default/apport\n.\nAfter an application crashes, if enabled,\napport\nwill store a crash report under\n/var/crash\n:\n-rw-r----- 1 peter    whoopsie 150K Jul 24 16:17 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu-cached.1000.crash\nUse the\napport-cli\ncommand with no arguments to process any pending crash reports. It will offer to report them one by one, as in the following example:\napport-cli\n*** Send problem report to the developers?\n    \nAfter the problem report has been sent, please fill out the form in the\nautomatically opened web browser.\n    \nWhat would you like to do? Your options are:\n  S: Send report (153.0 KB)\n  V: View report\n  K: Keep report file for sending later or copying to somewhere else\n  I: Cancel and ignore future crashes of this program version\n  C: Cancel\nPlease choose (S/V/K/I/C): s\nIf you send the report, as was done above, the prompt will be returned immediately and the\n/var/crash\ndirectory will then contain 2 extra files:\n-rw-r----- 1 peter    whoopsie 150K Jul 24 16:17 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu-cached.1000.crash\n-rw-rw-r-- 1 peter    whoopsie    0 Jul 24 16:37 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu-cached.1000.upload\n-rw------- 1 whoopsie whoopsie    0 Jul 24 16:37 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu-cached.1000.uploaded\nSending in a crash report like this will not immediately result in the creation of a new public bug. The report will be made private on Launchpad, meaning that it will be visible to only a limited set of bug triagers. These triagers will then scan the report for possible private data before creating a public bug.\nFurther reading\n¶\nSee the\nReporting Bugs\nUbuntu wiki page.\nAlso,\nthe Apport page\nhas some useful information. Though some of it pertains to using a\nGUI\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:30Z", "original_len_words": 1044}}
{"id": "dbf9b2ea74", "source_url": "https://documentation.ubuntu.com/server/how-to/software/snapshot-service/", "title": "How to use the Ubuntu snapshot service - Ubuntu Server documentation", "text": "How to use the Ubuntu snapshot service - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to use the Ubuntu snapshot service\n¶\nThe Ubuntu snapshot service allows you to access old packages in the Ubuntu\narchive based on past dates. Some of the use cases for this service include:\nInstalling a superseded version of a package as it existed at a particular\ndate and time, which is useful to troubleshoot bugs or regressions, or to\nallow reproducibility.\nSetting upgrades to known states, which can be useful to ensure homogeneity\nfor a fleet of Ubuntu installations and to ensure predictability upon package\nupgrades (including unattended upgrades).\nSupport a structured update workflow by validating snapshots in different\nenvironments or by rolling out package upgrades in stages for a large set of\nmachines.\nHere we show how to set up and use the snapshot service.\nPrerequisites\n¶\nSnapshots are supported in Ubuntu 23.10 onwards and on updated installations of\nUbuntu 20.04 LTS (starting from\napt\n2.0.10) and Ubuntu 22.04 LTS (starting\nfrom\napt\n2.4.11).\nIn Ubuntu 24.04 LTS and later,\napt\nwill automatically detect whether a\nrepository supports snapshots. Therefore, there is no need for additional\nconfiguration before you can start using the Ubuntu snapshot service. For\nUbuntu versions lower than 24.04, you need to set a\nsnapshot\noption in the\nrelevant entries of the\n/etc/apt/sources.list\nfile. For instance:\nIf you have the following contents in the\n/etc/apt/sources.list\nfile\ndeb\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\nmain\nuniverse\nrestricted\nmultiverse\ndeb\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\n-\nupdates\nmain\nuniverse\nrestricted\nmultiverse\ndeb\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\n-\nbackports\nmain\nrestricted\nuniverse\nmultiverse\ndeb\nhttp\n:\n//\nsecurity\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\n-\nsecurity\nmain\nrestricted\nuniverse\nmultiverse\nyou can enable the snapshot service for\nall components in all\npockets\nby changing it to\ndeb\n[\nsnapshot\n=\nyes\n]\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\nmain\nuniverse\nrestricted\nmultiverse\ndeb\n[\nsnapshot\n=\nyes\n]\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\n-\nupdates\nmain\nuniverse\nrestricted\nmultiverse\ndeb\n[\nsnapshot\n=\nyes\n]\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\n-\nbackports\nmain\nrestricted\nuniverse\nmultiverse\ndeb\n[\nsnapshot\n=\nyes\n]\nhttp\n:\n//\nsecurity\n.\nubuntu\n.\ncom\n/\nubuntu\njammy\n-\nsecurity\nmain\nrestricted\nuniverse\nmultiverse\nWithout the setup shown above,\napt\nwill simply ignore snapshot related\ncommand line options in Ubuntu releases lower than 24.04.\nThe following table describes whether the snapshot service can be used with\neach supported Ubuntu release, and whether or not it is necessary to configure\nthe\nsnapshot\noption in the\nsources.list\nfile.\nUbuntu Release\nSupports Snapshot Service?\nNeed to configure\nsources.list\n?\nNext Ubuntu releases\nYes\nNo\nUbuntu 25.10\nYes\nNo\nUbuntu 25.04\nYes\nNo\nUbuntu 24.04 LTS\nYes\nNo\nUbuntu 22.04 LTS\nYes (with\napt\n>=\n2.4.11\n)\nYes\nUbuntu 20.04 LTS\nYes (with\napt\n>=\n2.0.10\n)\nYes\nQuick start\n¶\nYou can start using the snapshot service right away by passing the\n--snapshot\noption to\napt\nfollowed by a timestamp in the format shown below.\n$ sudo apt install hello --update --snapshot 20240301T030400Z\nThe\n--snapshot\noption and the the timestamp format are discussed with more\ndetails in the next sections.\nUsing the snapshot service\n¶\nThere are several options when it comes to setting up Ubuntu snapshot services.\nHere we present the most relevant, supported ones.\nBelow, we refer to the snapshot ID as\n$SNAPSHOT_ID\n. This is a UTC date and\ntime formatted as YYYYMMDDTHHMMSSZ, e.g., 20240430T214500Z, which refers to the\nsnapshot that represents the state of the Ubuntu archive on April 30th, 2024,\nat 21:45:00 UTC.\nCurrently, the Ubuntu snapshot service provides snapshots for any date and time\nafter 1 March 2023\n.\nThe\n--snapshot\nCLI option\n¶\nThe snapshot service can be used through the\n--snapshot\n(or\n-S\n) CLI option\nfor\napt\n, which receives a snapshot ID argument.\n$ sudo apt update --snapshot $SNAPSHOT_ID\n$ sudo apt install $PACKAGE_NAME --snapshot $SNAPSHOT_ID\nYou can also use the short, atomic version to merge the two actions above in a\nsingle command, which will download the package information from the\nrepository, then download and install the package:\n$ sudo apt install $PACKAGE_NAME --update --snapshot $SNAPSHOT_ID\nor using the short version of the snapshot option:\n$ sudo apt install $PACKAGE_NAME --update -S $SNAPSHOT_ID\nFor instance, if you want to install Docker as it was released when Ubuntu\n24.04 LTS first came out, you could try a snapshot from May 2025:\n$ sudo apt update --snapshot 20240501T120000Z\nNote that you will need to keep using the snapshot option to let\napt\noperate\nover the snapshot repositories. For example, if you want to check what versions\nof Docker are available with the snapshot:\n$ apt policy docker.io --snapshot 20240501T120000Z\ndocker.io:\n  Installed: (none)\n  Candidate: 24.0.7-0ubuntu4\n  Version table:\n     24.0.7-0ubuntu4 500\n        500 https://snapshot.ubuntu.com/ubuntu/20240501T120000Z noble/universe amd64 Packages\nIf you forget to use the snapshot option,\napt\nwill operate over the regular archive:\n$ apt policy docker.io\ndocker.io:\n  Installed: (none)\n  Candidate: 28.2.2-0ubuntu1~24.04.1\n  Version table:\n     28.2.2-0ubuntu1~24.04.1 500\n        500 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages\n     27.5.1-0ubuntu3~24.04.2 500\n        500 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages\n     24.0.7-0ubuntu4 500\n        500 http://archive.ubuntu.com/ubuntu noble/universe amd64 Packages\nThen, you can install Docker from that snapshot:\n$ sudo apt install docker.io --snapshot 20240501T120000Z\n...\n$ docker --version\nDocker version 24.0.7, build 24.0.7-0ubuntu4\nNow, let’s say you checked the\nDocker package publishing history in\nLaunchpad\nand (for some reason) decided you want to install version\n26.1.3-0ubuntu1~24.04.1\n, which, as inferred from the publishing history,\nsuperseded\ndocker.io\n24.0.7-0ubuntu4.1\nin\nnoble-updates\non\n2024-11-25\n.\nThen,\n24.0.7-0ubuntu4.1\nwas removed from the\n-updates\npocket on\n2024-11-26\n.\nYou could then use\n20241126T230000Z\nas the snapshot ID to get the target\npackage (\n26.1.3-0ubuntu1~24.04.1\n):\n$ sudo apt install docker.io --update --snapshot 20241126T230000Z\n...\n$ docker --version\nDocker version 26.1.3, build 26.1.3-0ubuntu1~24.04.1\nNote\nNote that, at the time of writing, the version of\ndocker.io\nin the\nnoble-security\npocket is\n27.5.1-0ubuntu3~24.04.2\n, i.e., the version used in\nthe example above may be affected by known vulnerabilities. When using the\nsnapshot service, do make sure to check the latest version available in your\nUbuntu release’s\n-security\npocket to make sure you are not installing a vulnerable\npackage.\nConfiguring the snapshot service for specific repositories\n¶\nIn your\napt\nrepositories configuration files, it is possible to specify\nsnapshots to be used for each specific repository.\nFor Ubuntu 24.04 LTS and later, which uses the deb822 style by default, we add\na\nSnapshot\noption to the relevant sources file in the following format:\nSnapshot: $SNAPSHOT_ID\nFor Ubuntu series lower than 24.04, you change the value of the\nsnapshot\noption in your sources file, i.e., if you had the following entry in your\n/etc/apt/sources.list\nfile\ndeb\n[\nsnapshot\n=\nyes\n]\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\n...\nyou replace the\nyes\nvalue with the snapshot ID:\ndeb [snapshot=$SNAPSHOT_ID] http://archive.ubuntu.com/ubuntu ...\nLet’s say you want to pin a specific snapshot for all the pockets in your Ubuntu\n24.04 LTS server. Then, you add the\nSnapshot\noptions to\n/etc/apt/sources.list.d/ubuntu.sources\n:\nTypes\n:\ndeb\nURIs\n:\nhttp\n:\n//\narchive\n.\nubuntu\n.\ncom\n/\nubuntu\nSuites\n:\nnoble\nnoble\n-\nupdates\nnoble\n-\nbackports\nComponents\n:\nmain\nuniverse\nrestricted\nmultiverse\nSigned\n-\nBy\n:\n/\nusr\n/\nshare\n/\nkeyrings\n/\nubuntu\n-\narchive\n-\nkeyring\n.\ngpg\nSnapshot\n:\n20250530\nT223000Z\nTypes\n:\ndeb\nURIs\n:\nhttp\n:\n//\nsecurity\n.\nubuntu\n.\ncom\n/\nubuntu\nSuites\n:\nnoble\n-\nsecurity\nComponents\n:\nmain\nuniverse\nrestricted\nmultiverse\nSigned\n-\nBy\n:\n/\nusr\n/\nshare\n/\nkeyrings\n/\nubuntu\n-\narchive\n-\nkeyring\n.\ngpg\nSnapshot\n:\n20250530\nT223000Z\nAfterwards, you can run the following commands to install the Docker version\npresent in that snapshot:\n$ sudo apt install --update docker.io\nto install the Docker version present in that snapshot.\n$ docker --version\nDocker version 27.5.1, build 27.5.1-0ubuntu3~24.04.1\nNote that, when you configure a repository to use a snapshot using the method\ndescribed above,\napt\nwill always ignore the\n--snapshot\noption:\n$ sudo apt install --update docker.io --snapshot 20251013T2300Z\n...\nHit:5 https://snapshot.ubuntu.com/ubuntu/20250530T223000Z noble InRelease\nHit:6 https://snapshot.ubuntu.com/ubuntu/20250530T223000Z noble-updates InRelease\nHit:7 https://snapshot.ubuntu.com/ubuntu/20250530T223000Z noble-backports InRelease\nHit:8 https://snapshot.ubuntu.com/ubuntu/20250530T223000Z noble-security InRelease\n...\ndocker.io is already the newest version (27.5.1-0ubuntu3~24.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\nAs you can see from the logs above,\napt\nis still using the snapshot\nconfigured in\n/etc/apt/sources.list.d/ubuntu.sources\n, ignoring the\n--snapshot\nCLI option.\nConfiguring the snapshot service globally\n¶\nAs an alternative to configuring a specific snapshot for each individual\nrepository, as described in the previous section, you can configure a snapshot\nglobally for all repositories that have snapshots enabled.\nWarning\nIf you are following these examples until here, make sure to revert the changes\nto the sources configuration file made in the previous section.\nLet’s configure\napt\nto default to a specific snapshot by setting the\nAPT::Snapshot\noption:\n$ echo 'APT::Snapshot \"20250801T111111Z\";' | sudo tee /etc/apt/apt.conf.d/50snapshot\nThe system will now default to fetching packages from the configured snapshot:\n$ sudo apt install docker.io --update\n...\n$ docker --version\nDocker version 27.5.1, build 27.5.1-0ubuntu3~24.04.2\nNote that, in contrast to repositories with snapshots configured in the\nsources file as shown in the previous section, configuring a snapshot globally\nwill not make\napt\nignore the\n--snapshot\nCLI option:\n$ sudo apt install docker.io --update --snapshot 20251013T120000Z\n$ docker --version\nDocker version 28.2.2, build 28.2.2-0ubuntu1~24.04.1\nYou can revert the global configuration by removing the file which added it:\n$ sudo rm /etc/apt/apt.conf.d/50snapshot\nFurther reading\n¶\nThe Ubuntu snapshot service official page and documentation\nSecuring multiple Ubuntu instances while maximising uptime\nIntegrating the Ubuntu Snapshot Service into systems management and update tools", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:31Z", "original_len_words": 1656}}
{"id": "b7cd44b6a0", "source_url": "https://documentation.ubuntu.com/server/how-to/software/upgrade-your-release/", "title": "How to upgrade your Ubuntu release - Ubuntu Server documentation", "text": "How to upgrade your Ubuntu release - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to upgrade your Ubuntu release\n¶\nIn this page we show how to upgrade an Ubuntu Server or Ubuntu cloud image to the next major release. This is different from your regular software updates.\nWe recommend running a Long Term Support (LTS) release as it provides 5 years of standard support and security updates, whereas interim releases are only supported for nine months.\nAfter the initial standard support period ends for an LTS release, an extended maintenance period is available via an\nUbuntu Pro subscription\n, which provides coverage for an additional five years and is available for free on up to five machines. Find out more about the\nrelease lifecycle and support period\nfor your release.\nUnderstanding Upgrade Paths\n¶\nYou can only upgrade from one LTS release directly to the\nnext sequential LTS release\n. For example, if you are on Ubuntu 16.04 LTS, you can upgrade to Ubuntu 18.04 LTS.\nHowever, you cannot skip releases (e.g. jump from 16.04 LTS to 20.04 LTS).If you need to reach a later LTS, you will have to upgrade in stages: first to Ubuntu 18.04 LTS, then to Ubuntu 20.04 LTS, and so on.\nPre-upgrade checklist\n¶\nBefore starting a major release upgrade, it’s important to prepare your system to ensure a smooth transition. This step is essential, so we need to review the following items:\nReview Releases notes:\nAlways check the\nrelease notes\nfor the new Ubuntu version we are moving to. This can be found on the\nUbuntu Wiki Releases Page\n.\nFully update the current system:\nThe release upgrade process requires that the current system has all the latest updates installed. This is a standard\npackage upgrade\n:\napt update\nwill refresh package index database, and\napt upgrade\nwill download and install the latest versions of installed packages.\nRun these commands to ensure everything is up to date:\nsudo\napt\nupdate\nsudo\napt\nupgrade\nConfirm both commands complete successfully and no further updates are available.\nAfter applying all updates, it may be necessary to\nreboot your system.\nThe release upgrade process will let you know if that’s needed, but you can also check manually before: if the file\n/run/reboot-required\nexists, then you will need to reboot.\nCheck that there is enough free space:\nA release upgrade involves downloading hundreds of new packages, which can be several gigabytes. Make sure you have enough free disk space.\nDedicate time for the upgrade:\nThis is an interactive process. The release upgrade will sometimes stop and ask questions, so you should monitor the upgrade and be available to respond.\nUnderstand Third-party repositories:\nThird-party software repositories and Personal Package Archives (PPAs) are disabled during the release upgrade. While software installed from these sources will not be removed, it’s the most common cause of upgrade issues. Be prepared to re-enable them or find updated versions after the upgrade.\nBackup all your data:\nAlthough upgrades are normally safe, there is always a chance that something could go wrong. It is extremely important that the data is safely copied to a backup location to allow restoration if any problems occur.\nUpgrade the system\n¶\nWe recommend upgrading the system using the\ndo-release-upgrade\ncommand on Server edition and cloud images. This command can handle system configuration changes that are sometimes needed between releases.\nTo start the process, run this command:\nsudo\ndo\n-release-upgrade\nNote\nUpgrading to a development release of Ubuntu is available using the\n-d\nflag. However, using the development release (or the\n-d\nflag) is\nnot recommended\nfor production environments.\nUpgrades from one LTS release to the next one are only available after the first point release. For example, Ubuntu 18.04 LTS will only upgrade to Ubuntu 20.04 LTS after the 20.04.1 point release. If users wish to update before the point release (e.g., on a subset of machines to evaluate the LTS upgrade) users can force the upgrade via the\n-d\nflag.\nPre-upgrade summary\n¶\nBefore making any changes the command\ndo-release-upgrade\nwill first do some checks to verify the system is ready to upgrade, and provide a summary of the upgrade before proceeding. If you accept the changes, the process will begin to update the system’s packages:\nDo you want to start the upgrade?  \n\n\n5 installed packages are no longer supported by Canonical. You can  \nstill get support from the community.  \n\n4 packages are going to be removed. 117 new packages are going to be  \ninstalled. 424 packages are going to be upgraded.  \n\nYou have to download a total of 262 M. This download will take about  \n33 minutes with a 1Mbit DSL connection and about 10 hours with a 56k  \nmodem.  \n\nFetching and installing the upgrade can take several hours. Once the  \ndownload has finished, the process cannot be canceled.  \n\nContinue [yN]  Details [d]\nConfiguration changes\n¶\nDuring the upgrade process you may be presented with a message to make decisions about package updates. These prompts occur when there are existing configuration files (e.g. edited by the user) and the new package configuration file are different. Below is an example prompt:\nConfiguration file '/etc/ssh/ssh_config'\n ==> Modified (by you or by a script) since installation.\n ==> Package distributor has shipped an updated version.\n   What would you like to do about it ?  Your options are:\n    Y or I  : install the package maintainer's version\n    N or O  : keep your currently-installed version\n      D     : show the differences between the versions\n      Z     : start a shell to examine the situation\n The default action is to keep your current version.\n*** ssh_config (Y/I/N/O/D/Z) [default=N] ?\nYou should look at the differences between the files and decide what to do. The default response is to keep the current version of the file. There are situations where accepting the new version, like with\n/boot/grub/menu.lst\n, is required for the system to boot correctly with the new kernel.\nRemoving Obsolete Packages\n¶\nAfter all packages are updated, you can choose to remove any obsolete packages that are no longer needed:\nRemove obsolete packages?  \n\n\n30 packages are going to be removed.  \n\nContinue [yN]  Details [d]\nReboot\n¶\nFinally, when the upgrade is complete you are prompted to reboot the system. The system is not considered upgraded until this reboot occurs:\nSystem upgrade is complete.\n\nRestart required  \n\nTo finish the upgrade, a restart is required.  \nIf you select 'y' the system will be restarted.  \n\nContinue [yN]\nFurther reading\n¶\nFor a complete list of releases and current support status, see the\nList of releases\npage.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:31Z", "original_len_words": 1112}}
{"id": "fa858a5095", "source_url": "https://documentation.ubuntu.com/server/how-to/storage/", "title": "Storage - Ubuntu Server documentation", "text": "Storage - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nStorage\n¶\nThe Ubuntu Server installer can set up and install to\nLogical Volume Management (LVM)\npartitions.\nUbuntu Server can be configured as both an\niSCSI initiator and an iSCSI target.\nSee also\n¶\nExplanation:\nAbout Logical Volume Management (LVM)", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:31Z", "original_len_words": 71}}
{"id": "4138b57cb2", "source_url": "https://documentation.ubuntu.com/server/how-to/storage/iscsi-initiator-or-client/", "title": "iSCSI initiator (or client) - Ubuntu Server documentation", "text": "iSCSI initiator (or client) - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\niSCSI initiator (or client)\n¶\nWikipedia\niSCSI Definition\n:\niSCSI an acronym for\nInternet Small Computer Systems Interface\n, an\nInternet Protocol\n(IP)-based storage networking standard for linking data storage facilities. It provides\nblock-level access\nto\nstorage devices\nby carrying\nSCSI\ncommands over a\nTCP/IP\nnetwork.\niSCSI is used to facilitate data transfers over\nintranets\nand to manage storage over long distances. It can be used to transmit data over\nlocal area networks\n(LANs),\nwide area networks\n(WANs), or the\nInternet\nand can enable location-independent data storage and retrieval.\nThe\nprotocol\nallows clients (called\ninitiators\n) to send SCSI commands (\nCDBs\n) to storage devices (\ntargets\n) on remote servers.  It is a\nstorage area network\n(SAN) protocol, allowing organizations to consolidate storage into\nstorage arrays\nwhile providing clients (such as database and web servers) with the illusion of locally attached SCSI disks.\nIt mainly competes with\nFibre Channel\n, but unlike traditional\nFibre Channel\n, which usually requires dedicated cabling, iSCSI can be run over long distances using existing network infrastructure.\nUbuntu Server can be configured as both:\niSCSI initiator\nand\niSCSI target\n. This guide provides commands and configuration options to setup an\niSCSI initiator\n(or Client).\nNote\nIt is assumed that\nyou already have an iSCSI target on your local network\nand have the appropriate rights to connect to it. The instructions for setting up a target vary greatly between hardware providers, so consult your vendor documentation to configure your specific iSCSI target.\nNetwork Interfaces Configuration\n¶\nBefore start configuring iSCSI, make sure to have the network interfaces correctly set and configured in order to have open-iscsi package to behave appropriately, specially during boot time. In Ubuntu 20.04 LTS, the default network configuration tool is\nnetplan.io\n.\nFor all the iSCSI examples below please consider the following netplan configuration for my iSCSI initiator:\n/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n{\nconfig\n:\ndisabled\n}\n/etc/netplan/50-cloud-init.yaml\nnetwork\n:\nethernets\n:\nenp5s0\n:\nmatch\n:\nmacaddress\n:\n00\n:\n16\n:\n3\ne\n:\naf\n:\nc4\n:\nd6\nset\n-\nname\n:\neth0\ndhcp4\n:\ntrue\ndhcp\n-\nidentifier\n:\nmac\nenp6s0\n:\nmatch\n:\nmacaddress\n:\n00\n:\n16\n:\n3\ne\n:\n50\n:\n11\n:\n9\nc\nset\n-\nname\n:\niscsi01\ndhcp4\n:\ntrue\ndhcp\n-\nidentifier\n:\nmac\ndhcp4\n-\noverrides\n:\nroute\n-\nmetric\n:\n300\nenp7s0\n:\nmatch\n:\nmacaddress\n:\n00\n:\n16\n:\n3\ne\n:\nb3\n:\ncc\n:\n50\nset\n-\nname\n:\niscsi02\ndhcp4\n:\ntrue\ndhcp\n-\nidentifier\n:\nmac\ndhcp4\n-\noverrides\n:\nroute\n-\nmetric\n:\n300\nversion\n:\n2\nrenderer\n:\nnetworkd\nWith this configuration, the interfaces names change by matching their mac addresses. This makes it easier to manage them in a server containing multiple interfaces.\nFrom this point and beyond, 2 interfaces are going to be mentioned:\niscsi01\nand\niscsi02\n. This helps to demonstrate how to configure iSCSI in a multipath environment as well (check the Device Mapper Multipath session in this same Server Guide).\nIf you have only a single interface for the iSCSI network, make sure to follow the same instructions, but only consider the\niscsi01\ninterface command line examples.\niSCSI Initiator Install\n¶\nTo configure Ubuntu Server as an iSCSI initiator install the open-iscsi package. In a terminal enter:\n$ sudo apt install open-iscsi\nOnce the package is installed you will find the following files:\n/etc/iscsi/iscsid.conf\n/etc/iscsi/initiatorname.iscsi\niSCSI Initiator Configuration\n¶\nConfigure the main configuration file like the example bellow:\n/etc/iscsi/iscsid.conf\n### startup settings\n## will be controlled by systemd, leave as is\niscsid\n.\nstartup\n=\n/\nusr\n/\nsbin\n/\niscsidnode\n.\nstartup\n=\nmanual\n### chap settings\n# node.session.auth.authmethod = CHAP\n## authentication of initiator by target (session)\n# node.session.auth.username = username\n# node.session.auth.password = password\n# discovery.sendtargets.auth.authmethod = CHAP\n## authentication of initiator by target (discovery)\n# discovery.sendtargets.auth.username = username\n# discovery.sendtargets.auth.password = password\n### timeouts\n## control how much time iscsi takes to propagate an error to the\n## upper layer. if using multipath, having 0 here is desirable\n## so multipath can handle path errors as quickly as possible\n## (and decide to queue or not if missing all paths)\nnode\n.\nsession\n.\ntimeo\n.\nreplacement_timeout\n=\n0\nnode\n.\nconn\n[\n0\n]\n.\ntimeo\n.\nlogin_timeout\n=\n15\nnode\n.\nconn\n[\n0\n]\n.\ntimeo\n.\nlogout_timeout\n=\n15\n## interval for a NOP-Out request (a ping to the target)\nnode\n.\nconn\n[\n0\n]\n.\ntimeo\n.\nnoop_out_interval\n=\n5\n## and how much time to wait before declaring a timeout\nnode\n.\nconn\n[\n0\n]\n.\ntimeo\n.\nnoop_out_timeout\n=\n5\n## default timeouts for error recovery logics (lu & tgt resets)\nnode\n.\nsession\n.\nerr_timeo\n.\nabort_timeout\n=\n15\nnode\n.\nsession\n.\nerr_timeo\n.\nlu_reset_timeout\n=\n30\nnode\n.\nsession\n.\nerr_timeo\n.\ntgt_reset_timeout\n=\n30\n### retry\nnode\n.\nsession\n.\ninitial_login_retry_max\n=\n8\n### session and device queue depth\nnode\n.\nsession\n.\ncmds_max\n=\n128\nnode\n.\nsession\n.\nqueue_depth\n=\n32\n### performance\nnode\n.\nsession\n.\nxmit_thread_priority\n=\n-\n20\nand re-start the iSCSI daemon:\n$ systemctl restart iscsid.service\nThis will set basic things up for the rest of configuration.\nThe other file mentioned:\n/etc/iscsi/initiatorname.iscsi\nInitiatorName\n=\niqn\n.2004\n-\n10.\ncom\n.\nubuntu\n:\n01\n:\n60\nf3517884c3\ncontains this node’s initiator name and is generated during open-iscsi package installation. If you modify this setting, make sure that you don’t have duplicates in the same iSCSI SAN (Storage Area Network).\niSCSI Network Configuration\n¶\nBefore configuring the Logical Units that are going to be accessed by the initiator, it is important to inform the iSCSI service what are the interfaces acting as paths.\nA straightforward way to do that is by:\nconfiguring the following environment variables\n$ iscsi01_ip=$(ip -4 -o addr show iscsi01 | sed -r 's:.* (([0-9]{1,3}\\.){3}[0-9]{1,3})/.*:\\1:')\n$ iscsi02_ip=$(ip -4 -o addr show iscsi02 | sed -r 's:.* (([0-9]{1,3}\\.){3}[0-9]{1,3})/.*:\\1:')\n\n$ iscsi01_mac=$(ip -o link show iscsi01 | sed -r 's:.*\\s+link/ether (([0-f]{2}(\\:|)){6}).*:\\1:g')\n$ iscsi02_mac=$(ip -o link show iscsi02 | sed -r 's:.*\\s+link/ether (([0-f]{2}(\\:|)){6}).*:\\1:g')\nconfiguring\niscsi01\ninterface\n$ sudo iscsiadm -m iface -I iscsi01 --op=new\nNew interface iscsi01 added\n$ sudo iscsiadm -m iface -I iscsi01 --op=update -n iface.hwaddress -v $iscsi01_mac\niscsi01 updated.\n$ sudo iscsiadm -m iface -I iscsi01 --op=update -n iface.ipaddress -v $iscsi01_ip\niscsi01 updated.\nconfiguring\niscsi02\ninterface\n$ sudo iscsiadm -m iface -I iscsi02 --op=new\nNew interface iscsi02 added\n$ sudo iscsiadm -m iface -I iscsi02 --op=update -n iface.hwaddress -v $iscsi02_mac\niscsi02 updated.\n$ sudo iscsiadm -m iface -I iscsi02 --op=update -n iface.ipaddress -v $iscsi02_ip\niscsi02 updated.\ndiscovering the\ntargets\n$ sudo iscsiadm -m discovery -I iscsi01 --op=new --op=del --type sendtargets --portal storage.iscsi01\n10.250.94.99:3260,1 iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca\n\n$ sudo iscsiadm -m discovery -I iscsi02 --op=new --op=del --type sendtargets --portal storage.iscsi02\n10.250.93.99:3260,1 iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca\nconfiguring\nautomatic login\n$ sudo iscsiadm -m node --op=update -n node.conn[0].startup -v automatic\n$ sudo iscsiadm -m node --op=update -n node.startup -v automatic\nmake sure needed\nservices\nare enabled during OS initialization:\n$ systemctl enable open-iscsi\nSynchronizing state of open-iscsi.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install enable open-iscsi\nCreated symlink /etc/systemd/system/iscsi.service → /lib/systemd/system/open-iscsi.service.\nCreated symlink /etc/systemd/system/sysinit.target.wants/open-iscsi.service → /lib/systemd/system/open-iscsi.service.\n\n$ systemctl enable iscsid\nSynchronizing state of iscsid.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install enable iscsid\nCreated symlink /etc/systemd/system/sysinit.target.wants/iscsid.service → /lib/systemd/system/iscsid.service.\nrestarting\niscsid\nservice\n$ systemctl restart iscsid.service\nand, finally,\nlogin in\ndiscovered logical units\n$ sudo iscsiadm -m node --loginall=automatic\nLogging in to [iface: iscsi02, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.93.99,3260] (multiple)\nLogging in to [iface: iscsi01, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.94.99,3260] (multiple)\nLogin to [iface: iscsi02, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.93.99,3260] successful.\nLogin to [iface: iscsi01, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.94.99,3260] successful.\nAccessing the Logical Units (or LUNs)\n¶\nCheck\ndmesg\nto make sure that the new disks have been detected:\ndmesg\n[\n166.840694\n]\nscsi\n7\n:\n0\n:\n0\n:\n4\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.840892\n]\nscsi\n8\n:\n0\n:\n0\n:\n4\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.841741\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\nAttached\nscsi\ngeneric\nsg2\ntype\n0\n[\n166.841808\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\nAttached\nscsi\ngeneric\nsg3\ntype\n0\n[\n166.842278\n]\nscsi\n7\n:\n0\n:\n0\n:\n3\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.842571\n]\nscsi\n8\n:\n0\n:\n0\n:\n3\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.843482\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\nAttached\nscsi\ngeneric\nsg4\ntype\n0\n[\n166.843681\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\nAttached\nscsi\ngeneric\nsg5\ntype\n0\n[\n166.843706\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\n[\nsdd\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.843884\n]\nscsi\n8\n:\n0\n:\n0\n:\n2\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.843971\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\n[\nsdd\n]\nWrite\nProtect\nis\noff\n[\n166.843972\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\n[\nsdd\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.844127\n]\nscsi\n7\n:\n0\n:\n0\n:\n2\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.844232\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\n[\nsdc\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.844421\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\n[\nsdd\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.844566\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\n[\nsdc\n]\nWrite\nProtect\nis\noff\n[\n166.844568\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\n[\nsdc\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.844846\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\nAttached\nscsi\ngeneric\nsg6\ntype\n0\n[\n166.845147\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\n[\nsdc\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.845188\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\n[\nsdd\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.845527\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\nAttached\nscsi\ngeneric\nsg7\ntype\n0\n[\n166.845678\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\n[\nsde\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.845785\n]\nscsi\n8\n:\n0\n:\n0\n:\n1\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.845799\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\n[\nsdc\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.845931\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\n[\nsde\n]\nWrite\nProtect\nis\noff\n[\n166.845933\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\n[\nsde\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.846424\n]\nscsi\n7\n:\n0\n:\n0\n:\n1\n:\nDirect\n-\nAccess\nLIO\n-\nORG\nTCMU\ndevice\n>\n0002\nPQ\n:\n0\nANSI\n:\n5\n[\n166.846552\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\n[\nsde\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.846708\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\n[\nsdf\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.847024\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\nAttached\nscsi\ngeneric\nsg8\ntype\n0\n[\n166.847029\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\n[\nsdf\n]\nWrite\nProtect\nis\noff\n[\n166.847031\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\n[\nsdf\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.847043\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\n[\nsde\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.847133\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\n[\nsdg\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.849212\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\n[\nsdg\n]\nWrite\nProtect\nis\noff\n[\n166.849214\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\n[\nsdg\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.849711\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\n[\nsdf\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.849718\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\nAttached\nscsi\ngeneric\nsg9\ntype\n0\n[\n166.849721\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\n[\nsdh\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.853296\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\n[\nsdg\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.853721\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\n[\nsdg\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.853810\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\n[\nsdh\n]\nWrite\nProtect\nis\noff\n[\n166.853812\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\n[\nsdh\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.854026\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\n[\nsdf\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.854431\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\n[\nsdh\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.854625\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\n[\nsdi\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.854898\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\n[\nsdi\n]\nWrite\nProtect\nis\noff\n[\n166.854900\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\n[\nsdi\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.855022\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\n[\nsdh\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.855465\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\n[\nsdi\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.855578\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\n[\nsdj\n]\n2097152\n512\n-\nbyte\nlogical\nblocks\n:\n>\n(\n1.07\nGB\n/\n1.00\nGiB\n)\n[\n166.855845\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\n[\nsdj\n]\nWrite\nProtect\nis\noff\n[\n166.855847\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\n[\nsdj\n]\nMode\nSense\n:\n2\nf\n00\n00\n00\n[\n166.855978\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\n[\nsdi\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.856305\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\n[\nsdj\n]\nWrite\ncache\n:\nenabled\n,\nread\ncache\n:\n>\nenabled\n,\ndoesn\n't support DPO or FUA\n[\n166.856701\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\n[\nsdj\n]\nOptimal\ntransfer\nsize\n65536\nbytes\n[\n166.859624\n]\nsd\n8\n:\n0\n:\n0\n:\n4\n:\n[\nsdd\n]\nAttached\nSCSI\ndisk\n[\n166.861304\n]\nsd\n7\n:\n0\n:\n0\n:\n4\n:\n[\nsdc\n]\nAttached\nSCSI\ndisk\n[\n166.864409\n]\nsd\n8\n:\n0\n:\n0\n:\n3\n:\n[\nsde\n]\nAttached\nSCSI\ndisk\n[\n166.864833\n]\nsd\n7\n:\n0\n:\n0\n:\n3\n:\n[\nsdf\n]\nAttached\nSCSI\ndisk\n[\n166.867906\n]\nsd\n8\n:\n0\n:\n0\n:\n2\n:\n[\nsdg\n]\nAttached\nSCSI\ndisk\n[\n166.868446\n]\nsd\n8\n:\n0\n:\n0\n:\n1\n:\n[\nsdi\n]\nAttached\nSCSI\ndisk\n[\n166.871588\n]\nsd\n7\n:\n0\n:\n0\n:\n1\n:\n[\nsdj\n]\nAttached\nSCSI\ndisk\n[\n166.871773\n]\nsd\n7\n:\n0\n:\n0\n:\n2\n:\n[\nsdh\n]\nAttached\nSCSI\ndisk\nIn the output above you will find\n8 x SCSI disks\nrecognized. The storage server is mapping\n4 x LUNs\nto this node, AND the node has\n2  x PATHs\nto each LUN. The OS recognizes each path to each device as 1 SCSI device.\nYou will find different output depending on the storage server your node is mapping the LUNs from, and the amount of LUNs being mapped as well.\nAlthough not the objective of this session, let’s find the 4 mapped LUNs using multipath-tools.\nYou will find further details about multipath in “Device Mapper Multipathing” session of this same guide.\n$ apt-get install multipath-tools\n$ sudo multipath -r\n$ sudo multipath -ll\nmpathd (360014051a042fb7c41c4249af9f2cfbc) dm-3 LIO-ORG,TCMU device\nsize=1.0G features='0' hwhandler='0' wp=rw\n|-+- policy='service-time 0' prio=1 status=active\n| `- 7:0:0:4 sde 8:64  active ready running\n`-+- policy='service-time 0' prio=1 status=enabled\n  `- 8:0:0:4 sdc 8:32  active ready running\nmpathc (360014050d6871110232471d8bcd155a3) dm-2 LIO-ORG,TCMU device\nsize=1.0G features='0' hwhandler='0' wp=rw\n|-+- policy='service-time 0' prio=1 status=active\n| `- 7:0:0:3 sdf 8:80  active ready running\n`-+- policy='service-time 0' prio=1 status=enabled\n  `- 8:0:0:3 sdd 8:48  active ready running\nmpathb (360014051f65c6cb11b74541b703ce1d4) dm-1 LIO-ORG,TCMU device\nsize=1.0G features='0' hwhandler='0' wp=rw\n|-+- policy='service-time 0' prio=1 status=active\n| `- 7:0:0:2 sdh 8:112 active ready running\n`-+- policy='service-time 0' prio=1 status=enabled\n  `- 8:0:0:2 sdg 8:96  active ready running\nmpatha (36001405b816e24fcab64fb88332a3fc9) dm-0 LIO-ORG,TCMU device\nsize=1.0G features='0' hwhandler='0' wp=rw\n|-+- policy='service-time 0' prio=1 status=active\n| `- 7:0:0:1 sdj 8:144 active ready running\n`-+- policy='service-time 0' prio=1 status=enabled\n  `- 8:0:0:1 sdi 8:128 active ready running\nNow it is much easier to understand each recognized SCSI device and common paths to same LUNs in the storage server. With the output above one can easily see that:\nmpatha device\n(/dev/mapper/mpatha) is a multipath device for:\n/dev/sdj\n/dev/dsi\nmpathb device\n(/dev/mapper/mpathb) is a multipath device for:\n/dev/sdh\n/dev/dsg\nmpathc device\n(/dev/mapper/mpathc) is a multipath device for:\n/dev/sdf\n/dev/sdd\nmpathd device\n(/dev/mapper/mpathd) is a multipath device for:\n/dev/sde\n/dev/sdc\nDo not use this in production\nwithout checking appropriate multipath configuration options in the\nDevice Mapper Multipathing\nsession. The\ndefault multipath configuration\nis less than optimal for regular usage.\nFinally, to access the LUN (or remote iSCSI disk) you will:\nIf accessing through a single network interface:\naccess it through /dev/sdX where X is a letter given by the OS\nIf accessing through multiple network interfaces:\nconfigure multipath and access the device through /dev/mapper/X\nFor everything else, the created devices are block devices and all commands used with local disks should work the same way:\nCreating a partition:\n$ sudo fdisk /dev/mapper/mpatha\n\nWelcome to fdisk (util-linux 2.34).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\nDevice does not contain a recognized partition table.\nCreated a new DOS disklabel with disk identifier 0x92c0322a.\n\nCommand (m for help): p\nDisk /dev/mapper/mpatha: 1 GiB, 1073741824 bytes, 2097152 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 65536 bytes\nDisklabel type: dos\nDisk identifier: 0x92c0322a\n\nCommand (m for help): n\nPartition type\n   p   primary (0 primary, 0 extended, 4 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (1-4, default 1):\nFirst sector (2048-2097151, default 2048):\nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-2097151, default 2097151):\n\nCreated a new partition 1 of type 'Linux' and of size 1023 MiB.\n\nCommand (m for help): w\nThe partition table has been altered.\nCreating a\nfilesystem\n:\n$ sudo mkfs.ext4 /dev/mapper/mpatha-part1\nmke2fs 1.45.5 (07-Jan-2020)\nCreating filesystem with 261888 4k blocks and 65536 inodes\nFilesystem UUID: cdb70b1e-c47c-47fd-9c4a-03db6f038988\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376\n\nAllocating group tables: done\nWriting inode tables: done\nCreating journal (4096 blocks): done\nWriting superblocks and filesystem accounting information: done\nMounting the block device:\n$ sudo mount /dev/mapper/mpatha-part1 /mnt\nAccessing the data:\n$ ls /mnt\nlost+found\nMake sure to read other important sessions in Ubuntu Server Guide to follow up with concepts explored in this one.\nReferences\n¶\niscsid\niscsi.conf\niscsid.conf\niscsi.service\niscsid.service\nOpen-iSCSI\nDebian Open-iSCSI", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:31Z", "original_len_words": 3506}}
{"id": "6df78d4635", "source_url": "https://documentation.ubuntu.com/server/how-to/storage/manage-logical-volumes/", "title": "How to manage logical volumes - Ubuntu Server documentation", "text": "How to manage logical volumes - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to manage logical volumes\n¶\nThe Ubuntu Server installer has the ability to set up and install to LVM partitions, and this is the supported way of doing so. If you would like to know more about any of the topics in this page, refer to our\nexplanation of logical volume management (LVM)\n.\nCreate the physical volume\n¶\nFirst, you need a physical volume. Typically you start with a hard disk, and create a regular partition whose type is “LVM” on it. You can create it with\ngparted\nor\nfdisk\n, and usually only want one LVM-type partition in the whole disk, since LVM will handle subdividing it into logical volumes. In\ngparted\n, you need to check the\nlvm\nflag when creating the partition, and with\nfdisk\n, tag the type with code\n8e\n.\nOnce you have your LVM partition, you need to initialize it as a physical volume. Assuming this partition is\n/dev/sda1\n:\nsudo\npvcreate\n/dev/sda1\nCreate the volume group\n¶\nAfter that, you can create a volume group; in our example, it will be named\nfoo\nand uses only one physical volume:\nsudo\nvgcreate\nfoo\n/dev/sda1\nCreate a logical volume\n¶\nNow you want to create a logical volume from some of the free space in\nfoo\n:\nsudo\nlvcreate\n--name\nbar\n--size\n5g\nfoo\nThis creates a logical volume named\nbar\nin volume group\nfoo\nusing 5 GB of space. You can find the block device for this logical volume in\n/dev/foo/bar\nor\ndev/mapper/foo-bar\n.\nYou might also want to try the\nlvs\nand\npvs\ncommands, which list the logical volumes and physical volumes respectively, and their more detailed variants;\nlvdisplay\nand\npvdisplay\n.\nResize a partition\n¶\nYou can extend a logical volume with:\nsudo\nlvextend\n--resizefs\n--size\n+5g\nfoo/bar\nThis will add 5 GB to the\nbar\nlogical volume in the\nfoo\nvolume group, and will automatically resize the underlying\nfilesystem\n(if supported). The space is allocated from free space anywhere in the\nbar\nvolume group. You can specify an absolute size instead of a relative size if you want by omitting the leading\n+\n.\nIf you have multiple physical volumes you can add the names of one (or more) of them to the end of the command to limit which ones are used to fulfill the request.\nMove a partition\n¶\nIf you only have one physical volume then you are unlikely to ever need to move, but if you add a new disk, you might want to. To move the logical volume\nbar\noff of physical volume\n/dev/sda1\n, you can run:\nsudo\npvmove\n--name\nbar\n/dev/sda1\nIf you omit the\n--name\nbar\nargument, then all logical volumes on the\n/dev/sda1\nphysical volume will be moved. If you only have one other physical volume then that is where it will be moved to. Otherwise you can add the name of one or more specific physical volumes that should be used to satisfy the request, instead of any physical volume in the volume group with free space.\nThis process can be resumed safely if interrupted by a crash or power failure, and can be done while the logical volume(s) in question are in use. You can also add\n--background\nto perform the move in the background and return immediately, or\n--interval\ns\nto have it print how much progress it has made every\ns\nseconds. If you background the move, you can check its progress with the\nlvs\ncommand.\nCreate a snapshot\n¶\nWhen you create a snapshot, you create a new logical volume to act as a clone of the original logical volume.\nThe snapshot volume does not initially use any space, but as changes are made to the original volume, the changed blocks are copied to the snapshot volume before they are changed in order to preserve them. This means that the more changes you make to the origin, the more space the snapshot needs. If the snapshot volume uses all of the space allocated to it, then the snapshot is broken and can not be used any more, leaving you with only the modified origin.\nThe\nlvs\ncommand will tell you how much space has been used in a snapshot logical volume. If it starts to get full, you might want to extend it with the\nlvextend\ncommand. To create a snapshot of the bar logical volume and name it\nlv_snapshot\n, run:\nsudo\nlvcreate\n--snapshot\n--name\nlv_snapshot\n--size\n5g\nfoo/bar\nThis will create a snapshot named\nlv_snapshot\nof the original logical volume\nbar\nand allocate 5 GB of space for it. Since the snapshot volume only stores the areas of the disk that have changed since it was created, it can be much smaller than the original volume.\nWhile you have the snapshot, you can mount it if you wish to see the original filesystem as it appeared when you made the snapshot. In the above example you would mount the\n/dev/foo/lv_snapshot\ndevice. You can modify the snapshot without affecting the original, and the original without affecting the snapshot. For example, if you take a snapshot of your root logical volume, make changes to your system, and then decide you would like to revert the system back to its previous state, you can merge the snapshot back into the original volume, which effectively reverts it to the state it was in when you made the snapshot. To do this, you can run:\nsudo\nlvconvert\n--merge\nfoo/lv_snapshot\nIf the origin volume of\nfoo/lv_snapshot\nis in use, it will inform you that the merge will take place the next time the volumes are activated. If this is the root volume, then you will need to reboot for this to happen. At the next boot, the volume will be activated and the merge will begin in the background, so your system will boot up as if you had never made the changes since the snapshot was created, and the actual data movement will take place in the background while you work.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:32Z", "original_len_words": 1039}}
{"id": "adbafa3641", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/", "title": "Virtualisation - Ubuntu Server documentation", "text": "Virtualisation - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nVirtualisation\n¶\nIn this section we show how to install, configure and use various options for creating virtual machines (VMs). For more information about these options, you may want to refer to our\nIntroduction to virtualization\nVirtual machines\n¶\nCreate VMs with Multipass\nCreate cloud image VMs with UVtool\nQEMU\nConfidential Computing with AMD\nVM tooling\n¶\nHow to use the libvirt library with virsh\nHow to use virt-manager and other virt* tools\nHow to enable nested virtualisation\nUbuntu in other virtual environments\n¶\nSetting up Ubuntu on Hyper-V\n(Windows 11)\nSee also\n¶\nExplanation:\nVirtualisation and containers", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:32Z", "original_len_words": 129}}
{"id": "2c2cc48c2f", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/cloud-image-vms-with-uvtool/", "title": "Create cloud image VMs with UVtool - Ubuntu Server documentation", "text": "Create cloud image VMs with UVtool - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nCreate cloud image VMs with UVtool\n¶\nWith Ubuntu being one of the most popular operating systems on many cloud platforms, the availability of stable and secure cloud images has become very important. Since Ubuntu 12.04, the use of cloud images outside of a cloud infrastructure has been improved so that it is now possible to use those images to create a virtual machine without needing a complete installation.\nCreating virtual machines using\nuvtool\n¶\nStarting with Ubuntu 14.04 LTS, a tool called\nuvtool\nhas greatly facilitated the creation of virtual machines (VMs) using cloud images.\nuvtool\nprovides a simple mechanism for synchronizing cloud images locally and using them to create new VMs in minutes.\nInstall\nuvtool\npackages\n¶\nThe following packages and their dependencies are required in order to use\nuvtool\n:\nuvtool\nuvtool-libvirt\nTo install\nuvtool\n, run:\nsudo\napt\n-y\ninstall\nuvtool\nThis will install\nuvtool\n’s main commands,\nuvt-simplestreams-libvirt\nand\nuvt-kvm\n.\nGet the Ubuntu cloud image with\nuvt-simplestreams-libvirt\n¶\nThis is one of the major simplifications that\nuvtool\nprovides. It knows where to find the cloud images so you only need one command to get a new cloud image. For instance, if you want to synchronize all cloud images for the amd64 architecture, the\nuvtool\ncommand would be:\nuvt-simplestreams-libvirt\n--verbose\nsync\narch\n=\namd64\nAfter all the images have been downloaded from the Internet, you will have a complete set of locally-stored cloud images. To see what has been downloaded, use the following command:\nuvt-simplestreams-libvirt\nquery\nWhich will provide you with a list like this:\nrelease=bionic arch=amd64 label=daily (20191107)\nrelease=focal arch=amd64 label=daily (20191029)\n...\nIn the case where you want to synchronize only one specific cloud image, you need to use the\nrelease=\nand\narch=\nfilters to identify which image needs to be synchronized.\nuvt-simplestreams-libvirt\nsync\nrelease\n=\nDISTRO-SHORT-CODENAME\narch\n=\namd64\nFurthermore, you can provide an alternative URL to fetch images from. A common case is the daily image, which helps you get the very latest images, or if you need access to the not-yet-released development release of Ubuntu. As an example:\nuvt-simplestreams-libvirt\nsync\n--source\nhttp://cloud-images.ubuntu.com/daily\n[\n...\nfurther\noptions\n]\nCreate a valid SSH key\n¶\nTo connect to the virtual machine once it has been created, you must first have a valid SSH key available for the Ubuntu user. If your environment does not have an SSH key, you can create one using the\nssh-keygen\ncommand, which will produce similar output to this:\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\nThe key fingerprint is:\n4d:ba:5d:57:c9:49:ef:b5:ab:71:14:56:6e:2b:ad:9b ubuntu@DISTRO-SHORT-CODENAMES\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|               ..|\n|              o.=|\n|          .    **|\n|         +    o+=|\n|        S . ...=.|\n|         o . .+ .|\n|        . .  o o |\n|              *  |\n|             E   |\n+-----------------+\nCreate the VM using\nuvt-kvm\n¶\nTo create a new virtual machine using\nuvtool\n, run the following in a terminal:\nuvt-kvm\ncreate\nfirsttest\nThis will create a VM named ‘firsttest’ using the current locally-available LTS cloud image. If you want to specify a release to be used to create the VM, you need to use the\nrelease=\nfilter, and the short codename of the release, e.g. “jammy”:\nuvt-kvm\ncreate\nsecondtest\nrelease\n=\nDISTRO-SHORT-CODENAME\nThe\nuvt-kvm\nwait\ncommand can be used to wait until the creation of the VM has completed:\nuvt-kvm\nwait\nsecondttest\nConnect to the running VM\n¶\nOnce the virtual machine creation is completed, you can connect to it using SSH:\nuvt-kvm\nssh\nsecondtest\nYou can also connect to your VM using a regular SSH session using the IP address of the VM. The address can be queried using the following command:\n$\nuvt-kvm\nip\nsecondtest\n192\n.168.122.199\n$\nssh\n-i\n~/.ssh/id_rsa\nubuntu@192.168.122.199\n[\n...\n]\nTo\nrun\na\ncommand\nas\nadministrator\n(\nuser\n\"root\"\n)\n,\nuse\n\"sudo <command>\"\n.\nSee\n\"man sudo_root\"\nfor\ndetails.\n\nubuntu@secondtest:~$\nGet the list of running VMs\n¶\nYou can get the list of VMs running on your system with the\nuvt-kvm\nlist\ncommand.\nDestroy your VM\n¶\nOnce you are finished with your VM, you can destroy it with:\nuvt-kvm\ndestroy\nsecondtest\nNote\nUnlike libvirt’s\ndestroy\nor\nundefine\nactions, this will (by default) also remove the associated virtual storage files.\nMore\nuvt-kvm\noptions\n¶\nThe following options can be used to change some of the characteristics of the VM that you are creating:\n--memory\n: Amount of RAM in megabytes. Default: 512.\n--disk\n: Size of the OS disk in gigabytes. Default: 8.\n--cpu\n: Number of CPU cores. Default: 1.\nSome other parameters will have an impact on the cloud-init configuration:\n--password\n<password>\n: Allows logging into the VM using the Ubuntu account and this provided password.\n--run-script-once\n<script_file>\n: Run\nscript_file\nas root on the VM the first time it is booted, but never again.\n--packages\n<package_list>\n: Install the comma-separated packages specified in\npackage_list\non first boot.\nA complete description of all available modifiers is available in the\nuvt-kvm(1)\nmanpages.\nResources\n¶\nIf you are interested in learning more, have questions or suggestions, please contact the Ubuntu Server Team at:\nIRC:\n#ubuntu-server\non Libera\nMailing list:\nubuntu-server at lists.ubuntu.com", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:32Z", "original_len_words": 924}}
{"id": "7f1d5fa6b5", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/enable-nested-virtualisation/", "title": "How to enable nested virtualisation - Ubuntu Server documentation", "text": "How to enable nested virtualisation - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to enable nested virtualisation\n¶\nImportant\nNested virtualisation is enabled by default on Ubuntu. If you are using Ubuntu, it’s unlikely that you will need to manually enable the feature. If you check (using the steps below) and discover that nested virtualisation is enabled, then you will not need to do anything further.\nThere may be use cases where you need to enable nested virtualisation so that you can deploy instances inside other instances. The sections below explain how to check if nested virtualisation is enabled/available and how to enable it if that is not the case. Bear in mind that currently nested virtualisation is only supported in Ubuntu on\nx86\nmachine architecture.\nCheck if nested virtualisation is enabled\n¶\nCheck if the required kernel module for your CPU is already loaded. Hosts with Intel CPUs require the\nkvm_intel\nmodule while\nAMD\nhosts require\nkvm_amd\ninstead:\n$\nlsmod\n|\ngrep\n-i\nkvm\nkvm_intel\n204800\n0\nkvm\n1347584\n1\nkvm_intel\nIf the module is loaded\n¶\nIf the module is already loaded, you can check if nested virtualisation is enabled by running the following command:\ncat\n/sys/module/<module>/parameters/nested\nAs an example for AMD hosts:\n$\ncat\n/sys/module/kvm_amd/parameters/nested\n1\nIf the output is either\n1\nor\nY\nthen nested virtualisation is enabled and you will not need to manually enable the feature (this should be the case for Ubuntu users).\nIf the module is not loaded\n¶\nIf the module your host requires is not loaded, you can load it using\nmodprobe\nin combination with the property\nnested=1\nto enable nested virtualisation, as shown below for Intel hosts:\nmodprobe\nkvm-intel\nnested\n=\n1\nOr as follows for AMD hosts:\nmodprobe\nkvm-amd\nnested\n=\n1\nEnable nested virtualisation\n¶\nIf the above checks indicate that nested virtualisation is not enabled, you can follow the below steps to enable it.\nCreate a file in\n/etc/modprobe.d\n-e.g.,\n/etc/modprobe.d/kvm.conf\n- and add the line\noptions\nkvm-intel\nnested=1\nto that file (replace\nkvm-intel\nwith\nkvm-amd\nfor AMD hosts).\nReload the kernel module to apply the changes:\nsudo\nmodprobe\n-r\n<module>\nExample for Intel hosts:\nsudo\nmodprobe\n-r\nkvm-intel\nYou should now be able to see nested virtualisation enabled:\nExample for Intel hosts:\n$\ncat\n/sys/module/kvm_intel/parameters/nested\nY\nCheck and enable nested virtualisation inside an instance\n¶\nOnce the host is ready to use nested virtualisation, it is time to check if the guest instance can run additional nested VMs inside it.\nTo determine if an instance can host another instance on top, run the below command within the instance:\negrep\n\"svm|vmx\"\n/proc/cpuinfo\nIf any of these are present in the output (depending on whether the host is AMD or Intel respectively), then virtualisation is available in that instance. If this is not the case you will need to edit the instance CPU settings:\nShut down the instance\nEdit the instance XML definition file executing:\nvirsh\nedit\n<instance>\nSearch the\ncpu\nmode\nparameter in and set its value to either\nhost-model\nor\nhost-passthrough\n(details about these modes can be found\nhere\n).\nSample\ncpu\nmode\nparameter in XML with nested virtualisation:\n<cpu\nmode=\n'host-model'\ncheck=\n'partial'\n/>\nSave the modifications and start the instance\nLimitations of nested virtualisation\n¶\nNested virtualisation has some key limitations you’d need to consider, namely that not all KVM features will be available for instances running nested VMs, and actions such as migrating or saving the parent instance will not be possible until the nested instance is stopped.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:33Z", "original_len_words": 609}}
{"id": "8cbbcb1c32", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/libvirt/", "title": "Libvirt - Ubuntu Server documentation", "text": "Libvirt - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nLibvirt\n¶\nThe\nlibvirt library\nis used to interface with many different virtualisation technologies. Before getting started with libvirt it is best to make sure your hardware supports the necessary virtualisation extensions for\nKernel-based Virtual Machine (KVM)\n. To check this, enter the following from a terminal prompt:\nkvm-ok\nA message will be printed informing you if your CPU\ndoes\nor\ndoes not\nsupport hardware virtualisation.\nNote\nOn many computers with processors supporting hardware-assisted virtualisation, it is necessary to first activate an option in the BIOS to enable it.\nVirtual networking\n¶\nThere are a few different ways to allow a virtual machine access to the external network. The default virtual network configuration includes\nbridging\nand\niptables\nrules implementing\nusermode\nnetworking, which uses the\nSLiRP\nprotocol. Traffic is NATed through the host interface to the outside network.\nTo enable external hosts to directly access services on virtual machines, a different type of\nbridge\nthan the default needs to be configured. This allows the virtual interfaces to connect to the outside network through the physical interface, making them appear as normal hosts to the rest of the network.\nThere is a great example of\nhow to configure a bridge\nand combine it with libvirt so that guests will use it at the\nnetplan.io documentation\n.\nInstall libvirt\n¶\nTo install the necessary packages, from a terminal prompt enter:\nsudo\napt\nupdate\nsudo\napt\ninstall\nqemu-kvm\nlibvirt-daemon-system\nAfter installing\nlibvirt-daemon-system\n, the user that will be used to manage virtual machines needs to be added to the\nlibvirt\ngroup. This is done automatically for members of the\nsudo\ngroup, but needs to be done in addition for anyone else that should access system-wide libvirt resources. Doing so will grant the user access to the advanced networking options.\nIn a terminal enter:\nsudo\nadduser\n$USER\nlibvirt\nNote\nIf the chosen user is the current user, you will need to log out and back in for the new group membership to take effect.\nYou are now ready to install a\nGuest\noperating system. Installing a virtual machine follows the same process as installing the operating system directly on the hardware.\nYou will need\none\nof the following:\nA way to automate the installation.\nA keyboard and monitor attached to the physical machine.\nTo use cloud images which are meant to self-initialize (see\nMultipass\nand\nUVTool\n).\nIn the case of virtual machines, a\nGraphical User Interface (GUI)\nis analogous to using a physical keyboard and mouse on a real computer. Instead of installing a GUI the\nvirt-viewer\nor\nvirt-manager\napplication can be used to connect to a virtual machine’s console using VNC. See\nVirtual Machine Manager / Viewer\nfor more information.\nVirtual machine management\n¶\nThe following section covers\nvirsh\n, a virtual machine management tool that is a part of libvirt. But there are other tools available at different levels of complexities and feature-sets, like:\nMultipass\nUVTool\nvirt-* tools\nOpenStack\nManage VMs with\nvirsh\n¶\nThere are several utilities available to manage virtual machines and libvirt. The\nvirsh\nutility can be used from the command line. Some examples:\nTo list running virtual machines:\nvirsh\nlist\nTo start a virtual machine:\nvirsh\nstart\n<guestname>\nSimilarly, to start a virtual machine at boot:\nvirsh\nautostart\n<guestname>\nReboot a virtual machine with:\nvirsh\nreboot\n<guestname>\nThe\nstate\nof virtual machines can be saved to a file in order to be restored later. The following will save the virtual machine state into a file named according to the date:\nvirsh\nsave\n<guestname>\nsave-my.state\nOnce saved, the virtual machine will no longer be running.\nA saved virtual machine can be restored using:\nvirsh\nrestore\nsave-my.state\nTo shut down a virtual machine you can do:\nvirsh\nshutdown\n<guestname>\nA CD-ROM device can be mounted in a virtual machine by entering:\nvirsh\nattach-disk\n<guestname>\n/dev/cdrom\n/media/cdrom\nTo change the definition of a guest,\nvirsh\nexposes the domain via:\nvirsh\nedit\n<guestname>\nThis will allow you to edit the\nXML representation that defines the guest\n. When saving, it will apply format and integrity checks on these definitions.\nEditing the XML directly certainly is the most powerful way, but also the most complex one. Tools like\nVirtual Machine Manager / Viewer\ncan help inexperienced users to do most of the common tasks.\nNote\nIf\nvirsh\n(or other\nvir*\ntools) connect to something other than the default\nqemu-kvm\n/system hypervisor, one can find alternatives for the\n--connect\noption using\nman\nvirsh\nor the\nlibvirt docs\n.\nsystem\nand\nsession\nscope\n¶\nYou can pass connection strings to\nvirsh\n- as well as to most other tools for managing virtualisation.\nvirsh\n--connect\nqemu:///system\nThere are two options for the connection.\nqemu:///system\n- connect locally as\nroot\nto the daemon supervising QEMU and KVM domains\nqemu:///session\n- connect locally as a\nnormal user\nto their own set of QEMU and KVM domains\nThe\ndefault\nwas always (and still is)\nqemu:///system\nas that is the behavior most users are accustomed to. But there are a few benefits (and drawbacks) to\nqemu:///session\nto consider.\nqemu:///session\nis per user and can – on a multi-user system – be used to separate the people.\nMost importantly, processes run under the permissions of the user, which means no permission struggle on the just-downloaded image in your\n$HOME\nor the just-attached USB-stick.\nOn the other hand it can’t access system resources very well, which includes network setup that is known to be hard with\nqemu:///session\n. It falls back to\nSLiRP networking\nwhich is functional but slow, and makes it impossible to be reached from other systems.\nqemu:///system\nis different in that it is run by the global system-wide libvirt that can arbitrate resources as needed. But you might need to\nmv\nand/or\nchown\nfiles to the right places and change permissions to make them usable.\nApplications will usually decide on their primary use-case. Desktop-centric applications often choose\nqemu:///session\nwhile most solutions that involve an administrator anyway continue to default to\nqemu:///system\n.\nSee also\nThere is more information about this topic in the\nlibvirt FAQ\nand this\nblog post\nabout the topic.\nMigration\n¶\nThere are different types of migration available depending on the versions of libvirt and the hypervisor being used. In general those types are:\nOffline migration\nLive migration\nPostcopy migration\nThere are various options to those methods, but the entry point for all of them is\nvirsh\nmigrate\n. Read the integrated help for more detail.\nvirsh\nmigrate\n--help\nSome useful documentation on the constraints and considerations of live migration can be found at the\nUbuntu Wiki\n.\nCPU model and topology\n¶\nlibvirt abstracts CPU configuration and provides several options to specify the VM CPU model in the domain definition:\ncustom\nhost-model\nhost-passthrough\nmaximum\nWhile most of thee modes are straightforward, the behavior of\nhost-model\nis more subtle and the source of many common misunderstandings. There is no direct\ntranslation of\nhost-model\ninto a single QEMU\n-cpu\nargument. Instead, libvirt selects a baseline CPU model and appends\na list of features:\n# example snippet from qemu command line generated by libvirt\n...\n-cpu\nHaswell-noTSX-IBRS,vmx\n=\non,pdcm\n=\noff,...,vmx-entry-load-efer\n=\non,vmx-eptp-switching\n=\non\n...\nBecause libvirt can not include every known CPU model and variant, it chooses the model that shares the largest set of features with\nthe host’s physical CPU and then lists the remaining features explicitly. In many cases, libvirt therefore can not detect the\nexact host CPU model. At first this may seem like a flaw, but in practice, it is not necessary to know the exact model.\nFor example, running\nvirsh\ncapabilities\non a host with an Intel\nBroadwell CPU\nmay produce the following output,\nwhere libvirt uses\nHaswell-noTSX-IBRS\nas the baseline:\n<capabilities>\n<host>\n<uuid>\n30303837-3831-584d-5135-323430354a38\n</uuid>\n<cpu>\n<arch>\nx86_64\n</arch>\n<model>\nHaswell-noTSX-IBRS\n</model>\n<vendor>\nIntel\n</vendor>\n<microcode\nversion=\n'73'\n/>\n<signature\nfamily=\n'6'\nmodel=\n'63'\nstepping=\n'2'\n/>\n<counter\nname=\n'tsc'\nfrequency=\n'2397195000'\nscaling=\n'no'\n/>\n<topology\nsockets=\n'1'\ndies=\n'1'\ncores=\n'6'\nthreads=\n'2'\n/>\n<maxphysaddr\nmode=\n'emulate'\nbits=\n'46'\n/>\n<feature\nname=\n'vme'\n/>\n<feature\nname=\n'ds'\n/>\n<feature\nname=\n'acpi'\n/>\n<feature\nname=\n'ss'\n/>\nThis mismatch between the baseline model reported by libvirt and the actual physical CPU model is not a bug and can safely be ignored.\nFor more details, refer to the\nupstream documentation\n.\nHost CPU capabilities\n¶\nDevice passthrough/hotplug\n¶\nIf you want to always pass through a device rather than using the hotplugging method described here, add the XML content of the device to your static guest XML representation via\nvirsh\nedit\n<guestname>\n. In that case, you won’t need to use\nattach/detach\n. There are different kinds of passthrough, and the types available to you depend on your hardware and software setup.\nUSB\nhotplug\n/passthrough\nVF hotplug/Passthrough\nBoth kinds are handled in a very similar way and while there are various way to do it (e.g. also via QEMU monitor), driving such a change via libvirt is recommended. That way, libvirt can try to manage all sorts of special cases for you and also somewhat masks version differences.\nIn general, when driving hotplug via libvirt, you create an XML snippet that describes the device just as you would do in a static\nguest description\n. A USB device is usually identified by vendor/product ID:\n<hostdev\nmode=\n'subsystem'\ntype=\n'usb'\nmanaged=\n'yes'\n>\n<source>\n<vendor\nid=\n'0x0b6d'\n/>\n<product\nid=\n'0x3880'\n/>\n</source>\n</hostdev>\nVirtual functions are usually assigned via their PCI ID (domain, bus, slot, and function).\n<hostdev\nmode=\n'subsystem'\ntype=\n'pci'\nmanaged=\n'yes'\n>\n<source>\n<address\ndomain=\n'0x0000'\nbus=\n'0x04'\nslot=\n'0x10'\nfunction=\n'0x0'\n/>\n</source>\n</hostdev>\nNote\nGetting the virtual function in the first place is very device-dependent and can, therefore, not be fully covered here. But in general, it involves setting up an\nIOMMU\n, registering via\nVFIO\nand sometimes requesting a number of VFs.\nHere is an example of configuring a\nppc64el\nsystem to create four VFs on a device:\n$\nsudo\nmodprobe\nvfio-pci\n# identify device\n$\nlspci\n-n\n-s\n0005\n:01:01.3\n0005\n:01:01.3\n0200\n:\n10df:e228\n(\nrev\n10\n)\n# register and request VFs\n$\necho\n10df\ne228\n|\nsudo\ntee\n/sys/bus/pci/drivers/vfio-pci/new_id\n$\necho\n4\n|\nsudo\ntee\n/sys/bus/pci/devices/0005\n\\:\n01\n\\:\n00\n.0/sriov_numvfs\nYou then attach or detach the device via libvirt by relating the guest with the XML snippet.\nvirsh\nattach-device\n<guestname>\n<device-xml>\n# Use the Device in the Guest\nvirsh\ndetach-device\n<guestname>\n<device-xml>\nAccess QEMU Monitor via libvirt\n¶\nThe\nQEMU Monitor\nis the way to interact with QEMU/KVM while a guest is running. This interface has many powerful features for experienced users. When running under libvirt, the monitor interface is bound by libvirt itself for management purposes, but a user can still run QEMU monitor commands via libvirt. The general syntax is\nvirsh\nqemu-monitor-command\n[options]\n[guest]\n'command'\n.\nLibvirt covers most use cases needed, but if you ever want/need to work around libvirt or want to tweak very special options you can e.g. add a device as follows:\nvirsh\nqemu-monitor-command\n--hmp\nfocal-test-log\n'drive_add 0 if=none,file=/var/lib/libvirt/images/test.img,format=raw,id=disk1'\nThe monitor is a power tool, especially for debugging purposes. For example, one can use the monitor to show the guest registers:\n$\nvirsh\nqemu-monitor-command\n--hmp\ny-ipns\n'info registers'\nRAX\n=\n00ffffc000000000\nRBX\n=\nffff8f0f5d5c7e48\nRCX\n=\n0000000000000000\nRDX\n=\nffffea00007571c0\nRSI\n=\n0000000000000000\nRDI\n=\nffff8f0fdd5c7e48\nRBP\n=\nffff8f0f5d5c7e18\nRSP\n=\nffff8f0f5d5c7df8\n[\n...\n]\nHuge pages\n¶\nLet’s start with a summary of the allocation and structuring of memory in an OS, and its relationship to transparent huge pages and\nhuge pages\n.\nWhen you launch an application, the OS allocates virtual memory to it as a range of virtual addresses. The virtual memory is fake; it only allows the application to think it has more memory than what’s physically available.\nHowever, the CPU architecture will determine the amount of virtual memory allocated to the application;\n64-bit CPUs\nsupport 16 EB, whereas\n32-bit CPUs\nsupport 4 GB.\nx86_64\n, however, typically supports only 256 TB of virtual memory.\nThe OS splits the virtual memory into pages (4 KB each). A page contains several virtual addresses (\n1 byte each\n). These pages contain different parts of the application, like the code, data, stack, and heap. The OS maps these pages to real memory (RAM) because the virtual memory is a fake.\nAlthough the OS allocates the virtual memory, the range of virtual addresses is generated by the CPU whenever the application accesses memory (e.g., reading a variable, fetching an instruction, or writing data). The\nMemory Management Unit (MMU)\nreceives these virtual addresses and uses the page table to convert them into physical addresses.\nWith a 4 KB page size in the table, a large application having 200 MB in size, for example, could have thousands of pages. So, constantly looking up the page table in RAM would be slow. To briefly fix this, the CPU uses a\nTranslation Lookaside Buffer (TLB)\nto cache recent page table entries to speed up memory access; however, the TLB can only hold a certain number of entries.\nTo reduce this overhead, you can use huge pages, which increase the page size from 4 KB to larger sizes (e.g., 2 MB or 1 GB). This reduces the number of page table entries in the TLB, making lookups faster.\nHuge pages must frequently be\npre-allocated\non the host for Libvirt to\nmap the VMs memory to the host\n. This is because VMs rely on two layers of address translation — one for the VM and one for the host - so expect memory lookups to be CPU-intensive.\nSince huge pages can be configured on Libvirt, the page table entries are reduced, hence, memory access from the VM speeds up.\nIt’s now clear how huge pages can lessen page table entries and TLB overhead.\nHuge pages can have some disadvantages too, as they frequently require manual setup. To address this,\ntransparent huge pages\nare used to manage pages dynamically.\nThe dynamic page resizing can also be an issue when using libvirt since huge pages have to be pre-allocated. So, if it is clear that using huge pages is preferred, then making them explicit usually has some gains.\nWhile huge pages are admittedly harder to manage (especially later in the system’s lifetime if memory is fragmented), they provide a useful boost, especially for rather large guests.\nTip\nWhen using device passthrough on very large guests, there is an extra benefit of using huge pages, as it is faster to do the initial memory clear on the VFIO\nDMA\npin.\nHuge page allocation\n¶\nHuge pages come in different sizes. A\nnormal\npage is usually 4k and huge pages are either 2M or 1G, but depending on the architecture, other options are possible.\nThe simplest yet least reliable way to allocate some huge pages is to just echo a value to\nsysfs\n:\necho\n256\n|\nsudo\ntee\n/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages\nBe sure to re-check if it worked:\n$\ncat\n/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages\n256\nThere one of these sizes is the “default huge page size”, which will be used in the auto-mounted\n/dev/hugepages\n. Changing the default size requires a reboot and is set via\ndefault_hugepagesz\n.\nYou can check the current default size:\n$\ngrep\nHugepagesize\n/proc/meminfo\n\nHugepagesize:\n2048\nkB\nBut there can be more than one at the same time – so it’s a good idea to check:\n$\ntail\n/sys/kernel/mm/hugepages/hugepages-*/nr_hugepages\n`\n==\n>\n/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n<\n==\n0\n==\n>\n/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages\n<\n==\n2\nAnd even that could – on bigger systems – be further split per\nNuma node\n.\nOne can allocate huge pages at\nboot or runtime\n, but due to fragmentation there are no guarantees it works later. The\nkernel documentation\nlists details on both ways.\nHuge pages need to be allocated by the kernel as mentioned above, but to be consumable, they also have to be mounted. By default,\nsystemd\nwill make\n/dev/hugepages\navailable for the default huge page size.\nFeel free to add more mount points if you need different sized ones. An overview can be queried with\nhugeadm\n:\n$\napt\ninstall\nlibhugetlbfs-bin\n$\nhugeadm\n--list-all-mounts\n\nMount\nPoint\nOptions\n/dev/hugepages\nrw,relatime,pagesize\n=\n2M\nA one-stop info for the overall huge page status of the system can be reported with:\nhugeadm\n--explain\nHuge page usage in libvirt\n¶\nWith the above in place, libvirt can map guest memory to huge pages. In a guest definition add the most simple form of:\n<memoryBacking>\n<hugepages/>\n</memoryBacking>\nThat will allocate the huge pages using the default huge page size from an autodetected mount point.\nFor more control, e.g. how memory is spread over\nNuma nodes\nor which page size to use, check out the details at the\nlibvirt docs\n.\nControlling addressing bits\n¶\nThis is a topic that rarely matters on a single computer with virtual machines for generic use; libvirt will automatically use the hypervisor default, which in the case of QEMU is 40 bits. This default aims for compatibility since it will be the same on all systems, which simplifies migration between them and usually is compatible even with older hardware.\nHowever, it can be very important when driving more advanced use cases. If one needs bigger guest sizes with more than a terabyte of memory then controlling the addressing bits is crucial.\n-hpb machine types\n¶\nSince Ubuntu 18.04, the QEMU in Ubuntu has\nprovided special machine-types\n. These include machine types like\npc-q35-jammy\nor\npc-i440fx-jammy\n, but with a\n-hpb\nsuffix. The “\nHPB\n” abbreviation stands for “host-physical-bits”, which is the QEMU option that this represents.\nFor example, by using\npc-q35-jammy-hpb\n, the guest would use the number of physical bits that the Host CPU has available.\nProviding the configuration that a guest should use more address bits as a machine type has the benefit that many higher level management stacks like for example openstack, are already able to control it through libvirt.\nOne can check the bits available to a given CPU via the procfs:\n$\ncat\n/proc/cpuinfo\n|\ngrep\n'^address sizes'\n...\n# an older server with a E5-2620\naddress\nsizes\n:\n46\nbits\nphysical,\n48\nbits\nvirtual\n# a laptop with an i7-8550U\naddress\nsizes\n:\n39\nbits\nphysical,\n48\nbits\nvirtual\nmaxphysaddr guest configuration\n¶\nSince libvirt version 8.7.0 (>= Ubuntu 22.10 Lunar),\nmaxphysaddr\ncan be controlled via the\nCPU model and topology section\nof the guest configuration.\nIf one needs just a large guest, like before when using the\n-hpb\ntypes, all that is needed is the following libvirt guest xml configuration:\n<maxphysaddr\nmode=\n'passthrough'\n/>\nSince libvirt 9.2.0 and 9.3.0 (>= Ubuntu 23.10 Mantic), an explicit number of emulated bits or a limit to the passthrough can be specified. Combined, this pairing can be very useful for computing clusters where the CPUs have different hardware physical addressing\nbits. Without these features, guests could be large, but potentially unable to migrate freely between all nodes since not all systems would support the same amount of addressing bits.\nBut now, one can either set a fixed value of addressing bits:\n<maxphysaddr\nmode=\n'emulate'\nbits=\n'42'\n/>\nOr use the best available by a given hardware, without going over a certain limit to retain some compute node compatibility.\n<maxphysaddr\nmode=\n'passthrough'\nlimit=\n'41/\n>\nAppArmor isolation\n¶\nBy default, libvirt will spawn QEMU guests using AppArmor isolation for enhanced security. The\nAppArmor rules for a guest\nwill consist of multiple elements:\nA static part that all guests share =>\n/etc/apparmor.d/abstractions/libvirt-qemu\nA dynamic part created at guest start time and modified on hotplug/unplug =>\n/etc/apparmor.d/libvirt/libvirt-f9533e35-6b63-45f5-96be-7cccc9696d5e.files\nOf the above, the former is provided and updated by the\nlibvirt-daemon\npackage, and the latter is generated on guest start. Neither of the two should be manually edited. They will, by default, cover the vast majority of use cases and work fine. But there are certain cases where users either want to:\nFurther lock down the guest, e.g. by explicitly denying access that usually would be allowed.\nOpen up the guest isolation. Most of the time this is needed if the setup on the local machine does not follow the commonly used paths.\nTo do so there are two files. Both are local overrides which allow you to modify them without getting them clobbered or command file prompts on package upgrades.\n/etc/apparmor.d/local/abstractions/libvirt-qemu\nThis will be applied to every guest. Therefore it is a rather powerful (if blunt) tool. It is a quite useful place to add additional\ndeny rules\n.\n/etc/apparmor.d/local/usr.lib.libvirt.virt-aa-helper\nThe above-mentioned\ndynamic part\nthat is individual per guest is generated by a tool called\nlibvirt.virt-aa-helper\n. That is under AppArmor isolation as well. This is most commonly used if you want to use uncommon paths as it allows one to have those uncommon paths in the\nguest XML\n(see\nvirsh\nedit\n) and have those paths rendered to the per-guest dynamic rules.\nSharing files between Host<->Guest\n¶\nTo be able to exchange data, the memory of the guest has to be allocated as “shared”. To do so you need to add the following to the guest config:\n<memoryBacking>\n<access\nmode=\n'shared'\n/>\n</memoryBacking>\nFor performance reasons (it helps\nvirtiofs\n, but also is generally wise to consider) it\nis recommended to use huge pages which then would look like:\n<memoryBacking>\n<hugepages>\n<page\nsize=\n'2048'\nunit=\n'KiB'\n/>\n</hugepages>\n<access\nmode=\n'shared'\n/>\n</memoryBacking>\nIn the guest definition, one then can add\nfilesystem\nsections to specify host paths to share with the guest. The\ntarget dir\nis a bit special as it isn’t really a directory – instead, it is a\ntag\nthat in the guest can be used to access this particular\nvirtiofs\ninstance.\n<filesystem\ntype=\n'mount'\naccessmode=\n'passthrough'\n>\n<driver\ntype=\n'virtiofs'\n/>\n<source\ndir=\n'/var/guests/h-virtiofs'\n/>\n<target\ndir=\n'myfs'\n/>\n</filesystem>\nAnd in the guest, this can now be used based on the tag\nmyfs\nlike:\nsudo\nmount\n-t\nvirtiofs\nmyfs\n/mnt/\nCompared to other Host/Guest file sharing options – commonly Samba, NFS, or 9P –\nvirtiofs\nis usually much faster and also more compatible with usual file system semantics.\nSee the\nlibvirt domain/filesystem\ndocumentation for further details on these.\nNote\nWhile\nvirtiofs\nworks with >=20.10 (Groovy), with >=21.04 (Hirsute) it became more comfortable, especially in small environments (no hard requirement to specify guest Numa topology, no hard requirement to use huge pages). If needed to set up on 20.10 or just interested in those details - the libvirt\nknowledge-base about virtiofs\nholds more details about these.\nResources\n¶\nSee the\nKVM home page\nfor more details.\nFor more information on libvirt see the\nlibvirt home page\n.\nXML configuration of\ndomains\nand\nstorage\nare the most often used libvirt reference.\nAnother good resource is the\nUbuntu Wiki KVM\npage.\nFor basics on how to assign VT-d devices to QEMU/KVM, please see the\nlinux-kvm\npage.\nIntroduction to Memory Management in Linux", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:33Z", "original_len_words": 3829}}
{"id": "00781582ac", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/multipass/", "title": "How to create a VM with Multipass - Ubuntu Server documentation", "text": "How to create a VM with Multipass - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to create a VM with Multipass\n¶\nMultipass\nis the recommended method for creating Ubuntu VMs on Ubuntu. It’s designed for developers who want a fresh Ubuntu environment with a single command, and it works on Linux, Windows and macOS.\nOn Linux it’s available as a snap:\nsudo\nsnap\ninstall\nmultipass\nIf you’re running an older version of Ubuntu where\nsnapd\nisn’t pre-installed, you will need to install it first:\nsudo\napt\nupdate\nsudo\napt\ninstall\nsnapd\nFind available images\n¶\nTo find available images you can use the\nmultipass\nfind\ncommand, which will produce a list like this:\nImage                       Aliases           Version          Description\nsnapcraft:core18            18.04             20201111         Snapcraft builder for Core 18\nsnapcraft:core20            20.04             20210921         Snapcraft builder for Core 20\nsnapcraft:core22            22.04             20220426         Snapcraft builder for Core 22\nsnapcraft:devel                               20221128         Snapcraft builder for the devel series\ncore                        core16            20200818         Ubuntu Core 16\ncore18                                        20211124         Ubuntu Core 18\n18.04                       bionic            20221117         Ubuntu 18.04 LTS\n20.04                       focal             20221115.1       Ubuntu 20.04 LTS\n22.04                       jammy,lts         20221117         Ubuntu 22.04 LTS\n22.10                       kinetic           20221101         Ubuntu 22.10\ndaily:23.04                 devel,lunar       20221127         Ubuntu 23.04\nappliance:adguard-home                        20200812         Ubuntu AdGuard Home Appliance\nappliance:mosquitto                           20200812         Ubuntu Mosquitto Appliance\nappliance:nextcloud                           20200812         Ubuntu Nextcloud Appliance\nappliance:openhab                             20200812         Ubuntu openHAB Home Appliance\nappliance:plexmediaserver                     20200812         Ubuntu Plex Media Server Appliance\nanbox-cloud-appliance                         latest           Anbox Cloud Appliance\ncharm-dev                                     latest           A development and testing environment for charmers\ndocker                                        latest           A Docker environment with Portainer and related tools\njellyfin                                      latest           Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media.\nminikube                                      latest           minikube is local Kubernetes\nLaunch a fresh instance of the Ubuntu Jammy (22.04) LTS\n¶\nYou can launch a fresh instance by specifying either the image name from the list (in this example, 22.04) or using an alias, if the image has one.\n$\nmultipass\nlaunch\n22\n.04\nLaunched:\ncleansing-guanaco\nThis command is equivalent to:\nmultipass\nlaunch\njammy\nor\nmultipass\nlaunch\nlts\nin the list above. It will launch an instance based on the specified image, and provide it with a random name – in this case,\ncleansing-guanaco\n.\nCheck out the running instances\n¶\nYou can check out the currently running instance(s) by using the\nmultipass\nlist\ncommand:\n$\nmultipass\nlist\nName\nState\nIPv4\nImage\ncleansing-guanaco\nRunning\n10\n.140.26.17\nUbuntu\n22\n.04\nLTS\nLearn more about the VM instance you just launched\n¶\nYou can use the\nmultipass\ninfo\ncommand to find out more details about the VM instance parameters:\n$\nmultipass\ninfo\ncleansing-guanaco\nName:\ncleansing-guanaco\nState:\nRunning\nIPv4:\n10\n.140.26.17\nRelease:\nUbuntu\n22\n.04.1\nLTS\nImage\nhash:\ndc5b5a43c267\n(\nUbuntu\n22\n.04\nLTS\n)\nLoad:\n0\n.45\n0\n.19\n0\n.07\nDisk\nusage:\n1\n.4G\nout\nof\n4\n.7G\nMemory\nusage:\n168\n.3M\nout\nof\n969\n.5M\nMounts:\n--\nConnect to a running instance\n¶\nTo enter the VM you created, use the\nshell\ncommand:\n$\nmultipass\nshell\ncleansing-guanaco\nWelcome\nto\nUbuntu\n22\n.04.1\nLTS\n(\nGNU/Linux\n5\n.15.0-53-generic\nx86_64\n)\n(\n...\n)\nubuntu@cleansing-guanaco:~$\nDisconnect from the instance\n¶\nDon’t forget to log out (or\nCtrl\n+\nD\n) when you are done, or you may find yourself heading all the way down the Inception levels…\nRun commands inside an instance from outside\n¶\n$\nmultipass\nexec\ncleansing-guanaco\n--\nlsb_release\n-a\nNo\nLSB\nmodules\nare\navailable.\nDistributor\nID:\nUbuntu\nDescription:\nUbuntu\n22\n.04.1\nLTS\nRelease:\n22\n.04\nCodename:\njammy\nStop or start an instance\n¶\nYou can stop an instance to save resources using the\nstop\ncommand:\n$\nmultipass\nstop\ncleansing-guanaco\nYou can start it back up again using the\nstart\ncommand:\n$\nmultipass\nstart\ncleansing-guanaco\nDelete the instance\n¶\nOnce you are finished with the instance, you can delete it as follows:\n$\nmultipass\ndelete\ncleansing-guanaco\nIt will now show up as deleted when you use the\nlist\ncommand:\n$\nmultipass\nlist\nName\nState\nIPv4\nImage\ncleansing-guanaco\nDeleted\n--\nNot\nAvailable\nAnd when you want to completely get rid of it (and any other deleted instances), you can use the\npurge\ncommand:\n$\nmultipass\npurge\nWhich we can check again using\nlist\n:\n$\nmultipass\nlist\nNo\ninstances\nfound.\nIntegrate with the rest of your virtualisation\n¶\nIf you already have a hypervisor interacting with\nLibvirt\n, such as\nQEMU\n,\nKVM\n, or\nESXi\n, you might\nbe managing virtual machines through tools like\nvirt-manager\nor the older\nuvtool\n.\nIn that case, integrating Multipass with your existing setup would allow VMs to share the same network bridge for communication\nand be managed using virsh. However, Multipass runs as a headless system, so you don’t have direct GUI access through virt-viewer. Follow this\nguide\nto set up a GUI.\nTo begin, integrate Multipass into your existing setup by selecting libvirt as your local driver:\n$\nsudo\nmultipass\nset\nlocal.driver\n=\nlibvirt\nNote\nIf you are having issues interacting with Multipass after switching to the libvirt driver, check if there is a restriction by\nAppArmor\n, for example. AppArmor may have a default policy which restricts the multipass service from interacting with the\nlibvert service\n. So you need to add an explicit permission that allows it.\nStart a guest, and access it via tools like\nvirt-manager\nor\nvirsh\n:\n$\nmultipass\nlaunch\nlts\nLaunched:\nengaged-amberjack\n\n$\nvirsh\nlist\nId\nName\nState\n----------------------------------------------------\n15\nengaged-amberjack\nrunning\nFor more detailed and comprehensive instructions on changing your drivers, refer to the\nMultipass drivers documentation\n.\nGet help\n¶\nYou can use the following commands on the CLI:\nmultipass\nhelp\nmultipass\nhelp\n<command>\nmultipass\nhelp\n--all\nOr, check out the\nMultipass documentation\nfor more details on how to use it.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:33Z", "original_len_words": 958}}
{"id": "5e21bca42c", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/qemu/", "title": "QEMU - Ubuntu Server documentation", "text": "QEMU - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nQEMU\n¶\nNote\nPlease bear in mind that invoking QEMU manually may sometimes require your user to be part of the\nkvm\ngroup.\nVirtualisation with QEMU\n¶\nQEMU\nis a machine emulator that can run operating systems and programs for one machine on a different machine. However, it is more often used as a virtualizer in collaboration with\nKVM\nkernel components. In that case it uses the hardware virtualization technology to virtualize guests.\nAlthough QEMU has a\ncommand line interface\nand a\nmonitor\nto interact with running guests, they are typically only used for development purposes. On the other hand,\nlibvirt\nprovides an abstraction from specific versions and hypervisors, and encapsulates some workarounds and best practices.\nInstall QEMU/KVM\n¶\nThe first step to using QEMU/KVM on Ubuntu is to check if your system supports KVM.\nkvm-ok\nYou should get an output saying\nKVM\nacceleration\ncan\nbe\nused\n.\nThe next step is to install QEMU.\nsudo\napt-get\ninstall\nqemu-system\nBoot a VM\n¶\nThe quickest way to get started with QEMU is by booting a VM directly from a netboot ISO. You can achieve this by running the following command:\nqemu-system-x86_64\n-enable-kvm\n-cdrom\nhttp://archive.ubuntu.com/ubuntu/dists/bionic-updates/main/installer-amd64/current/images/netboot/mini.iso\nCaution\nThis example is just for illustration purposes - it is not generally recommended without verifying the checksums;\nMultipass\nand\nUVTool\nare much better ways to get actual guests easily.\nIf you are testing this example on a headless system, specify an alternative display method such as\nVNC\n.\nCreate a virtual disk\n¶\nThe command in the previous sub-section boots the system entirely in RAM without persistent storage. To maintain the OS state across reboots, you should allocate space for the VM:\nqemu-img\ncreate\n-f\nqcow2\ndisk.qcow\n5G\nAnd then we can use the disk space we have just allocated for storage by adding the argument:\n-drive\nfile=disk.qcow,format=qcow2\n.\nThese tools can do much more, as you’ll discover in their respective (long)\nmanpages\n. They can also be made more consumable for specific use-cases and needs through a vast selection of auxiliary tools - for example\nvirt-manager\nfor UI-driven use through\nlibvirt\n. But in general, it comes down to:\nqemu-system-x86_64\noptions\nimage\n[\ns\n]\nSo take a look at the\nQEMU manpage\n,\nqemu-img(1)\nand the\nQEMU documentation\nand see which options best suit your needs.\nWhile a standard QEMU configuration works for most use cases, some scenarios demand high-vCPU VMs. In the next section, we’ll cover how to create QEMU virtual machines with up to 1024 vCPUs.\nCreate QEMU VMs with up to 1024 vCPUs\n¶\nFor a long time, QEMU only supported launching virtual machines with 288 vCPUs or fewer. While this was acceptable a decade ago, nowadays it is more common to see processors with 300+ physical cores available. For this reason, QEMU has been modified to support virtual machines with up to 1024 vCPUs. The caveat is that the user has to provide a few specific (and not trivial to guess) command line options to enable such a feature, and that is the gap that this document aims to fill.\nRequirement\n¶\nTo support more than 288 vCPUs, some QEMU versions are only compatible with special machine types.\nQEMU version\nUbuntu release\nSupported machine types\nQEMU 8.2.1+\nUbuntu 24.04 LTS (Noble)\nubuntu\n(Native support, also supports Jammy and Mantic machine types)\nQEMU 8.0.4\nUbuntu 23.10 (Mantic)\npc-q35-mantic-maxcpus\n,\npc-i440fx-mantic-maxcpus\n(Also supports Jammy machine types)\nQEMU 6.2\nUbuntu 22.04 LTS (Jammy)\npc-q35-jammy-maxcpus\n,\npc-i440fx-jammy-maxcpus\nConfiguration by Ubuntu release\n¶\n24.04 Noble\nFrom Noble onwards, the regular\nubuntu\nmachine type supports up to 1024 vCPUs out of the box, which simplifies the command used to create such virtual machines:\nqemu\n-\nsystem\n-\nx86_64\n-\nM\nubuntu\n,\naccel\n=\nkvm\n,\nkernel\n-\nirqchip\n=\nsplit\n-\ndevice\nintel\n-\niommu\n,\nintremap\n=\non\n-\nsmp\ncpus\n=\n300\n,\nmaxcpus\n=\n300\n...\nAlthough the regular machine type can now be used to launch the virtual machine, it is still necessary to provide some special command line options to make sure that the VM is created with a virtual IOMMU with interrupt mapping.\nNow that we’ve covered high-vCPU configurations for x86_64 VMs, let’s look at how to boot ARM64 virtual machines on QEMU.\n23.10 Mantic\nIf you are using QEMU on Mantic, the special machine types are named in a similar fashion to Jammy’s:\npc-q35-mantic-maxcpus\nor\npc-i440fx-mantic-maxcpus\n.\nTherefore, you command line to create a virtual machine with support for more than 288 vCPUs on Mantic should start with:\nqemu\n-\nsystem\n-\nx86_64\n-\nM\npc\n-\nq35\n-\nmantic\n-\nmaxcpus\n,\naccel\n=\nkvm\n,\nkernel\n-\nirqchip\n=\nsplit\n-\ndevice\nintel\n-\niommu\n,\nintremap\n=\non\n-\nsmp\ncpus\n=\n300\n,\nmaxcpus\n=\n300\n...\nIn the example above, the virtual machine will be launched using 300 vCPUs and a\npc-q35-mantic-maxcpus\nmachine type. You can adjust the option according to your use case.\nThe\nkernel-irqchip=split\n-device\nintel-iommu,intremap=on\ncommand line options are required, to make sure that the VM is created with a virtual IOMMU with interrupt mapping. This is needed due to some idiosyncrasies present in this scenario.\nNote that both machine types for Mantic are supported in subsequent versions of Ubuntu, so you should be able to migrate your virtual machines to newer versions of QEMU in Ubuntu without problems. As noted in the previous section, it is also possible to create virtual machines using the special Jammy machine types on Mantic.\n22.04 Jammy\nIf you are using QEMU on Jammy and want to create VMs with more than 288 vCPUs, you will need to use either of the special\npc-q35-jammy-maxcpus\nor\npc-i440fx-jammy-maxcpus\nmachine types.\nThe command line needs to start with:\nqemu\n-\nsystem\n-\nx86_64\n-\nM\npc\n-\nq35\n-\njammy\n-\nmaxcpus\n,\naccel\n=\nkvm\n,\nkernel\n-\nirqchip\n=\nsplit\n-\ndevice\nintel\n-\niommu\n,\nintremap\n=\non\n-\nsmp\ncpus\n=\n300\n,\nmaxcpus\n=\n300\n...\nIn the example above, the virtual machine will be launched using 300 vCPUs and a\npc-q35-jammy-maxcpus\nmachine type. You can adjust the option according to your use case.\nThe\nkernel-irqchip=split\n-device\nintel-iommu,intremap=on\ncommand line options are required, to make sure that the VM is created with a virtual IOMMU with interrupt mapping. This is needed due to some idiosyncrasies present in this scenario.\nNote that both machine types for Jammy are supported in subsequent versions of Ubuntu, so you should be able to migrate your virtual machines to newer versions of QEMU in Ubuntu without problems.\nBoot ARM64 virtual machines on QEMU\n¶\nUbuntu ARM64 images can run inside QEMU. You can either do this fully emulated (e.g. on an x86 host) or accelerated with KVM if you have an ARM64 host. This page describes how to do both.\nNote\nThis requires Ubuntu 20.04 or greater\nInstall QEMU to run ARM64 virtual machines\n¶\nThe first step is to install the\nqemu-system-arm\npackage, which needs to be done regardless of where the ARM64 virtual machine will run:\nsudo\napt\ninstall\nqemu-system-arm\nCreate necessary support files\n¶\nNext, create a VM-specific flash volume for storing NVRAM variables, which are necessary when booting\nEFI\nfirmware:\ntruncate\n-s\n64m\nvarstore.img\nWe also need to copy the ARM UEFI firmware into a bigger file:\ntruncate\n-s\n64m\nefi.img\ndd\nif\n=\n/usr/share/qemu-efi-aarch64/QEMU_EFI.fd\nof\n=\nefi.img\nconv\n=\nnotrunc\nFetch the Ubuntu cloud image\n¶\nYou need to fetch the ARM64 variant of the Ubuntu cloud image you would like to use in the virtual machine. You can go to the official\nUbuntu cloud image\nwebsite, select the Ubuntu release, and then download the variant whose filename ends in\n-arm64.img\n. For example, if you want to use the latest Jammy cloud image, you should download the file named\njammy-server-cloudimg-arm64.img\n.\nRun QEMU natively on an ARM64 host\n¶\nIf you have access to an ARM64 host, you should be able to create and launch an ARM64 virtual machine there. Note that the command below assumes that you have already set up a network bridge to be used by the virtual machine.\nqemu-system-aarch64\n\\\n-enable-kvm\n\\\n-m\n1024\n\\\n-cpu\nhost\n\\\n-M\nvirt\n\\\n-nographic\n\\\n-drive\nif\n=\npflash,format\n=\nraw,file\n=\nefi.img,readonly\n=\non\n\\\n-drive\nif\n=\npflash,format\n=\nraw,file\n=\nvarstore.img\n\\\n-drive\nif\n=\nnone,file\n=\njammy-server-cloudimg-arm64.img,id\n=\nhd0\n\\\n-device\nvirtio-blk-device,drive\n=\nhd0\n-netdev\ntype\n=\ntap,id\n=\nnet0\n\\\n-device\nvirtio-net-device,netdev\n=\nnet0\nRun an emulated ARM64 VM on x86\n¶\nYou can also emulate an ARM64 virtual machine on an x86 host. To do that:\nqemu-system-aarch64\n\\\n-m\n2048\n\\\n-cpu\nmax\n\\\n-M\nvirt\n\\\n-nographic\n\\\n-drive\nif\n=\npflash,format\n=\nraw,file\n=\nefi.img,readonly\n=\non\n\\\n-drive\nif\n=\npflash,format\n=\nraw,file\n=\nvarstore.img\n\\\n-drive\nif\n=\nnone,file\n=\njammy-server-cloudimg-arm64.img,id\n=\nhd0\n\\\n-device\nvirtio-blk-device,drive\n=\nhd0\n\\\n-netdev\ntype\n=\ntap,id\n=\nnet0\n\\\n-device\nvirtio-net-device,netdev\n=\nnet0\nTroubleshooting\n¶\nNo output and no response\n¶\nIf you get no output from the QEMU command above, aligning your host and guest release versions may help. For example, if you generated\nefi.img\non Focal but want to emulate Jammy (with the Jammy cloud image), the firmware may not be fully compatible. Generating\nefi.img\non Jammy when emulating Jammy with the Jammy cloud image may help.\nFurther reading\n¶\nQEMU can be extended in many different ways. If you’d like to take QEMU further, you might want to explore these additional resources:\nVirtualizing graphics using QEMU/KVM\nUsing QEMU to create a microvm\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:34Z", "original_len_words": 1626}}
{"id": "e453532c0b", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/sev-snp/", "title": "Confidential Computing with AMD - Ubuntu Server documentation", "text": "Confidential Computing with AMD - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nConfidential Computing with AMD\n¶\nSecure Encrypted Virtualization - Secure Nested Paging (SEV-SNP) are a set of virtualization features available on the latest AMD EPIC CPUs (starting from “Rome”). These features enable\nConfidential Computing\nwhich is a way to better isolate a workload from the hypervisor and the host Operating System.\nVirtual Machines launched with SEV-SNP features enabled have their CPU registers protected from the host OS and their memory encrypted and integrity protected. The memory protection relies on the CPU’s Memory Management Unit (MMU) which verifies the integrity of the secure pages and generates per-VM encryption keys to encrypt them and prevent the host OS from reading their content.\nWhile using Ubuntu as a guest OS on SEV-SNP VMs has been supported since Ubuntu 24.04 LTS, the host enablement (QEMU and OVMF support) was only added later with Ubuntu 25.04.\nHost configuration\n¶\nAMD SEV-SNP is fully supported as of Ubuntu 25.04. To launch a VM with these features, enable memory encryption features and SNP in the firmware settings, and then assign Address-Space Identifiers (ASIDs) to SNP. For more details, refer to\nAMD’s documentation\nand to the manuals for your specific mainboard or Baseboard Management Controller (BMC).\nOn the host OS, install\nqemu-system-x86_64\nand launch QEMU with the following parameters:\nqemu-system-x86_64\n\\\n-enable-kvm\n\\\n-nographic\n\\\n-machine\nq35\n-smp\n6\n-m\n6G\n\\\n-drive\n\"if=virtio,format=qcow2,file=disk.img\"\n\\\n-net\nnic,model\n=\ne1000\n-net\nuser,hostfwd\n=\ntcp::2222-:22\n\\\n-cpu\nEPYC-v4\n\\\n-machine\nmemory-encryption\n=\nsev0,vmport\n=\noff\n\\\n-object\nmemory-backend-memfd,id\n=\nram1,size\n=\n6G,share\n=\ntrue,prealloc\n=\nfalse\n\\\n-machine\nmemory-backend\n=\nram1\n\\\n-object\nsev-snp-guest,id\n=\nsev0,cbitpos\n=\n47\n,reduced-phys-bits\n=\n1\n,kernel-hashes\n=\non\n\\\n-kernel\n./vmlinuz\n\\\n-append\n\"root=/dev/vda1 console=ttyS0\"\n\\\n-bios\n/usr/share/ovmf/OVMF.amdsev.fd\nHere we are configuring the VM to use 6GB of encrypted memory:\ncbitpos\nand\nreduced-phys-bits\nhave to be\n47\nand\n1\nrespectively for all EPYC CPUs.\nkernel-hashes=on\nmakes sure the kernel, initramfs and command line will be measured during the launch. It can be disabled but if enabled, the kernel needs to be provided with\n-kernel\nOVMF.amdsev.fd\nis a specific version of\nEDK II\n.\nFor more details about these parameters, refer to QEMU documentation pages for\ninvocation\nand\nAMD SEV\n.\nGuest configuration\n¶\nOn the guest side, Ubuntu 24.04 LTS and newer fully support AMD SEV-SNP. You can download the disk image and kernel for your VM from\ncloud-images.ubuntu.com\n. The latest image and kernel can be found here:\nhttps://cloud-images.ubuntu.com/releases/noble/release/ubuntu-24.04-server-cloudimg-amd64.img\nhttps://cloud-images.ubuntu.com/releases/noble/release/unpacked/\nOnce the VM is launched, install\nlinux-generic\nto get the\nsev-guest\nmodule and insert it with\nmodprobe\nsev-guest\n. This will create a new character device on the guest that can be used to generate attestation reports from the TEE:\n/dev/sev-guest\n.\nFinally, you can use AMD’s\nsnpguest\nto generate an attestation report that can be used for a remote attestation:\nsudo\n./snpguest\nreport\n--random\nattestation-report.bin\nrequest-file.txt", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:34Z", "original_len_words": 508}}
{"id": "ebd318ba92", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/ubuntu-on-hyper-v/", "title": "How to set up Ubuntu on Hyper-V - Ubuntu Server documentation", "text": "How to set up Ubuntu on Hyper-V - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to set up Ubuntu on Hyper-V\n¶\nHyper-V is a native\ntype 1 hypervisor\ndeveloped by Microsoft for the Windows family of operating systems, similar to Xen or VMWare\nESXi\n. It was first released for Windows Server in 2008, and has been available without additional charge since Windows Server 2012 and Windows 8.\nHyper-V allows Ubuntu to be run in parallel or in isolation on Windows operating systems. There are several use-cases for running Ubuntu on Hyper-V:\nTo introduce Ubuntu in a Windows-centric IT environment.\nTo have access to a complete Ubuntu desktop environment without dual-booting a PC.\nTo use Linux software on Ubuntu that is not yet supported on the\nWindows Subsystem for Linux\n.\nHyper-V system requirements\n¶\nThe following are typical\nsystem requirements for Hyper-V\n:\nA 64-bit processor with Second Level Address Translation (SLAT)\nCPU support for virtualization extensions and virtualization enabled in the system BIOS/EFI\nMinimum of 4 GB of memory, recommended 8 GB\nMinimum of 5 GB of disk space, recommended 15 GB\nInstall Hyper-V\n¶\nOur first step in enabling Ubuntu is to install Hyper-V, which can be used on the Windows 11 Pro, Enterprise, Education, and Server operating systems.\nHyper-V is not included in Windows 11 Home, which\nwould need to be upgraded\nto Windows 11 Pro.\nInstall Hyper-V graphically\n¶\nRight click on the Windows Start button and select ‘Apps and Features’.\nSelect ‘Programs and Features’ under Related Settings.\nSelect ‘Turn Windows Features on or off’.\nSelect ‘Hyper-V’ and click OK.\nRestart when prompted.\nInstall Hyper-V using PowerShell\n¶\nOpen a PowerShell console as Administrator.\nRun the following command:\nEnable\n-\nWindowsOptionalFeature\n-\nOnline\n-\nFeatureName\nMicrosoft\n-\nHyper\n-\nV\n-\nAll\nRestart when prompted.\nInstall Ubuntu on Hyper-V\n¶\nThere are two main methods for installing Ubuntu on Hyper-V depending on your use case. Read each of the descriptions of the following methods and then determine the best for your situation.\nUsing Quick Create\n¶\nThe recommended method is to use the curated Ubuntu image from the Hyper-V Quick Create Gallery. This is ideal for desktop development on Ubuntu and for users interested in running a complete Ubuntu desktop environment. The Ubuntu image from the Quick Create Gallery includes pre-configured features, such as clipboard sharing, dynamic resolution display, and shared folders.\nEnable Hyper-V as described above.\nOpen ‘Hyper-V Manager’ by either:\nSelecting the Windows Start button, then\n-> Expanding the ‘Windows Administrative Tools’ folder\n-> Selecting ‘Hyper-V Manager’\nor\nSelecting the Windows key, then\n-> typing ‘Hyper-V’\n-> selecting ‘Hyper-V Manager’\nIn the future, the Quick Create tool can be accessed directly using the above methods, but it is useful to know where Hyper-V Manager is because it is what you will use to manage your Ubuntu VM.\nOn the ‘Actions’ pane select ‘Quick Create’ and the Quick Create tool will open.\nSelect a version of Ubuntu from the versions on the list. A build of the\nmost recent LTS\nversion of Ubuntu and the\nmost recent interim release\nare provided.\nThe\nLTS version\nis recommended if you are developing for Ubuntu Server or an enterprise environment.\nThe\ninterim release\nis recommended if you would like to use the latest versions of software in Ubuntu.\nSelect ‘Create Virtual Machine’ and wait for the VM image to be downloaded.\nSelect ‘Connect’ to open a connection to your VM.\nSelect ‘Start’ to run your VM.\nComplete the final stages of the Ubuntu install, including username selection.\nUsing an Ubuntu CD image\n¶\nIt is possible to install Ubuntu on Hyper-V using a CD image ISO. This is useful if you are running Ubuntu Server and do not need an enhanced desktop experience. Note that the enhanced features of the Quick Create images are not enabled by default when you perform a manual install from an ISO.\nDownload an Ubuntu ISO from an\nofficial Ubuntu source\n.\nInstall Hyper-V as described above.\nOpen ‘Hyper-V Manager’ by either:\nSelecting the Windows Start button, then\n-> Expanding the ‘Windows Administrative Tools’ folder\n-> Selecting ‘Hyper-V Manager’\nor\nSelecting the Windows key, then\n-> Typing ‘Hyper-V’\n-> Selecting ‘Hyper-V Manager’\nOn the ‘Actions’ pane click ‘Quick Create’ and the Quick Create tool will open.\nSelect ‘Change installation source’ and choose the ISO file you downloaded before.\nIf you want to give your virtual machine a more descriptive name, select the ‘More options’ down-arrow and change ‘New Virtual Machine’ to something more useful, e.g. ‘Ubuntu Server 18.04 LTS’.\nSelect ‘Create Virtual Machine’ and wait for the virtual machine to be created.\nSelect ‘Connect’ to open a connection to your VM.\nSelect ‘File’ in the menu bar, then\n-> Choose ‘Settings’ and select the ‘Security’ tab\n-> Under Secure Boot choose ‘Microsoft UEFI Certificate Authority’\n-> Then select ‘Apply’ and ‘OK’ to return to your VM.\nSelect ‘Start’ to run your VM.\nComplete the manual installation of Ubuntu.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:34Z", "original_len_words": 850}}
{"id": "c5dc3f5011", "source_url": "https://documentation.ubuntu.com/server/how-to/virtualisation/virtual-machine-manager/", "title": "Virtual Machine Manager - Ubuntu Server documentation", "text": "Virtual Machine Manager - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nVirtual Machine Manager\n¶\nThe\nVirtual Machine Manager\n, through the\nvirt-manager\npackage, provides a graphical user interface (GUI) for managing local and remote virtual machines. In addition to the\nvirt-manager\nutility itself, the package also contains a collection of other helpful tools like\nvirt-install\n,\nvirt-clone\nand\nvirt-viewer\n.\nInstall virt-manager\n¶\nTo install\nvirt-manager\n, enter:\nsudo\napt\ninstall\nvirt-manager\nSince\nvirt-manager\nrequires a\nGraphical User Interface (GUI)\nenvironment we recommend installing it on a workstation or test machine instead of a production server. To connect to the local libvirt service, enter:\nvirt-manager\nYou can connect to the libvirt service running on another host by entering the following in a terminal prompt:\nvirt-manager\n-c\nqemu+ssh://virtnode1.mydomain.com/system\nNote\nThe above example assumes that SSH connectivity between the management system and the target system has already been configured, and uses\nSSH keys\nfor authentication. SSH keys are needed because libvirt sends the password prompt to another process. See our guide on OpenSSH for details on\nhow to set up SSH keys\n.\nUse virt-manager to manage guests\n¶\nGuest lifecycle\n¶\nWhen using\nvirt-manager\n, it is always important to know the context you’re looking at. The main window initially lists only the currently-defined guests. You’ll see their\nname\n,\nstate\n(e.g., ‘Shutoff’ or ‘Running’) and a small chart showing the\nCPU usage\n.\nIn this context, there isn’t much to do except start/stop a guest. However, by double-clicking on a guest or by clicking the\nOpen\nbutton at the top of the window, you can see the guest itself. For a running guest, that includes the guest’s main-console/virtual-screen output.\nIf you are deeper in the guest configuration, clicking on “Show the graphical console” in the top left of the guest’s window will get you back to this output.\nGuest modification\n¶\nvirt-manager\nprovides a handy, GUI-assisted way to edit guest definitions. To do so, the per-guest context view will have “Show virtual hardware details” at the top of the guest window. Here, you can edit the virtual hardware of the guest, which will alter the\nguest representation\nbehind the scenes.\nThe UI edit ability is limited to the features known to (and supported by) that GUI feature. Not only does libvirt grow features faster than\nvirt-manager\ncan keep up – adding every feature would also overload the UI and render it unusable.\nTo strike a balance between the two, there also is the XML view which can be reached via the “Edit libvirt XML” button.\nBy default, this will be read-only and you can see what the UI-driven actions have changed. You can allow read-write access in this view via the “Preferences”. This is the same content that the\nvirsh\nedit\nof the\nlibvirt-client\nexposes.\nVirtual Machine Viewer (virt-viewer)\n¶\nThe Virtual Machine Viewer application, through\nvirt-viewer\n, allows you to connect to a virtual machine’s console like\nvirt-manager\ndoes, but reduced to the GUI functionality.\nvirt-viewer\nrequires a GUI to interface with the virtual machine.\nInstall virt-viewer\n¶\nIf\nvirt-viewer\nis not already installed, you can install it from the terminal with the following command:\nsudo\napt\ninstall\nvirt-viewer\nOnce a virtual machine is installed and running you can connect to the virtual machine’s console by using:\nvirt-viewer\n<guestname>\nThe UI will show a window representing the virtual screen of the guest, just like with\nvirt-manager\nabove, but without the extra buttons and features around it.\nSimilarly to\nvirt-manager\n,\nvirt-viewer\ncan also connect to a remote host using SSH with key authentication:\nvirt-viewer\n-c\nqemu+ssh://virtnode1.mydomain.com/system\n<guestname>\nBe sure to replace\nweb_devel\nwith the appropriate virtual machine name.\nIf configured to use a\nbridged\nnetwork interface, you can also set up SSH access to the virtual machine.\nvirt-install\n¶\nvirt-install\nis part of the\nvirtinst\npackage. It can help with installing classic ISO-based systems and provides a CLI for the most common options needed to do so.\nInstall virt-install\n¶\nTo install\nvirt-install\n, if it is not installed already, run the following from a terminal prompt:\nsudo\napt\ninstall\nvirtinst\nThere are several options available when using\nvirt-install\n. For example:\nvirt-install\n\\\n--name\nweb_devel\n\\\n--ram\n8192\n\\\n--disk\npath\n=\n/home/doug/vm/web_devel.img,bus\n=\nvirtio,size\n=\n50\n\\\n--cdrom\nfocal-desktop-amd64.iso\n\\\n--network\nnetwork\n=\ndefault,model\n=\nvirtio\n\\\n--graphics\nvnc,listen\n=\n0\n.0.0.0\n\\\n--noautoconsole\n\\\n--hvm\n\\\n--vcpus\n=\n4\nThere are many more arguments that can be found in the\nvirt-install(1)\nmanpage. However, here is an explanation of arguments used in the example above, one by one:\n--name\nweb_devel\n:\nThe name of the new virtual machine will be\nweb_devel\n.\n--ram\n8192\n:\nSpecifies the amount of memory the virtual machine will use (in megabytes).\n--disk\npath=/home/doug/vm/web_devel.img,bus=virtio,size=50\n:\nIndicates the path to the virtual disk which can be a file, partition, or logical volume. In this example a file named\nweb_devel.img\nin the current user’s directory, with a size of 50 gigabytes, and using\nvirtio\nfor the disk bus. Depending on the disk path,\nvirt-install\nmay need to run with elevated privileges.\n--cdrom\nfocal-desktop-amd64.iso\n:\nFile to be used as a virtual CD-ROM. The file can be either an ISO file or the path to the host’s CD-ROM device.\n--network\n:\nProvides details related to the VM’s network interface. Here the default network is used, and the interface model is configured for\nvirtio\n.\n--graphics\nvnc,listen=0.0.0.0\n:\nExports the guest’s virtual console using VNC and on all host interfaces. Typically servers have no GUI, so another GUI-based computer on the Local Area Network (LAN) can connect via VNC to complete the installation.\n--noautoconsole\nWill not automatically connect to the virtual machine’s console.\n--hvm\n: creates a fully virtualized guest.\n--vcpus=4\n: allocate 4 virtual CPUs.\nAfter launching\nvirt-install\n, you can connect to the virtual machine’s console either locally using a GUI (if your server has a GUI), or via a remote VNC client from a GUI-based computer.\nvirt-clone\n¶\nThe\nvirt-clone\napplication can be used to copy one virtual machine to another. For example:\nvirt-clone\n--auto-clone\n--original\nfocal\nOptions used:\n--auto-clone\n:\nTo have\nvirt-clone\ncreate guest names and disk paths on its own.\n--original\n:\nName of the virtual machine to copy.\nYou can also use the\n-d\nor\n--debug\noption to help troubleshoot problems with\nvirt-clone\n.\nReplace\nfocal\nwith the appropriate virtual machine names for your case.\nCaution\nPlease be aware that this is a full clone, therefore any secrets, keys, and for example,\n/etc/machine-id\n, will be shared. This will cause issues with security and anything that needs to identify the machine like\nDHCP\n. You most likely want to edit those afterward and de-duplicate them as needed.\nResources\n¶\nSee the\nKVM\nhome page for more details.\nFor more information on libvirt see the\nlibvirt home page\nThe\nVirtual Machine Manager\nsite has more information on\nvirt-manager\ndevelopment.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:34Z", "original_len_words": 1172}}
{"id": "c65978c72c", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/", "title": "Web services - Ubuntu Server documentation", "text": "Web services - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nWeb services\n¶\nWeb servers are used to serve content over a network (or the Internet). If you want more of an introduction to the different types of web servers available in Ubuntu, check out our\nIntroduction to web servers\n.\nProxy servers\n¶\nThis section shows how to set up a Squid proxy caching server.\nInstall a Squid server\nWeb servers\n¶\nTwo of the most popular web servers in Ubuntu are Apache2 and nginx. This section covers the installation, configuration and extension of both.\nApache2\n¶\nInstall Apache2\nConfigure Apache2\nExtend Apache2 with modules\nNginx\n¶\nInstall nginx\nConfigure nginx\nExtend nginx with modules\nWeb programming\n¶\nIt is common to set up server-side scripting languages to support the creation of dynamic web content. Whichever scripting language you choose, you will need to have installed and configured your web and database servers beforehand.\nInstall PHP\nInstall Ruby on Rails\nSee also\n¶\nExplanation:\nWeb services", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:35Z", "original_len_words": 189}}
{"id": "9b7f845e4b", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/configure-apache2-settings/", "title": "How to configure Apache2 settings - Ubuntu Server documentation", "text": "How to configure Apache2 settings - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to configure Apache2 settings\n¶\nAfter you have\ninstalled Apache2\n, you will likely need to configure it. In this explanatory guide, we will explain the Apache2 server essential configuration parameters.\nBasic directives\n¶\nApache2 ships with a “virtual-host-friendly” default configuration – it is configured with a single default virtual host (using the\nVirtualHost\ndirective) which can be modified or used as-is if you have a single site, or used as a template for additional virtual hosts if you have multiple sites.\nIf left alone, the default virtual host will serve as your default site, or the site users will see if the URL they enter does not match the\nServerName\ndirective of any of your custom sites. To modify the default virtual host, edit the file\n/etc/apache2/sites-available/000-default.conf\n.\nNote\nThe directives set for a virtual host only apply to that particular virtual host. If a directive is set server-wide and not defined in the virtual host settings, the default setting is used. For example, you can define a Webmaster email address and not define individual email addresses for each virtual host.\nIf you want to configure a new virtual host or site, copy the\n000-default.conf\nfile into the same directory with a name you choose. For example:\nsudo\ncp\n/etc/apache2/sites-available/000-default.conf\n/etc/apache2/sites-available/mynewsite.conf\nEdit the new file to configure the new site using some of the directives described below:\nThe\nServerAdmin\ndirective\n¶\nFound in\n/etc/apache2/sites-available\nSpecifies the email address to be advertised for the server’s administrator. The default value is\nwebmaster@localhost\n. This should be changed to an email address that is delivered to you (if you are the server’s administrator). If your website has a problem, Apache2 will display an error message containing this email address to report the problem to.\nThe\nListen\ndirective\n¶\nFound in\n/etc/apache2/ports.conf\nSpecifies the port, and optionally the IP address, Apache2 should listen on. If the IP address is not specified, Apache2 will listen on all IP addresses assigned to the machine it runs on. The default value for the\nListen\ndirective is\n80\n. Change this to:\n127.0.0.1:80\nto make Apache2 listen only on your loopback interface so that it will not be available to the Internet,\nto e.g.\n81\nto change the port that it listens on,\nor leave it as is for normal operation.\nThe\nServerName\ndirective (optional)\n¶\nSpecifies what\nFQDN\nyour site should answer to. The default virtual host has no\nServerName\ndirective specified, so it will respond to all requests that do not match a ServerName directive in another virtual host. If you have just acquired the domain name\nmynewsite.com\nand wish to host it on your Ubuntu server, the value of the ServerName directive in your virtual host configuration file should be\nmynewsite.com\n.\nAdd this directive to the new virtual host file you created earlier (\n/etc/apache2/sites-available/mynewsite.conf\n).\nThe\nServerAlias\ndirective\n¶\nYou may also want your site to respond to\nwww.mynewsite.com\n, since many users will assume the www prefix is appropriate – use the\nServerAlias\ndirective for this. You may also use wildcards in the ServerAlias directive.\nFor example, the following configuration will cause your site to respond to any domain request ending in\n.mynewsite.com\n.\nServerAlias *.mynewsite.com\nThe\nDocumentRoot\ndirective\n¶\nSpecifies where Apache2 should look for the files that make up the site. The default value is\n/var/www/html\n, as specified in\n/etc/apache2/sites-available/000-default.conf\n. If desired, change this value in your site’s virtual host file, and remember to create that directory if necessary!\nEnable the new\nVirtualHost\nusing the a2ensite utility and restart Apache2:\nsudo\na2ensite\nmynewsite\nsudo\nsystemctl\nrestart\napache2.service\nNote\nBe sure to replace\nmynewsite\nwith a more descriptive name for the VirtualHost. One method is to name the file after the\nServerName\ndirective of the VirtualHost.\nSimilarly, use the\na2dissite\nutility to disable sites. This is can be useful when troubleshooting configuration problems with multiple virtual hosts:\nsudo\na2dissite\nmynewsite\nsudo\nsystemctl\nrestart\napache2.service\nApache2 server default settings\n¶\nThis section explains configuration of the Apache2 server default settings. For example, if you add a virtual host, the settings you configure for the virtual host take precedence for that virtual host. For a directive not defined within the virtual host settings, the default value is used.\nThe\nDirectoryIndex\n¶\nThe\nDirectoryIndex\nis the default page served by the server when a user requests an index of a directory by specifying a forward slash (/) at the end of the directory name.\nFor example, when a user requests the page\nhttp://www.example.com/this_directory/\n, they will get either the DirectoryIndex page (if it exists), a server-generated directory list (if it does not and the Indexes option is specified), or a Permission Denied page if neither is true.\nThe server will try to find one of the files listed in the DirectoryIndex directive and will return the first one it finds. If it does not find any of these files and if\nOptions Indexes\nis set for that directory, the server will generate and return a list, in HTML format, of the subdirectories and files in the directory. The default value, found in\n/etc/apache2/mods-available/dir.conf\nis “index.html index.cgi\nindex.pl\nindex.php index.xhtml index.htm”. Thus, if Apache2 finds a file in a requested directory matching any of these names, the first will be displayed.\nThe\nErrorDocument\n¶\nThe\nErrorDocument\ndirective allows you to specify a file for Apache2 to use for specific error events. For example, if a user requests a resource that does not exist, a 404 error will occur.\nBy default, Apache2 will return a HTTP 404 Return code. Read\n/etc/apache2/conf-available/localized-error-pages.conf\nfor detailed instructions on using ErrorDocument, including locations of example files.\nCustomLog\nand\nErrorLog\n¶\nBy default, the server writes the transfer log to the file\n/var/log/apache2/access.log\n. You can change this on a per-site basis in your virtual host configuration files with the\nCustomLog\ndirective, or omit it to accept the default, specified in\n/etc/apache2/conf-available/other-vhosts-access-log.conf\n.\nYou can also specify the file to which errors are logged, via the\nErrorLog\ndirective, whose default is\n/var/log/apache2/error.log\n. These are kept separate from the transfer logs to aid in troubleshooting problems with your Apache2 server. You may also specify the\nLogLevel\n(the default value is “warn”) and the\nLogFormat\n(see\n/etc/apache2/apache2.conf\nfor the default value).\nThe\nOptions\ndirective\n¶\nSome options are specified on a per-directory basis rather than per-server.\nOptions\nis one of these directives. A Directory stanza is enclosed in XML-like tags, like so:\n<Directory /var/www/html/mynewsite>\n...\n</Directory>\nThe Options directive within a Directory stanza accepts one or more of the following values (among others), separated by spaces:\nExecCGI\nAllow CGI scripts to be run. CGI scripts are not run if this option is not chosen.\nCaution\nMost files should not be run as CGI scripts. This would be very dangerous. CGI scripts should kept in a directory separate from and outside your\nDocumentRoot\n, and only this directory should have the ExecCGI option set. This is the default, and the default location for CGI scripts is\n/usr/lib/cgi-bin\n.\nIncludes\nAllow\nserver-side includes\n. Server-side includes allow an HTML file to\ninclude\nother files. See\nApache SSI documentation (Ubuntu community)\nfor more information.\nIncludesNOEXEC\nAllow server-side includes, but disable the\n#exec\nand\n#include\ncommands in CGI scripts.\nIndexes\nDisplay a formatted list of the directory’s contents, if no DirectoryIndex (such as\nindex.html\n) exists in the requested directory.\nCaution\nFor security reasons, this should usually not be set, and certainly should not be set on your DocumentRoot directory. Enable this option carefully on a per-directory basis\nonly\nif you are certain you want users to see the entire contents of the directory.\nMultiview\nSupport content-negotiated multiviews; this option is disabled by default for security reasons. See the\nApache2 documentation on this option\n.\nSymLinksIfOwnerMatch\nOnly follow symbolic links if the target file or directory has the same owner as the link.\nApache2 daemon settings\n¶\nThis section briefly explains some basic Apache2 daemon configuration settings.\nLockFile\nThe\nLockFile\ndirective sets the path to the lockfile used when the server is compiled with either\nUSE_FCNTL_SERIALIZED_ACCEPT\nor\nUSE_FLOCK_SERIALIZED_ACCEPT\n. It must be stored on the local disk. It should be left to the default value unless the logs directory is located on an NFS share. If this is the case, the default value should be changed to a location on the local disk and to a directory that is readable only by root.\nPidFile\nThe\nPidFile\ndirective sets the file in which the server records its process ID (pid). This file should only be readable by root. In most cases, it should be left to the default value.\nUser\nThe\nUser\ndirective sets the userid used by the server to answer requests. This setting determines the server’s access. Any files inaccessible to this user will also be inaccessible to your website’s visitors. The default value for User is “www-data”.\nWarning\nUnless you know exactly what you are doing, do not set the User directive to root. Using root as the User will create large security holes for your Web server.\nGroup\nThe\nGroup\ndirective is similar to the User directive. Group sets the group under which the server will answer requests. The default group is also “www-data”.\nExtending Apache2\n¶\nNow that you know how to configure Apache2, you may also want to know\nhow to extend Apache2\nwith modules.\nFurther reading\n¶\nThe\nApache2 Documentation\ncontains in depth information on Apache2 configuration directives. Also, see the\napache2-doc\npackage for the official Apache2 docs.\nO’Reilly’s\nApache Cookbook\nis a good resource for accomplishing specific Apache2 configurations.\nFor Ubuntu specific Apache2 questions, ask in the\n#ubuntu-server\nIRC channel on\nlibera.chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:35Z", "original_len_words": 1643}}
{"id": "63be990c25", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/configure-nginx/", "title": "How to configure nginx - Ubuntu Server documentation", "text": "How to configure nginx - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to configure nginx\n¶\nOnce you have\ninstalled nginx\n, you can customize it for your use with the configuration options explained in this guide.\nServer blocks\n¶\nnginx organizes sets of site-specific configuration details into\nserver blocks\n, and by default comes pre-configured for single-site operation. This can either be used “as-is”, or as a starting template\nfor serving multiple sites.\nThe single-site configuration serves files out of\n/var/www/html\n, as defined by the server block and as provided by\n/etc/nginx/sites-enabled/default\n:\nserver {\n        listen 80 default_server;                                                                                    \n        listen [::]:80 default_server;\n\n        root /var/www/html;                                                                                          \n                                                                                                                         \n        # Add index.php to the list if you are using PHP                                                             \n        index index.html index.htm index.nginx-debian.html;                                                          \n                                                                                                                         \n        server_name _;                                                                                               \n                                                                                                                         \n        location / {                                                                                                 \n                # First attempt to serve request as file, then                                                       \n                # as directory, then fall back to displaying a 404.                                                  \n                try_files $uri $uri/ =404;                                                                           \n        }\n}\nEven for a single-site configuration, while you can place your website at\n/var/www/html\n, you may want to place the website’s files at a different location in your\nfilesystem\n. For example, if you were hosting\nwww.my-site.org\nfrom\n/srv/my-site/html\nyou might edit the above file to look like this:\nserver {\n        listen                80;\n        root                  /srv/my-site/html;\n        index                 index.html;\n        server_name           my-site.org www.my-site.org;\n\n        location / {                                                                                                 \n                try_files $uri $uri/ =404;                                                                           \n        }\n}\nMake sure to create your web root directory structure:\n$\nsudo\nmkdir\n-p\n/srv/my-site/html\n$\nsudo\nchmod\n-R\n755\n/srv/my-site/html\n$\necho\n\"<html><body><h1>My Site!</h1></body></html>\"\n>\n/srv/my-site/html/index.html\nThen, to make nginx reload its configuration, run:\n$\nsudo\nsystemctl\nreload\nnginx\nCheck that the settings have taken effect using your web browser:\n$\nwww-browser\nwww.my-site.org\nMulti-site hosting\n¶\nSimilar to Apache, nginx uses the\nsites-available\nand\nsites-enabled\ndirectories for the configurations of multiple websites.  Unlike with Apache, you’ll need to handle the enablement manually.\nTo do that, first create a new server block in a configuration file as above, and save it to\n/etc/nginx/sites-available/<your-domain>\n. Make sure to give each site a unique\nserver_name\nand a different\nlisten\nport number.\nNext, enable the site by creating a symlink to it from the\nsites-enabled\ndirectory:\n$\nsudo\nln\n-s\n/etc/nginx/sites-available/<your-domain>\n/etc/nginx/sites-enabled/\nTo disable a website, you can delete the symlink in\nsites-enabled\n. For example, once you have your new site(s) configured and no longer need the default site configuration:\n$\nsudo\nrm\n/etc/nginx/sites-enabled/default\nSSL and HTTPS\n¶\nWhile establishing an HTTP website on port 80 is a good starting point (and perhaps adequate for static content), production systems will want HTTPS, such as serving on port 443 with SSL enabled via\ncert\nfiles.  A server block with such a configuration might look like this, with HTTP-to-HTTPS redirection handled in the first block, and HTTPS in the second block:\nserver {\n        listen                80;\n        server_name           our-site.org www.our-site.org;\n        return                301 https://$host$request_uri;\n}\n\nserver {\n        listen                443 ssl;\n\n        root                  /srv/our-site/html;\n        index                 index.html;\n\n        server_name           our-site.org www.our-site.org;\n                                                   \n        ssl_certificate       our-site.org.crt;\n        ssl_certificate_key   our-site.org.key;\n        ssl_protocols         TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;\n        ssl_ciphers           HIGH:!aNULL:!MD5;\n        ssl_session_timeout   15m;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n}\nThanks to the\nreturn\n301\nline in the above configuration, anyone visiting the site on port 80 via an HTTP URL will get automatically redirected to the equivalent secure HTTPS URL.\nRefer to the\nsecurity - certificates\npage in this manual for details on how to create and manage certificates, and the\nOpenSSL\npage for additional details on configuring and using that service. The\nGnuTLS\nsection explains how to configure different SSL protocol versions and their associated ciphers.\nFor example, to generate a self-signed certificate, you might run a set of commands similar to these:\n$\nsudo\nopenssl\ngenrsa\n-out\nour-site.org.key\n2048\n$\nopenssl\nreq\n-nodes\n-new\n-key\nour-site.org.key\n-out\nca.csr\n$\nopenssl\nx509\n-req\n-days\n365\n-in\nour-site.org.csr\n-signkey\nour-site.org.key\n-out\nour-site.org.crt\n$\nmkdir\n/etc/apache2/ssl\n$\ncp\nour-site.org.crt\nour-site.org.key\nour-site.org.csr\n/etc/apache2/ssl/\nSetting up nginx\n¶\nBeyond the settings outlined above, nginx can be further customized through the use of modules.  Please see the next guide in this series for details of how to do that.\nPart 3:\nHow to use nginx modules\nFurther reading\n¶\nnginx’s beginner’s guide\ncovers use cases such as proxy servers,\nFastCGI\nfor use with PHP and other frameworks, and optimising the handling of static content.\nThe\nnginx documentation\ndescribes HTTPS server configuration in greater detail, including certificate chains, disambiguating various multi-site certificate situations, performance optimisations and compatibility issues.\nFor Ubuntu-specific nginx questions, ask in the\n#ubuntu-server\nIRC channel on\nlibera.chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:35Z", "original_len_words": 760}}
{"id": "f07cba557b", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/install-a-squid-server/", "title": "How to install a Squid server - Ubuntu Server documentation", "text": "How to install a Squid server - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install a Squid server\n¶\nSquid is a filtering and caching mechanism for web servers that can optimise bandwidth and performance. For more information about Squid proxy servers,\nrefer to this guide\n.\nInstall Squid\n¶\nAt a terminal prompt, enter the following command to install the Squid server:\nsudo\napt\ninstall\nsquid\nConfigure Squid\n¶\nSquid is configured by editing directives in the\n/etc/squid/squid.conf\nconfiguration file. The following examples illustrate a sample of directives that can be modified to configure the Squid server’s behavior. For more in-depth configuration details, see the links at the bottom of the page.\nProtect the original config file\n¶\nBefore editing the configuration file, you should make a copy of the original and protect it from writing. You will then have the original settings as a reference, and can reuse it when needed. Run the following commands to make a copy of the original configuration file and protect it from being written to:\nsudo\ncp\n/etc/squid/squid.conf\n/etc/squid/squid.conf.original\nsudo\nchmod\na-w\n/etc/squid/squid.conf.original\nChange TCP port\n¶\nTo set your Squid server to listen on TCP port 8888 instead of the default TCP port 3128, change the\nhttp_port\ndirective as such:\nhttp_port 8888\nSet the hostname\n¶\nChange the\nvisible_hostname\ndirective to give the Squid server a specific\nhostname\n. This hostname does not need to be the same as the computer’s hostname. In this example it is set to\nweezie\n:\nvisible_hostname weezie\nConfigure the memory cache\n¶\nThe default setting is to use on-memory cache. This example tells squid to use up to 512MB of memory, erasing the last recently used content when the cache is full to free space for new items:\ncache_mem 512 MB\nmaximum_memory_policy lru\nConfigure on-disk cache\n¶\nBy changing the\ncache_dir\ndirective you can configure use of an on-disk cache. The\ncache_dir\ndirective takes the following arguments:\ncache_dir <Type> <Directory-Name> <Size-in-MB> <L1-Dirs> <L2-Dirs> [options]\nIn this example we set the cache configuration to use a\nufs\nstorage in\n/var/spool/squid\n, up to 10GB, with 16 directories on the first level of the hierarchy, each of those containing 256 directories for organization.\ncache_dir ufs /var/spool/squid 10000 16 256\nThe available storage types are:\nufs\n: This is the common Squid storage format, good for general use.\naufs\n: Uses the same storage format as\nufs\n, using POSIX-threads to avoid blocking the main Squid process on disk-I/O. This was formerly known in Squid as\nasync-io\n.\ndiskd\n: Uses the same storage format as\nufs\n, using a separate process to avoid blocking the main Squid process on disk-I/O.\nrock\n: This is a database-style storage. All cached entries are stored in a “database” file, using fixed-size slots. A single entry occupies one or more slots.\nConfigure cached objects size limits\n¶\nThe following configuration directives control which objects get cached based on their size, for space optimization, both on disk and in memory:\nmaximum_object_size 512 MB\nminimum_object_size 0 KB\nmaximum_object_size_in_memory 512 KB\nConfigure cached objects lifetime\n¶\nUsing the\nrefresh_pattern\nconfiguration directive controls how long cached objects stay fresh before they need to be revalidated with the origin server. It is configured as:\nrefresh_pattern regex min percent max [options]\nwhere\nregex\nneeds to match against the filename,\nmin\nand\nmax\nset the time limits in minutes for freshness, and a\npercent\nof the object’s age to calculate the refresh threshold.\nIn the following example, static web assets (such as images, css and scripts) are configured to be kept for 7 to 30 days + 90% of their age, based on the\nLast-Modified\nheader, while everything else is kept up to 3 days + 20% of their age.\nrefresh_pattern -i \\.(gif|jpg|png|css|js)$  10080  90%  43200\nrefresh_pattern .                           0      20%  4320\nCaching HTTPS content\n¶\nBy default, Squid can’t cache HTTPS because the traffic is encrypted. There are different strategies for enabling HTTPS caching, such as TLS interception via\nCONNECT\nrequests, origin server caching based on\nCache-Control\nand\nETag\nheaders, or access-control only solutions.\nPlease refer to the\nSquid HTTPS documentation\nto learn more.\nOther caching configuration options\n¶\nDifferent options can be used to fine-tune the caching behavior overall, by determining how squid stores files in the hierarchy, the algorithm for the replacement policy, DNS cache settings, compatibility with different scenarios, and more.\nFor a full list of configuration entries, please refer to the\nSquid configuration guide\n.\nAccess control\n¶\nUsing Squid’s access control, you can configure use of Squid-proxied Internet services to be available only to users with certain Internet Protocol (IP) addresses. For example, we will illustrate access by users of the\n192.168.42.0/24\nsubnetwork only:\nAdd the following to the\nbottom\nof the\nACL\nsection of your\n/etc/squid/squid.conf\nfile:\nacl fortytwo_network src 192.168.42.0/24\nThen, add the following to the\ntop\nof the\nhttp_access\nsection of your\n/etc/squid/squid.conf\nfile:\nhttp_access allow fortytwo_network\nUsing Squid’s access control features, you can configure Squid-proxied Internet services to only be available during normal business hours. As an example, we’ll illustrate access by employees of a business which is operating between 9:00AM and 5:00PM, Monday through Friday, and which uses the\n10.1.42.0/24\nsubnetwork:\nAdd the following to the\nbottom\nof the ACL section of your\n/etc/squid/squid.conf\nfile:\nacl biz_network src 10.1.42.0/24\nacl biz_hours time M T W T F 9:00-17:00\nThen, add the following to the\ntop\nof the\nhttp_access\nsection of your\n/etc/squid/squid.conf\nfile:\nhttp_access allow biz_network biz_hours\nRestart the Squid server\n¶\nAfter making any changes to the\n/etc/squid/squid.conf\nfile, you will need to save the file and restart the squid server application.\nFirst, you can verify the syntax of your configuration file by running:\nsudo\nsquid\n-k\nparse\nYou can restart the server using the following command:\nsudo\nsystemctl\nrestart\nsquid.service\nNote\nIf a formerly customized squid3 was used to set up the spool at\n/var/log/squid3\nto be a mount point, but otherwise kept the default configuration, the upgrade will fail. The upgrade tries to rename/move files as needed, but it can’t do so for an active mount point. In that case you will need to adapt either the mount point or the config in\n/etc/squid/squid.conf\nso that they match.\nThe same applies if the\ninclude\nconfig statement was used to pull in more files from the old path at\n/etc/squid3/\n. In those cases you should move and adapt your configuration accordingly.\nTroubleshooting\n¶\nTo monitor Squid behavior and check for potential errors and problems, there are useful commands to be executed and files to be checked.\nSquid version and status can be checked with:\nsudo\nsquid\n-v\nsudo\nsystemctl\nstatus\nsquid\nChecking or watching the log files may be useful to see potential errors, and to verify cache hits and misses:\nsudo\ncat\n/var/log/squid/cache.log\nsudo\ncat\n/var/log/squid/access.log\nFor a status summary containing runtime statistics and congfiguration, run:\nsquidclient\nmgr:info\nWhile monitoring, these cache status indicators can help identifying what is happening with requests:\nTCP_MISS\n: Content not in cache, fetched from origin\nTCP_HIT\n: Content served from disk cache\nTCP_MEM_HIT\n: Content served from memory cache\nTCP_REFRESH_HIT\n: Cached content revalidated with origin\nTCP_TUNNEL\n: HTTPS traffic (not cached by default)\nA healthy cache should show increasing hit ratios over time, of non-zero size, and a growing number of cached objects.\nFurther reading\n¶\nThe Squid Website\nUbuntu Wiki page on Squid\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:35Z", "original_len_words": 1252}}
{"id": "ada103a01f", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/install-apache2/", "title": "How to install Apache2 - Ubuntu Server documentation", "text": "How to install Apache2 - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install Apache2\n¶\nThe Apache HTTP Server\n(“httpd”) is the most commonly used web server on Linux systems, and is often used as part of the LAMP configuration. In this guide, we will show you how to install and configure Apache2, which is the current release of “httpd”.\nInstall\napache2\n¶\nTo install Apache2, enter the following command at the terminal prompt:\nsudo\napt\ninstall\napache2\nConfigure\napache2\n¶\nApache2 is configured by placing\ndirectives\nin plain text configuration files in\n/etc/apache2/\n. These\ndirectives\nare separated between the following files and directories:\nFiles\n¶\napache2.conf\nThe main Apache2 configuration file. Contains settings that are\nglobal\nto Apache2.\nNote\nHistorically, the main Apache2 configuration file was\nhttpd.conf\n, named after the “httpd” daemon. In other distributions (or older versions of Ubuntu), the file might be present. In modern releases of Ubuntu, all configuration options have been moved to\napache2.conf\nand the below referenced directories and\nhttpd.conf\nno longer exists.\nenvvars\nFile where Apache2\nenvironment\nvariables are set.\nmagic\nInstructions for determining MIME type based on the first few bytes of a file.\nports.conf\nHouses the directives that determine which TCP ports Apache2 is listening on.\nIn addition, other configuration files may be added using the\nInclude\ndirective, and wildcards can be used to include many configuration files. Any directive may be placed in any of these configuration files. Changes to the main configuration files are only recognized by Apache2 when it is started or restarted.\nThe server also reads a file containing MIME document types; the filename is set by the\nTypesConfig\ndirective, typically via\n/etc/apache2/mods-available/mime.conf\n, which might also include additions and overrides, and is\n/etc/mime.types\nby default.\nDirectories\n¶\nconf-available\nThis directory contains available configuration files. All files that were previously in\n/etc/apache2/conf.d\nshould be moved to\n/etc/apache2/conf-available\n.\nconf-enabled\nHolds\nsymlinks\nto the files in\n/etc/apache2/conf-available\n. When a configuration file is symlinked, it will be enabled the next time Apache2 is restarted.\nmods-available\nThis directory contains configuration files to both load\nmodules\nand configure them. Not all modules will have specific configuration files, however.\nmods-enabled\nHolds symlinks to the files in\n/etc/apache2/mods-available\n. When a module configuration file is symlinked it will be enabled the next time Apache2 is restarted.\nsites-available\nThis directory has configuration files for Apache2\nVirtual Hosts\n. Virtual Hosts allow Apache2 to be configured for multiple sites that have separate configurations.\nsites-enabled\nLike\nmods-enabled\n,\nsites-enabled\ncontains symlinks to the\n/etc/apache2/sites-available\ndirectory. Similarly, when a configuration file in\nsites-available\nis symlinked, the site configured by it will be active once Apache2 is restarted.\nDetailed configuration\n¶\nFor more detailed information on configuring Apache2, check out our follow-up guides.\nPart 2:\nApache2 configuration settings\nPart 3:\nhow to extend Apache2\nwith modules.\nFurther reading\n¶\nApache2 Documentation\ncontains in depth information on Apache2 configuration directives. Also, see the\napache2-doc\npackage for the official Apache2 docs.\nO’Reilly’s\nApache Cookbook\nis a good resource for accomplishing specific Apache2 configurations.\nFor Ubuntu-specific Apache2 questions, ask in the\n#ubuntu-server\nIRC channel on\nlibera.chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:36Z", "original_len_words": 543}}
{"id": "1a0aac9502", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/install-nginx/", "title": "How to install nginx - Ubuntu Server documentation", "text": "How to install nginx - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install nginx\n¶\nThe nginx HTTP server is a powerful alternative to\nApache\n. In this guide, we will demonstrate how to install and use nginx for web services.\nInstall nginx\n¶\nTo install nginx, enter the following command at the terminal prompt:\n$\nsudo\napt\nupdate\n$\nsudo\napt\ninstall\nnginx\nThis will also install any required dependency packages, and some common mods for your server, and then start the nginx web server.\nVerify nginx is running\n¶\nYou can verify that nginx is running via this command:\n$\nsudo\nsystemctl\nstatus\nnginx\n●\nnginx.service\n-\nA\nhigh\nperformance\nweb\nserver\nand\na\nreverse\nproxy\nserver\nLoaded:\nloaded\n(\n/lib/systemd/system/nginx.service\n;\nenabled\n;\nvendor\npreset:\nenabled\n)\nActive:\nactive\n(\nrunning\n)\nsince\nSun\n2023\n-08-20\n01\n:04:22\nUTC\n;\n53s\nago\nDocs:\nman:nginx\n(\n8\n)\nProcess:\n28210\nExecStartPre\n=\n/usr/sbin/nginx\n-t\n-q\n-g\ndaemon\non\n;\nmaster_process\non\n;\n(\ncode\n=\nexited,\nstatus\n=\n0\n/SU\n\\\nCCESS\n)\nProcess:\n28211\nExecStart\n=\n/usr/sbin/nginx\n-g\ndaemon\non\n;\nmaster_process\non\n;\n(\ncode\n=\nexited,\nstatus\n=\n0\n/SUCCESS\n)\nMain\nPID:\n28312\n(\nnginx\n)\nTasks:\n13\n(\nlimit:\n76969\n)\nMemory:\n13\n.1M\nCPU:\n105ms\nCGroup:\n/system.slice/nginx.service\n├─28312\n\"nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\"\n├─28314\n\"nginx: worker process\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n...\nRestarting nginx\n¶\nTo restart nginx, run:\n$\nsudo\nsystemctl\nrestart\nnginx\nEnable/disable nginx manually\n¶\nBy default, Nginx will automatically start at boot time.  To disable this behavior so that you can start it up manually, you can disable it:\n$\nsudo\nsystemctl\ndisable\nnginx\nThen, to re-enable it, run:\n$\nsudo\nsystemctl\nenable\nnginx\nThe default nginx homepage\n¶\nA default nginx home page is also set up during the installation process. You can load this page in your web browser using your web server’s IP address; http://\nyour_server_ip\n.\nThe default home page should look similar to:\nWelcome to nginx!        \n                                              Welcome to nginx!\n\nIf you see this page, the nginx web server is successfully installed and working. Further configuration is\nrequired.\n\nFor online documentation and support please refer to nginx.org.\nCommercial support is available at nginx.com.\n\nThank you for using nginx.\nSetting up nginx\n¶\nFor more information on customizing nginx for your needs, see these follow-up guides:\nPart 2: {ref}`How to configure nginx\nPart 3: {ref}`How to use nginx modules\nFurther reading\n¶\nThe\nnginx documentation\nprovides detailed explanations of configuration directives.\nO’Reilly’s nginx cookbook provides guidance on solving specific needs\nFor Ubuntu-specific nginx questions, ask in the\n#ubuntu-server\nIRC channel on\nlibera.chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:36Z", "original_len_words": 479}}
{"id": "190befa241", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/install-php/", "title": "How to install and configure PHP - Ubuntu Server documentation", "text": "How to install and configure PHP - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure PHP\n¶\nPHP\nis a general-purpose scripting language well-suited for Web development since PHP scripts can be embedded into HTML. This guide explains how to install and configure PHP in an Ubuntu System with Apache2 and MySQL.\nPrerequisites\n¶\nBefore installing PHP you should install Apache (or a preferred web server) and a database service such as MySQL.\nTo install the Apache package, please refer to\nour Apache guide\n.\nTo install and configure a MySQL database service, refer to\nour MySQL guide\n.\nInstall PHP\n¶\nPHP is available on Ubuntu Linux, but unlike Python (which comes pre-installed), must be manually installed.\nTo install PHP – and the Apache PHP module – you can enter the following command into a terminal prompt:\nsudo\napt\ninstall\nphp\nlibapache2-mod-php\nInstall optional packages\n¶\nThe following packages are optional, and can be installed if you need them for your setup.\nPHP-CLI\nYou can run PHP scripts via the Command Line Interface (CLI). To do this, you must first install the\nphp-cli\npackage. You can install it by running the following command:\nsudo\napt\ninstall\nphp-cli\nPHP-CGI\nYou can also execute PHP scripts without installing the Apache PHP module. To accomplish this, you should install the\nphp-cgi\npackage via this command:\nsudo\napt\ninstall\nphp-cgi\nPHP-MySQL\nTo use MySQL with PHP you should install the\nphp-mysql\npackage, like so:\nsudo\napt\ninstall\nphp-mysql\nPHP-PgSQL\nSimilarly, to use PostgreSQL with PHP you should install the\nphp-pgsql\npackage:\nsudo\napt\ninstall\nphp-pgsql\nConfigure PHP\n¶\nIf you have installed the\nlibapache2-mod-php\nor\nphp-cgi\npackages, you can run PHP scripts from your web browser. If you have installed the\nphp-cli\npackage, you can run PHP scripts at a terminal prompt.\nBy default, when\nlibapache2-mod-php\nis installed, the Apache2 web server is configured to run PHP scripts using this module. First, verify if the files\n/etc/apache2/mods-enabled/php8.*.conf\nand\n/etc/apache2/mods-enabled/php8.*.load\nexist. If they do not exist, you can enable the module using the\na2enmod\ncommand.\nOnce you have installed the PHP-related packages and enabled the Apache PHP module, you should restart the Apache2 web server to run PHP scripts, by running the following command:\nsudo\nsystemctl\nrestart\napache2.service\nTest your setup\n¶\nTo verify your installation, you can run the following PHP\nphpinfo\nscript:\n<?php\nphpinfo\n();\n?>\nYou can save the content in a file –\nphpinfo.php\nfor example – and place it under the\nDocumentRoot\ndirectory of the Apache2 web server. Pointing your browser to\nhttp://hostname/phpinfo.php\nwill display the values of various PHP configuration parameters.\nFurther reading\n¶\nFor more in depth information see\nthe php.net documentation\n.\nThere are a plethora of books on PHP 7 and PHP 8. A good book from O’Reilly is “Learning PHP”, which includes an exploration of PHP 7’s enhancements to the language.\nAlso, see the\nApache MySQL PHP Ubuntu Wiki\npage for more information.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:36Z", "original_len_words": 515}}
{"id": "54c633387e", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/install-ruby-on-rails/", "title": "How to install and configure Ruby on Rails - Ubuntu Server documentation", "text": "How to install and configure Ruby on Rails - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to install and configure Ruby on Rails\n¶\nRuby on Rails\nis an open source web framework for developing database-backed web applications. It is optimized for sustainable productivity of the programmer since it lets the programmer to write code by favoring convention over configuration. This guide explains how to install and configure Ruby on Rails for an Ubuntu system with Apache2 and MySQL.\nPrerequisites\n¶\nBefore installing Rails you should install Apache (or a preferred web server) and a database service such as MySQL.\nTo install the Apache package, please refer to\nour Apache guide\n.\nTo install and configure a MySQL database service, refer to\nour MySQL guide\n.\nInstall\nrails\n¶\nOnce you have a web server and a database service installed and configured, you are ready to install the Ruby on Rails package,\nrails\n, by entering the following in the terminal prompt.\nsudo\napt\ninstall\nrails\nThis will install both the Ruby base packages, and Ruby on Rails.\nConfigure the web server\n¶\nYou will need to modify the\n/etc/apache2/sites-available/000-default.conf\nconfiguration file to set up your domains.\nThe first thing to change is the\nDocumentRoot\ndirective:\nDocumentRoot /path/to/rails/application/public\nNext, change the\n<Directory\n\"/path/to/rails/application/public\">\ndirective:\n<Directory \"/path/to/rails/application/public\">\n        Options Indexes FollowSymLinks MultiViews ExecCGI\n        AllowOverride All\n        Order allow,deny\n        allow from all\n        AddHandler cgi-script .cgi\n</Directory>\nYou should also enable the\nmod_rewrite\nmodule for Apache. To enable the\nmod_rewrite\nmodule, enter the following command into a terminal prompt:\nsudo\na2enmod\nrewrite\nFinally, you will need to change the ownership of the\n/path/to/rails/application/public\nand\n/path/to/rails/application/tmp\ndirectories to the user that will be used to run the Apache process:\nsudo\nchown\n-R\nwww-data:www-data\n/path/to/rails/application/public\nsudo\nchown\n-R\nwww-data:www-data\n/path/to/rails/application/tmp\nIf you need to compile your application assets run the following command in\nyour application directory:\nRAILS_ENV\n=\nproduction\nrake\nassets:precompile\nConfigure the database\n¶\nWith your database service in place, you need to make sure your app database configuration is also correct. For example, if you are using MySQL the your\nconfig/database.yml\nshould look like this:\n# Mysql \nproduction:\n  adapter: mysql2\n  username: user\n  password: password\n  host: 127.0.0.1 \n  database: app\nTo finally create your application database and apply its migrations you can run the following commands from your app directory:\nRAILS_ENV\n=\nproduction\nrake\ndb:create\nRAILS_ENV\n=\nproduction\nrake\ndb:migrate\nThat’s it! Now your Server is ready for your Ruby on Rails application. You can\ndaemonize\nyour application as you want.\nFurther reading\n¶\nSee the\nRuby on Rails\nwebsite for more information.\nAgile Development with Rails\nis also a great resource.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:36Z", "original_len_words": 458}}
{"id": "5d4c338f8c", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/use-apache2-modules/", "title": "How to use Apache2 modules - Ubuntu Server documentation", "text": "How to use Apache2 modules - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to use Apache2 modules\n¶\nApache2 is a modular server. This implies that only the most basic functionality is included in the core server. Extended features are available through modules which can be loaded into Apache2.\nBy default, a base set of modules is included in the server at compile-time. If the server is compiled to use dynamically loaded modules, then modules can be compiled separately, and added at any time using the\nLoadModule\ndirective. Otherwise, Apache2 must be recompiled to add or remove modules.\nUbuntu compiles Apache2 to allow the dynamic loading of modules. Configuration directives may be conditionally included on the presence of a particular module by enclosing them in an\n<IfModule>\nblock.\nInstalling and handling modules\n¶\nYou can install additional Apache2 modules and use them with your web server. For example, run the following command at a terminal prompt to install the Python 3 WSGI module:\nsudo\napt\ninstall\nlibapache2-mod-wsgi-py3\nThe installation will enable the module automatically, but we can disable it with\na2dismod\n:\nsudo\na2dismod\nwsgi\nsudo\nsystemctl\nrestart\napache2.service\nAnd then use the\na2enmod\nutility to re-enable it:\nsudo\na2enmod\nwsgi\nsudo\nsystemctl\nrestart\napache2.service\nSee the\n/etc/apache2/mods-available\ndirectory for additional modules already available on your system.\nConfigure Apache2 for HTTPS\n¶\nThe\nmod_ssl\nmodule adds an important feature to the Apache2 server - the ability to encrypt communications. Thus, when your browser is communicating using SSL, the\nhttps://\nprefix is used at the beginning of the Uniform Resource Locator (URL) in the browser navigation bar.\nThe\nmod_ssl\nmodule is available in the\napache2-common\npackage. Run the following command at a terminal prompt to enable the\nmod_ssl\nmodule:\nsudo\na2enmod\nssl\nThere is a default HTTPS configuration file in\n/etc/apache2/sites-available/default-ssl.conf\n. In order for Apache2 to provide HTTPS, a\ncertificate\nand\nkey\nfile are also needed. The default HTTPS configuration will use a certificate and key generated by the\nssl-cert\npackage. They are good for testing, but the auto-generated certificate and key should be replaced by a certificate specific to the site or server.\nNote\nFor more information on generating a key and obtaining a certificate see\nCertificates\n.\nTo configure Apache2 for HTTPS, enter the following:\nsudo\na2ensite\ndefault-ssl\nNote\nThe directories\n/etc/ssl/certs\nand\n/etc/ssl/private\nare the default locations. If you install the certificate and key in another directory make sure to change\nSSLCertificateFile\nand\nSSLCertificateKeyFile\nappropriately.\nWith Apache2 now configured for HTTPS, restart the service to enable the new settings:\nsudo\nsystemctl\nrestart\napache2.service\nNote that depending on how you obtained your certificate, you may need to enter a passphrase when Apache2 restarts.\nYou can access the secure server pages by typing\nhttps://your_hostname/url/\nin your browser address bar.\nSharing write permission\n¶\nFor more than one user to be able to write to the same directory you will need to grant write permission to a group they share in common. The following example grants shared write permission to\n/var/www/html\nto the group “webmasters”.\nsudo\nchgrp\n-R\nwebmasters\n/var/www/html\nsudo\nchmod\n-R\ng\n=\nrwX\n/var/www/html/\nThese commands recursively set the group permission on all files and directories in\n/var/www/html\nto allow reading, writing and searching of directories. Many admins find this useful for allowing multiple users to edit files in a directory tree.\nWarning\nThe\napache2\ndaemon will run as the\nwww-data\nuser, which has a corresponding\nwww-data\ngroup. These\nshould not\nbe granted write access to the document root, as this would mean that vulnerabilities in Apache or the applications it is serving would allow attackers to overwrite the served content.\nFurther reading\n¶\nThe\nApache2 Documentation\ncontains in depth information on Apache2 configuration directives. Also, see the apache2-doc package for the official Apache2 docs.\nO’Reilly’s\nApache Cookbook\nis a good resource for accomplishing specific Apache2 configurations.\nFor Ubuntu specific Apache2 questions, ask in the\n#ubuntu-server\nIRC channel on\nlibera.chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:37Z", "original_len_words": 677}}
{"id": "58e6810c06", "source_url": "https://documentation.ubuntu.com/server/how-to/web-services/use-nginx-modules/", "title": "How to use nginx modules - Ubuntu Server documentation", "text": "How to use nginx modules - Ubuntu Server documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\nBack to top\nView this page\nHow to use nginx modules\n¶\nLike other web servers, nginx supports dynamically loaded modules to provide in-server support for programming languages, security mechanisms, and so on. Ubuntu provides a number of these modules as separate packages that are either installed simultaneously with nginx, or can be installed separately.\nAvailable modules\n¶\nnginx will report the modules it has been built with via its\n-V\noption.  A quick and dirty way to list the available modules is thus:\n$\nnginx\n-V\n2\n>\n&\n1\n|\ntr\n--\n-\n'\\n'\n|\ngrep\n_module\nhttp_ssl_module\nhttp_stub_status_module\nhttp_realip_module\n...\nhttp_image_filter_module\n=\ndynamic\nhttp_perl_module\n=\ndynamic\nhttp_xslt_module\n=\ndynamic\nstream_geoip_module\n=\ndynamic\nMany of these modules are built-in and thus are always available with nginx, but some exist as separate packages whose installation status can be checked via\napt\n. For example:\n$\napt\npolicy\nlibnginx-mod-http-image-filter\nlibnginx-mod-http-image-filter:\nInstalled:\n(\nnone\n)\nCandidate:\n1\n.24.0-1ubuntu1\nVersion\ntable:\n1\n.24.0-1ubuntu1\n500\n500\nhttp://archive.ubuntu.com/ubuntu\nmantic/main\namd64\nPackages\napt\ncan also be used to install the desired dynamic module:\n$\nsudo\napt\ninstall\nlibnginx-mod-http-image-filter\n...\nThe\nfollowing\nNEW\npackages\nwill\nbe\ninstalled:\nlibnginx-mod-http-image-filter\n0\nupgraded,\n1\nnewly\ninstalled,\n0\nto\nremove\nand\n34\nnot\nupgraded.\n...\nTriggering\nnginx\nreload\n...\nEnabling and disabling dynamic modules\n¶\nDynamic modules are automatically enabled and get reloaded by nginx on installation. If you need to manually disable an installed module, remove its file from the\n/etc/nginx/modules-enabled\ndirectory, for example:\n$\nls\n/etc/nginx/modules-*\n/etc/nginx/modules-available:\n/etc/nginx/modules-enabled:\n50\n-mod-http-image-filter.conf\n\n$\nsudo\nmv\n/etc/nginx/modules-enabled/50-mod-http-image-filter.conf\n/etc/nginx/modules-available/\n\n$\nservice\nnginx\nrestart\nNote that built-in modules cannot be disabled/enabled.\nConfiguring modules\n¶\nThe installed configuration file for an nginx module mainly consists of the dynamically-loaded binary library:\n## /etc/nginx/modules-enabled/50-mod-http-image-filter.conf\nload_module modules/ngx_http_image_filter_module.so;\nNote that you can also use the\nload_module\nparameter in your\n/etc/nginx/nginx.conf\nat the top level, if preferred for some reason.\nTo use a module for your website, its settings are specified in your server block. For example:\nlocation /img/ {\n    image_filter resize 240 360;\n    image_filter rotate 180;\n    image_filter_buffer 16M;\n    error_page   415 = /415.html;\n}\nFurther reading\n¶\nYou’ve completed the nginx guide! See the following resources for more in-depth information on further extending nginx’s capabilities:\nThe\nnginx documentation\nprovides detailed explanations of configuration directives.\nO’Reilly’s nginx cookbook provides guidance on solving specific needs.\nFor Ubuntu-specific nginx questions, ask in the\n#ubuntu-server\nIRC channel on\nlibera.chat\n.", "meta": {"site": "documentation.ubuntu.com", "crawl_ts": "2025-11-27T10:38:37Z", "original_len_words": 426}}