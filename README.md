# RAG System for a SysAdmin Assistant

A smart RAG system for creating a system administrator assistant using:

* **RAGatouille + ColBERT** for efficient semantic search
* **Ollama gemma:2b** for generating answers in Russian
* **Smart chunking** that respects document structure

## Features

* ğŸ§  Semantic search using ColBERT
* ğŸ“š Automatic splitting of documents into optimal chunks
* ğŸ’¬ Interactive chat interface
* ğŸ” Search through Ubuntu documentation
* ğŸ“– References to information sources
* ğŸ¯ **LangChain document compressors** for improving context quality (optional)

Demo:

## Installation

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Install and run Ollama

```bash
# Install Ollama (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Start the Ollama server
ollama serve

# In another terminal, download the gemma:2b model
ollama pull gemma:2b
```

### 3. Prepare the data

Make sure you have a `parsed.jsonl` file with data in the format:

```json
{"id": "...", "source_url": "...", "title": "...", "text": "...", "meta": {...}}
```

## Quick Start

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Ensure Ollama is running

```bash
ollama serve
# In another terminal:
ollama pull gemma:2b
```

### 3. Build the index if it does not exist in `.ragatouille/colbert/indexes`

```bash
python main.py --mode build --data parsed.jsonl
```

### 4. Launch the chat interface

Frontend:

```bash
streamlit run rag_chat_app/frontend/app.py
```

Backend:

```bash
uvicorn rag_chat_app.backend.app:app --reload --port 8000
```

## Usage

### Building the index

First, build the ColBERT index from your data:

```bash
python main.py --mode build --data parsed.jsonl
```

To rebuild the index:

```bash
python main.py --mode build --data parsed.jsonl --rebuild
```

**Note:** Building the index may take some time (10â€“30 minutes depending on data size and machine performance).

### Interactive chat

After building the index, start the interactive mode:

```bash
python main.py --mode chat
```

Example query:

```
Question: How do I set up Active Directory on Ubuntu?
```

### Test mode

For quick testing:

```bash
python main.py --mode test --query "How do I backup the system?"
```

## Project Structure

```
.
â”œâ”€â”€ benchmarks # 2 benchmarks
â”‚   â”œâ”€â”€ benchmark_rag.csv
â”‚   â”œâ”€â”€ benchmark_rag.py
â”‚   â”œâ”€â”€ build_all_indexes.sh
â”‚   â””â”€â”€ build_indexes.log
â”œâ”€â”€ code_data # parse/load/split prepare data
â”‚   â”œâ”€â”€ add_dataset.py
â”‚   â”œâ”€â”€ add_url.py
â”‚   â”œâ”€â”€ parse_ubiuntu.py
â”‚   â”œâ”€â”€ scraper.py
â”‚   â””â”€â”€ split_parsed.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ dataset1.jsonl
â”‚   â”œâ”€â”€ dataset2.jsonl
â”‚   â”œâ”€â”€ dataset_upload # datasets can be uploaded (commands.json makes too small chunks)
â”‚   â”‚   â”œâ”€â”€ commands.json
â”‚   â”‚   â””â”€â”€ dataset1.parquet
â”‚   â”œâ”€â”€ parsed.jsonl
â”‚   â”œâ”€â”€ parsed_part1.jsonl
â”‚   â”œâ”€â”€ parsed_part2.jsonl
â”‚   â”œâ”€â”€ parsed_part3.jsonl
â”‚   â”œâ”€â”€ parsed_part4.jsonl
â”‚   â”œâ”€â”€ parsed_part5.jsonl
â”‚   â””â”€â”€ urls.txt
â”œâ”€â”€ RAG
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ document_compressor.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ rag_system.py
â”œâ”€â”€ rag_chat_app # app
â”‚   â”œâ”€â”€ backend
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”‚   â””â”€â”€ rag_model.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ routes
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â””â”€â”€ services
â”‚   â”‚       â””â”€â”€ retrieval.py
â”‚   â”œâ”€â”€ frontend
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## Configuration

### Chunking parameters

In `rag_system.py` you can adjust chunking parameters:

```python
rag = SysAdminRAG(
    chunk_size=512,      # Chunk size (in tokens)
    chunk_overlap=50     # Overlap between chunks
)
```

### Ollama parameters

```bash
python main.py --ollama-url http://localhost:11434 --ollama-model gemma:2b
```

### Using document compressors

LangChain document compressors help filter and compress retrieved documents before sending them to the LLM, improving answer quality:

```bash
# With compression (recommended for better quality)
python main.py --mode chat --use-compression

# With similarity threshold adjustment
python main.py --mode chat --use-compression --compression-threshold 0.8
```

**How it works:**

* After searching documents via ColBERT, the compressor uses `EmbeddingsFilter` for additional filtering
* Documents with low semantic similarity to the query are filtered out
* This reduces noise in the context and improves answer generation quality

**Parameters:**

* `--use-compression`: Enable document compression
* `--compression-threshold`: Similarity threshold (0.0â€“1.0, default 0.76)

  * Higher = stricter filtering (fewer documents)
  * Lower = looser filtering (more documents)

## Architecture

1. **Chunking** (`chunking.py`):

   * Splits documents into semantically related parts
   * Respects sentence and paragraph boundaries
   * Preserves metadata for each chunk

2. **RAG System** (`rag_system.py`):

   * Uses RAGatouille with ColBERT for indexing
   * Performs semantic search on queries
   * Integrates with Ollama for answer generation

3. **Main** (`main.py`):

   * CLI interface for interacting with the system
   * Modes: build, chat, test

## Example questions

* "How do I set up Active Directory on Ubuntu?"
* "How do I backup the system?"
* "How do I install and configure Bacula?"
* "How do I configure DNS on Ubuntu Server?"
* "How do I use etckeeper to version configuration files?"

## Troubleshooting

### Ollama connection error

Make sure Ollama is running:

```bash
ollama serve
```

Check availability:

```bash
curl http://localhost:11434/api/tags
```

### Index building error

* Make sure you have enough RAM (ColBERT requires ~4â€“8GB)
* Check the format of `parsed.jsonl`
* Ensure all dependencies are installed

### Slow search

* Reduce the number of returned results (`k` parameter)
* Use smaller chunk sizes
* Make sure the index is correctly built
